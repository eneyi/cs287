% No 'submit' option for the problems by themselves.
%\documentclass{harvardml}
% Use the 'submit' option when you submit your solutions.
\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{Nicolas Drizard}
\email{nicolasdrizard@g.harvard.edu}

% List any people you worked with.
\collaborators{%
Virgile Audi\\
vaudi@g.harvard.edu
}

% You don't need to change these.
\course{CS 287}
\duedate{March 25, 2016}

\usepackage{url, enumitem}
\usepackage{amsfonts}
%\usepackage{listings}
\usepackage{bm}
\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{multirow}
\usepackage{graphicx, color}
\usepackage[toc,page]{appendix}
\usepackage{framed}
\begin{document}

\section*{Introduction}

If we want to communicate and reason with a machine, then the machine will need to be able to ingest and understand the underlying logic of the sentences we communicate to it. Pick the Echo for instance, say you are to tell it that your mother just bought this great new phone on Amazon, and that it makes you jealous, wouldn't it be great (or at least for Amazon) if the Echo understood that the answer to the question: "why am I jealous?" was the fact that you don't have the latest smartphone and replied by offering to order it for you immediately? This is the type of task that goes outside the scope of Factoid Question Answering that queries a knowledge base to answer question such as "Who was the 1st President of the United States?". In this project, we would like to tackle the issue of Non-Factoid Question Answer by implementing a Dynamic Memory Network for Natural Language Processing, introduced in "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing" [\ref{itm:1}].

\section{Problem Formulation}

The goal of this project to build a model capable of solving the following type of problem:

\begin{enumerate}
\item Read a set of sentences forming a story or knowledge base
\item Given the story, the model answers a question on this story by a word or a sentence itself
\end{enumerate}
An example of such problems could be:

\begin{framed}
\begin{center}
Story: John is bored. John goes out in the garden.\\
Q: Why is John in the garden?\\
A: John was bored
\end{center}
\end{framed}

The model we plan on implementing is supposed to successfully complete a set of 20 task presented in "Towards AI-complete question answering: a set of prerequisit toy tasks" by J. Weston et al. [\ref{item:2}] Such tasks include:
\begin{itemize}
\item Single Supporting Fact: ``consists of questions where a previously given single supporting fact, potentially amongst a set of other irrelevant facts, provides the answer''
\item Two or Three Supporting Facts: ``two supporting statements have to be chained to answer the question''
\item Positional and Size Reasoning
\item Time Reasoning, etc.
\end{itemize}

We will use the standard \emph{bAbI} dataset which consists of 20 pairs of training and test sets of each 1000 questions.

\section{Dynamic Memory Network}

Present and explain the different modules (using GRU or LSTM)

\section{Metric and Baseline}

\subsection{Metric}
The metric we will use to evaluate the quality of the model will be a measure of the accuracy on the 20 test sets of the \emph{bAbI} dataset. This is the same metric used in [\ref{itm:1}]. We will consider the project to be a success if we manage to reproduce the results presented in the paper, i.e. an average accuracy of 93.6\% of the 20 \emph{bAbi} tasks. If it is relatively easy to obtain positive results on the \emph{bAbi} dataset, as it is claimed in [\ref{itm:1}] that the model can be applied to many other such as Part-of-speech tagging, we plan to benchmarked the performance of the model using the models developped in HW2.

\subsection{Baseline}

Count based model

\section{Possible Extension}
paper Visual Question Answering

\section{Bibliography}
\begin{enumerate}
	\item \emph{Ask Me Anything:
Dynamic Memory Networks for Natural Language Processing
}, A. Kumar, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani, V. Zhong, R. Paulus \& R. Socher, MetaMind, 2016 \label{itm:1}

	\item \emph{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}, J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. van MerriÃ«nboer, A. Joulin, T. Mikolov, Facebook  AI Research, 2015 \label{item:2}
	
\end{enumerate}
\end{document}


