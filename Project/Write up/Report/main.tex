\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% Short headings should be running head and authors last names

\ShortHeadings{Memory Networks For Question Answering}{Audi\u{a} and Drizard}
\firstpageno{1}

\begin{document}

\title{Memory Networks For Question Answering}

\author{\name Virgile Audi \email vaudi@g.harvard.edu \\
       \addr  John A. Paulson School Of Engineering And Applied Sciences\\
       \AND
       \name Nicolas Drizard \email nicolasdrizard@g.harvard.edu \\
       \addr John A. Paulson School Of Engineering And Applied Sciences}

\editor{Leslie Pack Kaelbling}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
The focus of this final project is about non-factoid question answering. To tackle this problem, we chose to implement memory network type of models and in particular the dynamic memory network presented in \cite{dmn}. For this project update, we mainly worked on pre-processing the data from the bAbi dataset, implementing a count-base baseline as well as looking at a one hop memory weakly supervised memory network.
\end{abstract}

\begin{keywords}
  Question Answering, Memory Network
\end{keywords}

\section{Introduction}

If we want to communicate and reason with a machine, then the machine will need to be able to ingest and understand the underlying logic of the sentences we communicate to it. Pick the Echo for instance, say you are to tell it that your mother just bought this great new phone on Amazon, and that it makes you jealous. Wouldn't it be great (or at least for Amazon) if the Echo understood that the answer to the question: "why am I jealous?" was the fact that you don't have the latest smartphone and replied by offering to order it for you immediately? This kind of tasks are called non-factoid question answering as they go beyond the scope of querying a knowledge base to answer a question such as "Who was the 1st President of the United States?". In this project, we would like to tackle the issue of non-factoid question answering by implementing the Dynamic Memory Network developped in \cite{dmn}.

\section{Related Work}

\section{Count Based Model}
\subsection{Intuition}
\subsection{Model}

\section{End-to-end Memory Network}
\subsection{Architecture}
\subsection{Parameters Tying}

\section{Experiment}
\subsection{Implementation Details}
\paragraph{BOW}
basic recall + padding with parameters set to 0 for padding

\paragraph{Temporal Encoding}
\paragraph{Position Encoding}
\paragraph{Linear Start}

\subsection{Results}
Results for following configuration on test:
- 1 hop adjacent joint
- 1 hop TE adjacent joint
- 2 hop TE adjacent joint
- 3 hop TE adjacent joint
- 1 hop TE PE adjacent joint
- 2 hop TE PE adjacent joint
- 3 hop TE PE adjacent joint
- 3 hop TE PE adjacent by task
- 3 hop TE PE RNN like joint

if time:
- 1 hop TE PE adjacent joint LS
- 2 hop TE PE adjacent joint LS
- 3 hop TE PE adjacent joint LS

\section{Future Work}

\section{Conclusion}


\vskip 0.2in
\bibliography{sample}

\end{document}