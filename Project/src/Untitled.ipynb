{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "import h5py\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('../Data/preprocess/task2_train.hdf5','r') as hf:\n",
    "    ans = np.array(hf.get('answers'))\n",
    "    questions = np.array(hf.get('questions'))\n",
    "    questions_sentences = np.array(hf.get('questions_sentences'))\n",
    "    sentences = np.array(hf.get('sentences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_relevance(story, ques, n = 1, stopwords = None):\n",
    "\n",
    "    nsentence, nword =  story.shape\n",
    "    \n",
    "    relevance = np.zeros(nsentence)\n",
    "    for i in range(nsentence):\n",
    "        for j in range(nword):\n",
    "            for w in ques:\n",
    "                if story[i,j] == w and w not in stopwords:\n",
    "                    relevance[i] += 1\n",
    "                    \n",
    "    if n == 1:\n",
    "        return np.argmax(relevance\n",
    "    else:\n",
    "        res = list(np.arange(nsentence)[relevance.nonzero()]+1)\n",
    "        for i in list(np.arange(nsentence)[relevance.nonzero()]):\n",
    "            for ii in np.arange(nsentence):\n",
    "                if ii+1 in res:\n",
    "                    continue\n",
    "                else:\n",
    "                    count = 0\n",
    "                    for j in range(nword):\n",
    "                        for w in story[i,:]:\n",
    "                            if story[ii,j] == w and w not in stopwords:\n",
    "                                count += 1\n",
    "                    if count > 0:\n",
    "                        res.append(ii+1)\n",
    "                            \n",
    "        return sorted(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_question_vector(questions, answers, aw_number, alpha=0.1):\n",
    "    '''\n",
    "    Build embeddings of the first word of the question.\n",
    "    Output: dictionnary {q[0]: embeddings}\n",
    "    '''\n",
    "    # Build the embeddings\n",
    "    questions_embeddings = {}\n",
    "    for q, r in zip(questions, answers):\n",
    "        # Index starts at 1\n",
    "        if q[0] not in questions_embeddings:\n",
    "            questions_embeddings[q[0]] = alpha*np.ones(aw_number)\n",
    "        questions_embeddings[q[0]][r[0]-1] += 1\n",
    "\n",
    "    # Normalize\n",
    "    for k in questions_embeddings.keys():\n",
    "        questions_embeddings[k] /= np.sum(questions_embeddings[k])\n",
    "\n",
    "    return questions_embeddings\n",
    "\n",
    "\n",
    "def build_story_aw_distribution(facts, aw_number, alpha=0.1, decay=0.1):\n",
    "    '''\n",
    "    Compute the count of answer words in the fact. Weight down the\n",
    "    old words.\n",
    "    Output: normalized count vector\n",
    "    '''\n",
    "    count_vector = alpha*np.ones(aw_number)\n",
    "    bow = facts.flatten()\n",
    "    for i in xrange(len(bow)-1, -1, -1):\n",
    "        b = bow[i]\n",
    "        # check not padding and an answer word\n",
    "        if b != 0 and b <= aw_number:\n",
    "            # weighted coung\n",
    "            count_vector[b-1] += 1 + decay*i\n",
    "    # Normalization\n",
    "    count_vector /= np.sum(count_vector)\n",
    "\n",
    "    return count_vector\n",
    "\n",
    "\n",
    "def question_matching_feature(facts, question, aw_number, alpha=0.1):\n",
    "    '''\n",
    "    Indicator feature on the set of possible answer words to indicate\n",
    "    if a question word is inside the same sentence as an answer word\n",
    "    in the facts.\n",
    "    Output: normalized count vector.\n",
    "    '''\n",
    "    count_vector = alpha*np.ones(aw_number)\n",
    "    question_set = set([q for q in question if q != 0])\n",
    "    for fact in facts:\n",
    "        fact_set = set([q for q in fact if q != 0])\n",
    "        intersection = question_set.intersection(fact_set)\n",
    "        for i in intersection:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences, questions, questions_sentences, answers = read_preprocessed_matrix_data('task2_train')\n",
    "word2index, index2sentence, index2question = read_preprocessed_mapping('task2_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_embeddings = train_question_vector(questions, answers, 6,\n",
    "                                                 alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word = {}\n",
    "for k,v in word2index.iteritems():\n",
    "    index2word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'garden'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "for nq in range(1000):\n",
    "    story_1 = sentences[(questions_sentences[nq,0]-1):questions_sentences[nq,1],:]\n",
    "    ques = questions[nq]\n",
    "    facts_ = sentences[questions_sentences[nq][0]-1: questions_sentences[nq][1], :]\n",
    "    facts = facts_[np.array(sentence_relevance(facts_, questions[nq], 2, [0, 20, 90]),dtype = int)-1,:]\n",
    "    f2 = np.log(build_story_aw_distribution(facts, 6, decay = 0))\n",
    "    f1 = np.log(questions_embeddings[questions[nq][0]])\n",
    "    r = np.exp(f1+f2)\n",
    "    r /= np.sum(r)\n",
    "    if answers.flatten()[nq] == (np.argmax(r)+1):\n",
    "        ct+=1\n",
    "print float(ct)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nq = 0\n",
    "story_1 = sentences[(questions_sentences[nq,0]-1):questions_sentences[nq,1],:]\n",
    "ques = questions[nq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant sentences: \n",
      "[ 0.  0.  1.  0.  0.  0.]\n",
      "[1, 3, 5, 6]\n",
      "True relevant sentences: \n",
      "[1 6 3 6 0]\n"
     ]
    }
   ],
   "source": [
    "print \"Relevant sentences: \"\n",
    "print sentence_relevance(story_1, ques, 2, [0, 20, 90])\n",
    "print \"True relevant sentences: \"\n",
    "print questions_sentences[nq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "facts_ = sentences[questions_sentences[nq][0]-1: questions_sentences[nq][1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 19, 17, 16,  4,  0],\n",
       "       [11, 15, 17, 16,  1,  0],\n",
       "       [18, 15, 17, 16,  2,  0],\n",
       "       [11, 15, 17, 16,  6,  0],\n",
       "       [11, 15, 17, 16,  1,  0],\n",
       "       [18, 15, 17, 16,  6,  0],\n",
       "       [18, 19,  9, 17, 16,  4],\n",
       "       [10, 13, 17, 16,  1,  0]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# facts = sentences[sentence_relevance(facts_, questions[nq], 3, [0, 21, 90])-1,:]\n",
    "facts_[np.array(sentence_relevance(facts_, questions[nq], 3, [0, 21, 90]))-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = np.log(build_story_aw_distribution(facts, 6, decay = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.77258872, -2.77258872, -0.37469345, -2.77258872, -2.77258872,\n",
       "       -2.77258872])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = np.log(questions_embeddings[questions[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.87075336, -1.74299786, -1.79576508, -1.86428503, -1.75448564,\n",
       "       -1.73164055])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05789751  0.0657875   0.68646679  0.05827322  0.06503607  0.06653892]\n"
     ]
    }
   ],
   "source": [
    "r = np.exp(f1+f2)\n",
    "print r / np.sum(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 3\n",
      "Predicted: 3\n"
     ]
    }
   ],
   "source": [
    "print \"True: %i\" %answers.flatten()[nq]\n",
    "print \"Predicted: %i\" % (np.argmax(r)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
