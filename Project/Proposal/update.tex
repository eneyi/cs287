% No 'submit' option for the problems by themselves.
%\documentclass{harvardml}
% Use the 'submit' option when you submit your solutions.
\documentclass[submit]{harvardml}
% Put in your full name and email address.
\name{Nicolas Drizard}
\email{nicolasdrizard@g.harvard.edu}

\collaborators{Virgile Audi \\ 
vaudi@g.harvard.edu}

% You don't need to change these.
\course{CS281-F15}
\assignment{Project Proposal}
\duedate{23:59pm October 14, 2015}


% Useful macros.
\usepackage{graphicx, color}
\usepackage{url, enumitem}
\usepackage{amsfonts}
%\usepackage{listings}
\usepackage{bm}
\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{multirow}
\usepackage{graphicx, color}
\usepackage[toc,page]{appendix}

\usepackage{float}
\floatplacement{figure}{H} % force figures to be placed always at defined position!

\newcommand{\trans}{\mathsf{T}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\distNorm}{\mathcal{N}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\ident}{\mathbb{I}}

\begin{document}

\section*{Introduction}

Looking for an applied project, we decided to take part in Yelp Dataset Challenge round 6 ! The dataset is constituted of 1.6 million reviews for 61 thousands business in the UK, the US, Germany and Canada. It also contains 481 thousands attributes such as hours, parking availability or ambience as well as aggregated check-ins over time for the entire base of business. We chose to analyse this data set because it gave us the opportunity to come to conclusions that could eventually be implemented on Yelp’s platform. In more details, we focus in this project on the classification by categories. When looking for a restaurant, Yelp users enter keywords and get results corresponding to these “tag words”. But how are these tags associated to each venue? Until now, each owner manually associates the tags to their venue when creating the profile along many attributes that users can then update based on their experiences. Can we use Machine Learning techniques to create a more refine category system? Would it also be possible to find some new categories that are not as obvious as “Pizzeria” or “Fast Food” based on the features and characteristics of the venues? How much information can reviews give us on the type of restaurants ? These are some of the questions we are looking to answer in this project.

We now present a road map to the project with possible extensions if time permitted:

\section{Data Preparation}

The given dataset gathers many information under several formats. We first need to extract relevant features of our data and/or pre-process them to apply our algorithms.

\subsection{Numerical Feature Extraction}

This part works with two tables of the Yelp data: business and checkin. The first one stores information about the business (localizations, name, categories ...) and the second contains the average number of customers checkins for each hour of the week.\\
We joined and converted these two tables in order to have only numerical or binary features.\\
We applied different operations to build the features:
\begin{itemize}
	\item Dropping irelevant features
	\item Identifying categorical features and converting them into orthogonal vectors of a N dimensional space, with  \textit{N = number of categories}
	\item One attribute of each business stores different information in an unstructured way (each business does not have the same number of information stored by this attribute). It may concern, for instance, the price, the ambience, if alcohol is offered... We needed to identify the most shared informations to avoid too many missing values
\end{itemize}
On top of these steps, e chosed to focus first on the restaurants (we filtered the dataset to keep only the entries with 'Restaurants' in their list of categories). Also to build easily our first baselines we chosed not to handle the missing values and droped the business with not enough information.\\\\

In conclusion, we built in this part the data set of the Yelp restaurants with 205 features relatives to the customers checkins and the inherent properties of the restaurants. We will test these features under supervised learning algorithms.

Some improvement might be considered:
\begin{itemize}
	\item Combining features and/or applying them polynomial or cosinus base function to increase their complexity
	\item Considering more attributes and the business where we don't have the checkin information while filling the missing values. This inference can be done through the average or median values over the entire data set, or through a nearest neighbors estimation.
\end{itemize}

This preliminary work lead to ve

\subsection{Text Processing}

\textbf{Virgile}

\section{Supervised Learning} 

\subsection{Multinomial Logistic Regression}

Considering the data set built with the processing with numerical features. We applied a multiclass logistic regression from scikit learn. \\
The first question was to narrow down our targets. In the restaurants entries, there are still 261 different categories shared. As the text mining of the reviews seemed more complicated, we needed to consider the most diffenriating categories to have decent models. As a result, we focused on the nationality of the food to test among the restaurants.\\
With 3 classes: 1 without specific nationality and the two most represented nationalities, Mexican and Chinese, the multiclass logistic regression provided poor results on the test set (the data set was separated in the schema 80:20, train:test), only slightly better than predicting no nationality for each entry (dummy model). The explanations could be that the features do not contain relevant information to differentiate the style and the restaurant set without missing values contains only 13000 entries with around 1000 positive for the two classes.\\\\
Precision score:
\begin{align*}
	\rho_{dummy} & = 0.671423029551 \\
	\rho_{logreg} &  = 0.726111608768 
\end{align*}

\begin{figure}[H]
	\includegraphics[scale=0.45]{ROC.png}
	\centering
	\title{Multi class ROC curve}
\end{figure}




\subsection{Dimensionality Reduction}

Given the poor results of our model, applying a principal component analysis did not improve the performence. The intrinsic quality need to be improved first. 

\section{Infering New Subcategories}

Once we studied the current label, we would infer latent correlations among the existing categories to come up with new subcategories.

\subsection{K-Nearest-Neighbors}
The preliminary work on the features will embed the examples in a multi-dimensional space from which we can extract cluster based on the local geometry. We could apply an unsupervised method as the k-nearest-neighbors to infer new subcategories. For instance this clustering method could be used locally in each already known category to infer subcategories or more globally. Another approach if time could be to apply a decision tree, which has the advantage to work on categorical features. One drawback could be its exposure to overfitting.

\subsection{Latent Dirichlet Allocation with network component}

Using the results about the venues' clustering using the regular K-NN algorithm and taking them into account when performing LDA, we will look at a possible improvement of the performance. In other words, we will try to add an extra node in the graphical model representing the structure of the graph that has an influence on the topic node.

\section{Evaluation methods}
 
To assess the performance of the algorithms used in part B, we will divide the data set into a training and test sets in order to make prediction on the test set. We will evaluate the logistic regression through the classical classifiers metrics considering each class separately; we will compute the confusion matrix and extract the accuracy and the recall. 
Evaluating the new subcategories will be a challenge as it remains very subjective. We can always assess the performance of the LDA model using the perplexity metric but how good this will translate in terms of category classification still needs to be investigate.

\section{Possible Extensions}

\begin{itemize}
	\item Online variational inference for LDA (https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf) to speed up convergence
	\item Use Hierarchical Dirichlet Process to relax the assumptions of the number of categories

\end{itemize}

\section{Work Division}

Given our personal affinity, we subdivided each part. 
\begin{itemize}
	\item \textbf{Nicolas} : \textit{Feature Extraction}, \textit{Dimensionality Reduction}, \textit{Multinomial Logistic Regression} and \textit{K-nn}
	\item \textbf{Virgile}: \textit{Text Processing}, \textit{LDA} and \textit{LDA with network component}
\end{itemize}

\end{document}
