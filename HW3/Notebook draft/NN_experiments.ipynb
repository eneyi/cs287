{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'hdf5'\n",
    "require 'optim'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  test : LongTensor - size: 3761x55\n",
       "  nwords : LongTensor - size: 1\n",
       "  train_1000_nocounts : LongTensor - size: 696825x7\n",
       "  train_nocounts : DoubleTensor - size: 887522x6\n",
       "  train_1000 : DoubleTensor - size: 887522x6\n",
       "  train : LongTensor - size: 772670x7\n",
       "  valid : LongTensor - size: 3370x55\n",
       "  valid_txt : DoubleTensor - size: 70391x6\n",
       "  valid_output : LongTensor - size: 3370x50\n",
       "}\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFile = hdf5.open('../../6-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "myFile:close()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data['train_nocounts']:narrow(2,1,6)\n",
    "train_input = train:narrow(2,1,5)\n",
    "train_output = train:narrow(2,6,1)\n",
    "\n",
    "valid_txt = data['valid_txt']:narrow(2,1,6)\n",
    "valid_txt_input = valid_txt:narrow(2,1,5)\n",
    "valid_txt_output = valid_txt:narrow(2,6,1)\n",
    "\n",
    "valid_topredict = data['valid']:narrow(2,1,50)\n",
    "valid_input = data['valid']:narrow(2,51,5)\n",
    "valid_output = data['valid_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Model\n",
    "nwords = 10001\n",
    "N = 5\n",
    "dwin = N\n",
    "hid1 = 30\n",
    "hid2 = 100\n",
    "\n",
    "-- To store the whole model\n",
    "nnlm = nn.Sequential()\n",
    "\n",
    "-- Layer to embedd (and put the words along the window into one vector)\n",
    "LT = nn.Sequential()\n",
    "LT_ = nn.LookupTable(nwords,hid1)\n",
    "LT:add(LT_)\n",
    "LT:add(nn.View(-1, hid1*dwin))\n",
    "\n",
    "nnlm:add(LT)\n",
    "\n",
    "concat = nn.ConcatTable()\n",
    "\n",
    "lin_tanh = nn.Sequential()\n",
    "lin_tanh:add(nn.Linear(hid1*dwin,hid2))\n",
    "lin_tanh:add(nn.Tanh())\n",
    "\n",
    "id = nn.Identity()\n",
    "\n",
    "concat:add(lin_tanh)\n",
    "concat:add(id)\n",
    "\n",
    "nnlm:add(concat)\n",
    "nnlm:add(nn.JoinTable(2))\n",
    "nnlm:add(nn.Linear(hid1*dwin + hid2, nwords))\n",
    "nnlm:add(nn.LogSoftMax())\n",
    "\n",
    "-- Loss\n",
    "criterion = nn.ClassNLLCriterion()\n",
    "\n",
    "\n",
    "nEpochs = 1\n",
    "batchSize = 32\n",
    "eta = 0.01\n",
    "av_L = 0\n",
    "\n",
    "inputs_batch = torch.DoubleTensor(batchSize,dwin)\n",
    "targets_batch = torch.DoubleTensor(batchSize)\n",
    "outputs = torch.DoubleTensor(batchSize, nwords)\n",
    "dL_do = torch.DoubleTensor(batchSize, nwords)\n",
    "\n",
    "kag_pred_valid = torch.Tensor(valid_input:size(1),50)\n",
    "norm_mat = torch.Tensor(valid_input:size(1),50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters, gradParameters = nnlm:getParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 817.29979610443\t\n",
       "Average Perplexity on train: 1038.2426267938\t\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "/Users/virgileaudi/torch/install/share/lua/5.1/nn/THNN.lua:1091: multi-target not supported at /tmp/luarocks_nn-scm-1-9848/nn/lib/THNN/generic/ClassNLLCriterion.c:18\nstack traceback:\n\t[C]: in function 'v'\n\t/Users/virgileaudi/torch/install/share/lua/5.1/nn/THNN.lua:1091: in function 'ClassNLLCriterion_updateOutput'\n\t...udi/torch/install/share/lua/5.1/nn/ClassNLLCriterion.lua:41: in function 'forward'\n\t[string \"for i = 1, 1 do...\"]:41: in main chunk\n\t[C]: in function 'xpcall'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:179: in function <.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t.../virgileaudi/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0102352b50",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "/Users/virgileaudi/torch/install/share/lua/5.1/nn/THNN.lua:1091: multi-target not supported at /tmp/luarocks_nn-scm-1-9848/nn/lib/THNN/generic/ClassNLLCriterion.c:18\nstack traceback:\n\t[C]: in function 'v'\n\t/Users/virgileaudi/torch/install/share/lua/5.1/nn/THNN.lua:1091: in function 'ClassNLLCriterion_updateOutput'\n\t...udi/torch/install/share/lua/5.1/nn/ClassNLLCriterion.lua:41: in function 'forward'\n\t[string \"for i = 1, 1 do...\"]:41: in main chunk\n\t[C]: in function 'xpcall'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:179: in function <.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t.../virgileaudi/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0102352b50"
     ]
    }
   ],
   "source": [
    "for i = 1, 1 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    \n",
    "    print('Perplexity on valid.txt: '..math.exp(criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze())))\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat:zero()\n",
    "    norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "    kag_pred_valid:cdiv(norm_mat)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4636394432108\t70391\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     1\n",
       " 10001\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnlm:forward(valid_input[1]):size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 8.0082783571626\t\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kag_pred_valid:zero()\n",
    "for ii = 1, valid_input:size(1) do\n",
    "    kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "end\n",
    "\n",
    "norm_mat:zero()\n",
    "norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "kag_pred_valid:cdiv(norm_mat)\n",
    "\n",
    "CE = 0\n",
    "for iii = 1, valid_input:size(1) do\n",
    "    mm,aa = valid_output[iii]:max(1)\n",
    "    CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "end\n",
    "\n",
    "val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "print('Perplexity on valid: '..val_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 837.54529905319\t\n",
       "Average Perplexity on train: 601.86348771938\t\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 544.09100456299\t\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 7.5411670975744\t\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 1, 1 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    \n",
    "    print('Perplexity on valid.txt: '..math.exp(criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze())))\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat:zero()\n",
    "    norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "    kag_pred_valid:cdiv(norm_mat)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 3: 825.9562330246\t\n",
       "Average Perplexity on train: 527.4359496501\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 490.41028578536\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 7.3124630553358\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 810.81244516373\t\n",
       "Average Perplexity on train: 478.24240750295\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 451.28884307244\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 7.1345689871114\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 809.59715104103\t\n",
       "Average Perplexity on train: 441.10636934143\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 421.14663777603\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.9774349905299\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6: 815.37426805496\t\n",
       "Average Perplexity on train: 411.90963927563\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 397.32171347555\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.8419065078654\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7: 810.07148694992\t\n",
       "Average Perplexity on train: 388.22452792996\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 377.90359626936\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.7242994567635\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8: 843.48118114471\t\n",
       "Average Perplexity on train: 368.57023037956\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 361.76307273096\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.6223244922863\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 3, 8 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    \n",
    "    print('Perplexity on valid.txt: '..math.exp(criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze())))\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat:zero()\n",
    "    norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "    kag_pred_valid:cdiv(norm_mat)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 9: 827.41940593719\t\n",
       "Average Perplexity on train: 351.92530292078\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 348.08247912807\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.5329550483487\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10: 818.73921298981\t\n",
       "Average Perplexity on train: 337.5367575987\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 336.26468217226\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.4530105725527\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 11: 810.9077231884\t\n",
       "Average Perplexity on train: 324.88169664599\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 325.9017569163\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.3802435933612\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 12: 818.34377193451\t\n",
       "Average Perplexity on train: 313.59659021479\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 316.70651340805\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.3133236716505\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 13: 875.60341501236\t\n",
       "Average Perplexity on train: 303.4203653605\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 308.46612556472\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.2514416608928\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 14: 931.39901995659\t\n",
       "Average Perplexity on train: 294.15795710215\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 301.02181851019\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.1939794227997\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 15: 947.89197707176\t\n",
       "Average Perplexity on train: 285.66596548785\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 294.25169224865\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.1404268036459\t\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 9, 15 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    \n",
    "    print('Perplexity on valid.txt: '..math.exp(criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze())))\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat:zero()\n",
    "    norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "    kag_pred_valid:cdiv(norm_mat)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 16: 884.64695596695\t\n",
       "Average Perplexity on train: 289.63091355636\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 298.53214613822\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.161870172494\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 17: 862.36055397987\t\n",
       "Average Perplexity on train: 287.61173679513\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 298.16840792915\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.1582829766217\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 18: 913.85455417633\t\n",
       "Average Perplexity on train: 286.44803734131\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 297.64453359062\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.1549117249554\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 19: 907.04614782333\t\n",
       "Average Perplexity on train: 285.45577036762\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 297.05836750921\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.1512939860661\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 20: 945.45608305931\t\n",
       "Average Perplexity on train: 284.5339143652\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 296.44259460417\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.1474060630745\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 16, 20 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm:updateParameters(0.001)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    \n",
    "    print('Perplexity on valid.txt: '..math.exp(criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze())))\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat:zero()\n",
    "    norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "    kag_pred_valid:cdiv(norm_mat)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 21: 956.9749341011\t\n",
       "Average Perplexity on train: 274.63307799968\t\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 285.71248287443\t\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.0745420849267\t\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 22: 884.85721302032\t\n",
       "Average Perplexity on train: 267.31538454884\t\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 279.97206629824\t\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.026810149315\t\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 21, 22 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    \n",
    "    print('Perplexity on valid.txt: '..math.exp(criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze())))\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat:zero()\n",
    "    norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "    kag_pred_valid:cdiv(norm_mat)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 23: 925.0414981842\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Average Perplexity on train: 260.75603062799\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 274.84122276941\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.9832071111076\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 24: 861.51717591286\t\n",
       "Average Perplexity on train: 254.65781156113\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 270.11753172733\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.942581446567\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 25: 886.49924206734\t\n",
       "Average Perplexity on train: 248.95055580556\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 265.73155316471\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.9045085155161\t\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 23, 25 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    \n",
    "    print('Perplexity on valid.txt: '..math.exp(criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze())))\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat:zero()\n",
    "    norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "    kag_pred_valid:cdiv(norm_mat)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 26: 856.12632894516\t\n",
       "Average Perplexity on train: 243.58987799773\t\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 261.63750741407\t\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.8686917859439\t\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 27: 915.89268708229\t\n",
       "Average Perplexity on train: 238.54260990519\t\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 257.80547119188\t\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.834951373464\t\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 26, 27 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    \n",
    "    print('Perplexity on valid.txt: '..math.exp(criterion:forward(nnlm:forward(valid_txt_input),valid_txt_output:squeeze())))\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid[ii]:copy(nnlm:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat:zero()\n",
    "    norm_mat:copy(torch.expandAs(kag_pred_valid:sum(2), kag_pred_valid))\n",
    "    kag_pred_valid:cdiv(norm_mat)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid:size(1))\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save('../../../nnlm',nnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_topredict = data['test']:narrow(2,1,50)\n",
    "test_input = data['test']:narrow(2,51,5)\n",
    "kag_pred_test = torch.Tensor(test_input:size(1),50)\n",
    "norm_mat = torch.Tensor(test_input:size(1),50)\n",
    "\n",
    "kag_pred_test:zero()\n",
    "for ii = 1, test_input:size(1) do\n",
    "    kag_pred_test[ii]:copy(nnlm:forward(test_input[ii]):index(2, test_topredict[ii])):exp()\n",
    "end\n",
    "\n",
    "norm_mat:zero()\n",
    "norm_mat:copy(torch.expandAs(kag_pred_test:sum(2), kag_pred_test))\n",
    "kag_pred_test:cdiv(norm_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 3761\n",
       "   50\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kag_pred_test:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'bengio_1.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('test', kag_pred_test)\n",
    "myFile:write('valid',kag_pred_valid)\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bengio (no skip):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Model\n",
    "nwords = 10001\n",
    "N = 5\n",
    "dwin = N\n",
    "hid1 = 30\n",
    "hid2 = 100\n",
    "\n",
    "-- To store the whole model\n",
    "nnlm2 = nn.Sequential()\n",
    "\n",
    "-- Layer to embedd (and put the words along the window into one vector)\n",
    "LT2 = nn.Sequential()\n",
    "LT2_ = nn.LookupTable(nwords,hid1)\n",
    "LT2:add(LT2_)\n",
    "LT2:add(nn.View(-1, hid1*dwin))\n",
    "\n",
    "nnlm2:add(LT2)\n",
    "\n",
    "lin_tanh2 = nn.Sequential()\n",
    "lin_tanh2:add(nn.Linear(hid1*dwin,hid2))\n",
    "lin_tanh2:add(nn.Tanh())\n",
    "\n",
    "nnlm2:add(lin_tanh2)\n",
    "nnlm2:add(nn.Linear(hid2, nwords))\n",
    "nnlm2:add(nn.LogSoftMax())\n",
    "\n",
    "-- Loss\n",
    "criterion2 = nn.ClassNLLCriterion()\n",
    "\n",
    "batchSize = 32\n",
    "eta = 0.01\n",
    "av_L = 0\n",
    "\n",
    "inputs_batch = torch.DoubleTensor(batchSize,dwin)\n",
    "targets_batch = torch.DoubleTensor(batchSize)\n",
    "outputs = torch.DoubleTensor(batchSize, nwords)\n",
    "dL_do = torch.DoubleTensor(batchSize, nwords)\n",
    "\n",
    "kag_pred_valid2 = torch.Tensor(valid_input:size(1),50)\n",
    "norm_mat2 = torch.Tensor(valid_input:size(1),50)\n",
    "\n",
    "valtxtperp = torch.Tensor(25)\n",
    "valkagperp = torch.Tensor(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 440.03609108925\t\n",
       "Average Perplexity on train: 1082.4594136257\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 680.00174311834\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 8.0683115316289\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2: 437.69223618507\t\n",
       "Average Perplexity on train: 625.75434648624\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 565.59387298635\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 7.5508052997539\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3: 437.58501505852\t\n",
       "Average Perplexity on train: 542.04730055156\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 504.82438770484\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 7.2963433831007\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 440.55102109909\t\n",
       "Average Perplexity on train: 488.70411608916\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 462.7072798064\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 7.098015126089\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 437.68313002586\t\n",
       "Average Perplexity on train: 450.07392975748\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 431.43937680042\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.9357044990068\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6: 438.48814606667\t\n",
       "Average Perplexity on train: 420.43136766947\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 407.2472402181\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.8043197597726\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7: 438.92952895164\t\n",
       "Average Perplexity on train: 396.75551814658\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 387.86149170982\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.6919146102422\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8: 438.47046113014\t\n",
       "Average Perplexity on train: 377.31399735107\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 371.85625736873\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.5915150215024\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 9: 437.71296596527\t\n",
       "Average Perplexity on train: 360.89540150306\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 358.20662639867\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.4997537264968\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10: 439.26542687416\t\n",
       "Average Perplexity on train: 346.66001827333\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 346.28640781448\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.4149865991385\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 11: 437.89451003075\t\n",
       "Average Perplexity on train: 334.06440532189\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 335.70382460091\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.3367377316278\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 12: 439.25250911713\t\n",
       "Average Perplexity on train: 322.75947617039\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 326.21265528821\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.2646586589572\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 13: 437.93637800217\t\n",
       "Average Perplexity on train: 312.51662767159\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 317.64534339779\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.198286531227\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 14: 437.52219605446\t\n",
       "Average Perplexity on train: 303.17209168724\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 309.86898733408\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.1371942499275\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 15: 437.57384490967\t\n",
       "Average Perplexity on train: 294.59906686244\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 302.77414250815\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.0808821564847\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 16: 438.14680194855\t\n",
       "Average Perplexity on train: 286.6952607716\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 296.26926494731\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 6.0288753184055\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 17: 437.56386184692\t\n",
       "Average Perplexity on train: 279.3772274862\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 290.28151079237\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.9806027627422\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 18: 437.68017911911\t\n",
       "Average Perplexity on train: 272.57639873033\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 284.75322217638\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.9355272990494\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 19: 438.39722704887\t\n",
       "Average Perplexity on train: 266.23748893407\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 279.63531129512\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.8931416469176\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 20: 437.97945809364\t\n",
       "Average Perplexity on train: 260.31190936053\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 274.88321109219\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.8531056498463\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 21: 437.68694591522\t\n",
       "Average Perplexity on train: 254.7575250141\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 270.45718040551\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.8151450727278\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 22: 437.4172911644\t\n",
       "Average Perplexity on train: 249.53665455955\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 266.32196549948\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.7791420384127\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 23: 437.95185995102\t\n",
       "Average Perplexity on train: 244.61664732927\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 262.44770892005\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.7449874566149\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 24: 438.25385594368\t\n",
       "Average Perplexity on train: 239.96957797857\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 258.80916918393\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.7126058674399\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 25: 437.70601701736\t\n",
       "Average Perplexity on train: 235.57116926444\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid.txt: 255.38467631097\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity on valid: 5.6819592787889\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 1, 25 do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT2_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        nnlm2:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(nnlm2:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        L = criterion2:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L + L\n",
    "\n",
    "        -- Backward pass\n",
    "        dL_do:narrow(1,1,current_batch_size):copy(criterion2:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        nnlm2:backward(inputs_batch:narrow(1,1,current_batch_size), dL_do:narrow(1,1,current_batch_size))\n",
    "        nnlm2:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Perplexity on train: '.. math.exp(av_L/math.floor(train_input:size(1)/batchSize)))\n",
    "    \n",
    "    \n",
    "    -- Evaluating perplexity on valiadation_txt:\n",
    "    perp_txt = math.exp(criterion2:forward(nnlm2:forward(valid_txt_input),valid_txt_output:squeeze()))\n",
    "    valtxtperp[i] = perp_txt\n",
    "    print('Perplexity on valid.txt: '..perp_txt)\n",
    "    \n",
    "    -- Evaluatin perplexity on validation kaggle:\n",
    "    kag_pred_valid2:zero()\n",
    "\n",
    "    for ii = 1, valid_input:size(1) do\n",
    "        kag_pred_valid2[ii]:copy(nnlm2:forward(valid_input[ii]):index(2, valid_topredict[ii])):exp()\n",
    "    end\n",
    "    \n",
    "    norm_mat2:zero()\n",
    "    norm_mat2:copy(torch.expandAs(kag_pred_valid2:sum(2), kag_pred_valid2))\n",
    "    kag_pred_valid2:cdiv(norm_mat2)\n",
    "    \n",
    "    CE = 0\n",
    "    for iii = 1, valid_input:size(1) do\n",
    "        mm,aa = valid_output[iii]:max(1)\n",
    "        CE = CE + math.log(kag_pred_valid2[iii][aa[1]])\n",
    "    end\n",
    "    \n",
    "    val_res = math.exp(-CE/kag_pred_valid2:size(1))\n",
    "    valkagperp[i] = val_res\n",
    "    print('Perplexity on valid: '..val_res)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_topredict = data['test']:narrow(2,1,50)\n",
    "test_input = data['test']:narrow(2,51,5)\n",
    "kag_pred_test2 = torch.Tensor(test_input:size(1),50)\n",
    "norm_mat2 = torch.Tensor(test_input:size(1),50)\n",
    "\n",
    "kag_pred_test2:zero()\n",
    "for ii = 1, test_input:size(1) do\n",
    "    kag_pred_test2[ii]:copy(nnlm2:forward(test_input[ii]):index(2, test_topredict[ii])):exp()\n",
    "end\n",
    "\n",
    "norm_mat2:copy(torch.expandAs(kag_pred_test2:sum(2), kag_pred_test2))\n",
    "kag_pred_test2:cdiv(norm_mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'bengio_2.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('test', kag_pred_test2)\n",
    "myFile:write('valid',kag_pred_valid2)\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'bengio_2_perp.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('test', valtxtperp)\n",
    "myFile:write('valid',valkagperp)\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save('../../../nnlm2',nnlm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
