
\documentclass[11pt]{article}

\usepackage{common}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}

\title{HW3: (Neural) Language Modeling}
\author{Nicolas Drizard \\ nicolasdrizard@g.harvard.edu \and Virgile Audi \\ vaudi@g.harvard.edu }
\begin{document}

\maketitle{}
\section{Introduction}

This assignment focuses on the task of language modeling, a crucial first-step for many natural language applications. In this report, we will present several count-based multinomial language models with different smoothing methods, an influential neural network based language model from the work of Bengio et al. (2003), and an extension to this language model which learns using noise contrastive estimation, as well as their implementation using Torch. We found this homework more challenging than the previous ones and encountered significant challenges that we will underline in this report. 


\section{Problem Description}

The goal of the language models presented in this report is to learn a distributed representation for words as well as probability distribution for word sequences. Language models are usually represented as the probability of generating a new word conditioned on the preceeding words:

$$ P(\boldsymbol{w}_{1:n}) = \prod\limits_{i=1}^{n-1}P(w_{i+1}|w_i)$$

\noindent To simplify the analysis, it is common to make the assumption that a word is influenced only by the $N$ words immediately preceeding it, which we call the context. Even with reasonably small values for $N$, building such models are extremely expensive computationally-wise as well as time-consuming if not ran on GPU. The joint proabibility of a sequence of 6 words taken from a vocabulary of 10 000 words could possibly imply training the model to fit up to $10^{4^6}-1 =10^{24}-1$ parameters.\\

\noindent The objective of this homework was to predict a probability distribution over 50 words at a given place in the sentence and based on the previous words.
To do so, we tried implementing N-grams models in an efficient manner. Due to computational limitations (no access to GPUs...), we faced difficulties training the Neural Network and focused our efforts on building strong count-based models to solve the problem of language modeling.


\section{Model and Algorithms}

We will now dive into more details of two types of models, i.e. count-based models and neural network models.

\subsection{Count-based Models}

\subsection{Neural Networ Models}

\subsubsection{Regular Models}
As in the neural networks build for previous homeworks, the model has for input a window of words preceding the wanted predicted word. It first convert the words in the window of size $d_{win}$ my mapping them into a geometrical space of higher dimension $d_{in}$ (30 in Bengio's paper). It then concatenates the words embeddings into a vector of size $d_{in}\times d_{win}$. This has for advantage of adding information about the position of the words in the window, as opposed to making a bag-of-words assumption. The higher dimensional representation  of the window is then fed into a first linear model followed by a hyperbolic tangent layer to extract  non- linear features. A second linear layer is then applied followed by a softmax to get a probability distribution over the vocabulary. We then train the model using a Negative Log-Likelihood criterion and stochastic gradient descent.\\

\noindent We can summarize the model in the following formula:

$$ nnlm_1(\boldsymbol{x}) = \tanh(\boldsymbol{xW}+\boldsymbol{b})\boldsymbol{W'}+\boldsymbol{b'}$$

where we recall that:
\begin{itemize}
\item $\boldsymbol{x}\in \Re^{d_{in}\cdot d_{win}}$ is the concatenated word embeddings
\item $\boldsymbol{W}\in \Re^{(d_{in}\cdot d_{win})\times d_{hid}}$, and $\boldsymbol{b}\in \Re^{d_{hid}}$
\item $\boldsymbol{W'}\in \Re^{d_{hid}\times |\mathcal{V}|}$, and $\boldsymbol{b'}\in \Re^{|\mathcal{V}|}$, where $|\mathcal{V}|$ is the size of the vocabulary.
\end{itemize}

\noindent We give a diagram of the model to better illustrate it:

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.4\textwidth]{reg.png}
    \caption{\underline{Neural Language Model (Bengio,2003)}}
\end{center}
\end{figure}

We then implemented a variant of the model using a skip-layer that concatenates the output of the tanh layer again with the original embeddings. The updated formula for the model is:

$$ nnlm_2(\boldsymbol{x}) = [\tanh(\boldsymbol{xW}+\boldsymbol{b}),\boldsymbol{x}]\boldsymbol{W'}+\boldsymbol{b'}$$

where this time:
\begin{itemize}
\item $\boldsymbol{W'}\in \Re^{(d_{hid}+ d_{in}\cdot d_{win})\times |\mathcal{V}|}$, and $\boldsymbol{b'}\in \Re^{|\mathcal{V}|}$
\end{itemize}

The updated diagram is as follows:
\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.5\textwidth]{skip.png}
    \caption{\underline{Skip-Layer Model}}
\end{center}
\end{figure}

We now show the pseudo code for training these NNLMs using batch stochastic gradient descent:
  \begin{algorithmic}[1]
    \Procedure{$NNLM_i$}{$win_{1},...,win_{n}$,MaxEpoch,BatchSize,LearningRate}
    \For{epoch = 1,MaxEpoch}
    \For{batch = 1, $|$train$|$/BatchSize}
    \For{win in batch}
    \State{Call $NNLM_i$:forward(win)}
    \State{Evalute the loss}
    \State{Evaluate derivatives of the loss}
    \State{Backprop through $NNLM_i$}
    \EndFor{}
    \State{Update Parameters with LearningRate}
    \EndFor{}
    
    \EndFor{}
    \EndProcedure{}
  \end{algorithmic}


\subsubsection{Noise Contrastive Estimation}



\section{Experiments}

We now present the results of our experiments. We will first talk about the preprocessing and then continue with a comparison of the different models.

\subsection{Data and Preprocessing}

To complete this homework, we were given 3 datasets, one for training, one for validation and one for testing. The train set consisted of sentences with a total of over eight hundred thousands words from a vocabulary of ten thousands words. The validation set consisted of 70391 words. The particularity of the issue at hand consisted in the fact that we had to only predict a probability distribution over 50 words and not on the entire vocabulary. This is why we were provided the same validation set in the same format as for the test set. We could therefore predict 3370 words on the validation set to help us predict the 3361 words of the test set.\\

\noindent Most of the preprocessing was about building the N-grams for the different sets. We included in the preprocess.py file different functions to evaluate the windows as well as counting the different occurences of each N-grams. For instance, looking at 6-grams gave:

\begin{itemize}
\item 772 670 unique 6-grams on the training set,
\item 887 522 6-grams in total,
\item 70 391 6-grams on the validation set,
\item 3 370 words to predict on the validation set,
\item and 3 361 words of the test set
\end{itemize}

\subsection{Evaluation}

To evaluate the models, we will use the perplexity measure. For a set of $m$ $N$-grams, $\boldsymbol{w}_1,...,\boldsymbol{w}_m$, it is defined to be:

$$ P(\boldsymbol{w}_1,...,\boldsymbol{w}_m) = \exp\left(-\frac{1}{m} \sum\limits_{i = 1}^{m} \log P(w_N^i|w_{N-1}^i,...,w_1^i)\right)$$

\noindent In other words, the perplexity translates how likely is the predicted word given the previous N-1 words. In this report, we will evaluated perplexity both on the entire vocabulary but also on the reduced 50 words to predict from. Values between these two "different" perplexities wil range from 3 to 1000.
\subsection{Count-based Models}

\subsection{Neural Models}

The main issue we faced with Bengio's model was training time. Even we managed to have a working model with a smaller vocabulary, we struggled at first to get a code that ran fast enough to experiment extensively with different paremetrisations. Our original code ran one epoch in about one hour for the paremetrisation. While trying to code the NCE approximation, we nevertheless managed to cut the training time to about 14-15 minutes.\\

We started to train the more simple neural network i.e. the one without the skip-layer, with the parametrisation suggested by Bengio:

\begin{itemize}
\item Window size: $d_{win} = 5$
\item Dimension of the embeddings: $d_{in} = 30$
\item Hidden dimension: $d_{hid} = 100$
\end{itemize}

We summarize the results in the graphs below:

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{noskip_tr}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{noskip_val}
  \end{minipage}
\end{figure}

Before making a submission on kaggle, we decided to compare the encouraging results with the Skip-Layer model. We ran the experiment 5 epochs at a time to give us control on the learning rate. We thought that after the 15 epochs, the model was close to convergence, and decided to decrease the learning rate from 0.01 to 0.001. We obtain the following results:
\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{skip_tr}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{skip_val}
  \end{minipage}
\end{figure}

\noindent As one can see, changing the learning rate negatively impacted the results. It plateaued but perplexity started decreasing again as soon as we re-up the learning rate. It also unclear how much improvement the skip-layer model brings compared to the original Bengio model, and this even without the change in learning rate. Based on these observation, we decided to submit to Kaggle the results of the simple model. We obtained:

\subsection{NCE}

We unfortunately did not succeed to implement a valid version of the Noise Contrastive Estimation. We did not managed to have a speed improvement and even worse observed that the perplexity on the validation was increasing instead of decreasing.

\subsection{Mixtures of models}
In order to increase our score, we decided to combine the differente approaches by averaging the results over the distributions outputted by various models.

\section{Conclusion}

End the write-up with a very short recap of the main experiments and the main results. Describe any challenges you may have faced, and what could have been improved in the model.

\section{References}
\begin{itemize}
\item Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model. \emph{Journal of Machine Learning Research}, 3:1137–1155.

\item Mnih, A. and Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise- contrastive estimation. \emph{In Advances in Neural Information Processing Systems}, pages 2265–2273.

\item Chen, S. and Goodman, J. (1998). An Empirical Study of Smo othing Techniques for Language. \emph{Technical Report TR-10-98}, Computer Science Group, Harvard University.
\end{itemize}
\end{document}
