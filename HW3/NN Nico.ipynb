{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'hdf5'\n",
    "require 'nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('5-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwords = data['nwords'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 887522\n",
       "      5\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwords = data['nwords'][1]\n",
    "train = data['train_nocounts']:narrow(2,1,5)\n",
    "train_input = train:narrow(2,1,4)\n",
    "train_output = train:narrow(2,5,1)\n",
    "print(train:size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 620208\n",
       "      5\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwords = 1001\n",
    "train_1000 = data['train_1000']:narrow(2,1,5)\n",
    "train_input_1000 = train_1000:narrow(2,1,4)\n",
    "train_output_1000 = train_1000:narrow(2,5,1)\n",
    "print(train_1000:size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Model\n",
    "N = 4\n",
    "dwin = N\n",
    "hid1 = 30\n",
    "hid2 = 100\n",
    "\n",
    "-- To store the whole model\n",
    "dnnlm = nn.Sequential()\n",
    "\n",
    "-- Layer to embedd (and put the words along the window into one vector)\n",
    "LT = nn.Sequential()\n",
    "LT_ = nn.LookupTable(nwords,hid1)\n",
    "LT:add(LT_)\n",
    "LT:add(nn.View(-1, hid1*dwin))\n",
    "\n",
    "dnnlm:add(LT)\n",
    "\n",
    "concat = nn.ConcatTable()\n",
    "\n",
    "lin_tanh = nn.Sequential()\n",
    "lin_tanh:add(nn.Linear(hid1*dwin,hid2))\n",
    "lin_tanh:add(nn.Tanh())\n",
    "\n",
    "id = nn.Identity()\n",
    "\n",
    "concat:add(lin_tanh)\n",
    "concat:add(id)\n",
    "\n",
    "dnnlm:add(concat)\n",
    "dnnlm:add(nn.JoinTable(2))\n",
    "dnnlm:add(nn.Linear(hid1*dwin + hid2, nwords))\n",
    "dnnlm:add(nn.LogSoftMax())\n",
    "\n",
    "-- Loss\n",
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Testing the model\n",
    "\n",
    "input = train_input:narrow(1,2,1)\n",
    "input_batch = train_input:narrow(1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 2\n",
       " 4\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_batch:size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   2\n",
       " 120\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LT:forward(input_batch):size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_lt = LT:forward(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_lin_tanh = lin_tanh:forward(output_lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   2\n",
       " 220\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.JoinTable(2):forward(concat:forward(output_lt)):size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     2\n",
       " 10001\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnnlm:forward(input_batch):size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization: SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 69.119084119797\t\n",
       "Average Loss: 4.756511695101\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2: 76.563917160034\t\n",
       "Average Loss: 4.6551651763745\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3: 65.445013046265\t\n",
       "Average Loss: 4.5677477064412\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 69.316218137741\t\n",
       "Average Loss: 4.4928766393631\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 78.419605970383\t\n",
       "Average Loss: 4.431313198327\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nEpochs = 5\n",
    "batchSize = 32\n",
    "eta = 0.01\n",
    "av_L = 0\n",
    "\n",
    "inputs_batch = torch.DoubleTensor(batchSize,dwin)\n",
    "targets_batch = torch.DoubleTensor(batchSize)\n",
    "outputs = torch.DoubleTensor(batchSize, nwords)\n",
    "df_do = torch.DoubleTensor(batchSize, nwords)\n",
    "\n",
    "\n",
    "for i = 1, nEpochs do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input_1000:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input_1000:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input_1000:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output_1000:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        dnnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(dnnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        f = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L +f\n",
    "\n",
    "        -- Backward pass\n",
    "        df_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size)))\n",
    "        dnnlm:backward(inputs_batch:narrow(1,1,current_batch_size), df_do:narrow(1,1,current_batch_size))\n",
    "        dnnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Loss: '..av_L/math.floor(train_input_1000:size(1)/batchSize))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 92.552931785583\t\n",
       "Average Loss: 5.809480985211\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2: 83.495311021805\t\n",
       "Average Loss: 5.096047246622\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3: 81.004708051682\t\n",
       "Average Loss: 4.982135471234\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 85.910176992416\t\n",
       "Average Loss: 4.936083521231\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 83.465891122818\t\n",
       "Average Loss: 4.9097221622256\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nEpochs = 5\n",
    "batchSize = 32\n",
    "eta = 0.001\n",
    "av_L = 0\n",
    "\n",
    "inputs_batch = torch.DoubleTensor(batchSize,dwin)\n",
    "targets_batch = torch.DoubleTensor(batchSize)\n",
    "outputs = torch.DoubleTensor(batchSize, nwords)\n",
    "df_do = torch.DoubleTensor(batchSize, nwords)\n",
    "\n",
    "\n",
    "for i = 1, nEpochs do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input_1000:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input_1000:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input_1000:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output_1000:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        dnnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(dnnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        f = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L +f\n",
    "\n",
    "        -- Backward pass\n",
    "        df_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size)))\n",
    "        dnnlm:backward(inputs_batch:narrow(1,1,current_batch_size), df_do:narrow(1,1,current_batch_size))\n",
    "        dnnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Loss: '..av_L/math.floor(train_input_1000:size(1)/batchSize))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 75.719837903976\t\n",
       "Average Loss: 4.8516148841494\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2: 70.374673128128\t\n",
       "Average Loss: 4.7898740237402\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3: 65.336530208588\t\n",
       "Average Loss: 4.737851349316\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 67.845108985901\t\n",
       "Average Loss: 4.687586092071\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 75.712246894836\t\n",
       "Average Loss: 4.640044784213\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nEpochs = 5\n",
    "batchSize = 64\n",
    "eta = 0.01\n",
    "av_L = 0\n",
    "\n",
    "inputs_batch = torch.DoubleTensor(batchSize,dwin)\n",
    "targets_batch = torch.DoubleTensor(batchSize)\n",
    "outputs = torch.DoubleTensor(batchSize, nwords)\n",
    "df_do = torch.DoubleTensor(batchSize, nwords)\n",
    "\n",
    "\n",
    "for i = 1, nEpochs do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input_1000:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input_1000:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input_1000:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output_1000:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        dnnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(dnnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        f = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L +f\n",
    "\n",
    "        -- Backward pass\n",
    "        df_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size)))\n",
    "        dnnlm:backward(inputs_batch:narrow(1,1,current_batch_size), df_do:narrow(1,1,current_batch_size))\n",
    "        dnnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Loss: '..av_L/math.floor(train_input_1000:size(1)/batchSize))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 887522\n",
       "      5\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwords = data['nwords'][1]\n",
    "train = data['train_nocounts']:narrow(2,1,5)\n",
    "train_input = train:narrow(2,1,4)\n",
    "train_output = train:narrow(2,5,1)\n",
    "print(train:size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 957.66832304001\t\n",
       "Average Loss: 6.4620778781752\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nEpochs = 1\n",
    "batchSize = 32\n",
    "eta = 0.01\n",
    "av_L = 0\n",
    "\n",
    "inputs_batch = torch.DoubleTensor(batchSize,dwin)\n",
    "targets_batch = torch.DoubleTensor(batchSize)\n",
    "outputs = torch.DoubleTensor(batchSize, nwords)\n",
    "df_do = torch.DoubleTensor(batchSize, nwords)\n",
    "\n",
    "\n",
    "for i = 1, nEpochs do\n",
    "    -- timing the epoch\n",
    "    timer = torch.Timer()\n",
    "    av_L = 0\n",
    "    \n",
    "    -- max renorm\n",
    "    LT_.weight:renorm(2,1,1)\n",
    "    \n",
    "    -- mini batch loop\n",
    "    for t = 1, train_input:size(1), batchSize do\n",
    "        -- Mini batch data\n",
    "        current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "        inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "        targets_batch:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "        \n",
    "        -- reset gradients\n",
    "        dnnlm:zeroGradParameters()\n",
    "        --gradParameters:zero()\n",
    "\n",
    "        -- Forward pass (selection of inputs_batch in case the batch is not full, ie last batch)\n",
    "        outputs:narrow(1,1,current_batch_size):copy(dnnlm:forward(inputs_batch:narrow(1,1,current_batch_size)))\n",
    "\n",
    "        -- Average loss computation\n",
    "        f = criterion:forward(outputs:narrow(1,1,current_batch_size), targets_batch:narrow(1,1,current_batch_size))\n",
    "        av_L = av_L +f\n",
    "\n",
    "        -- Backward pass\n",
    "        df_do:narrow(1,1,current_batch_size):copy(criterion:backward(outputs:narrow(1,1,current_batch_size),\n",
    "                targets_batch:narrow(1,1,current_batch_size)))\n",
    "        dnnlm:backward(inputs_batch:narrow(1,1,current_batch_size), df_do:narrow(1,1,current_batch_size))\n",
    "        dnnlm:updateParameters(eta)\n",
    "        \n",
    "    end\n",
    "        \n",
    "    print('Epoch '..i..': '..timer:time().real)\n",
    "    print('Average Loss: '..av_L/math.floor(train_input:size(1)/batchSize))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
