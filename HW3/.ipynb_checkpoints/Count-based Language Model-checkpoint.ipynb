{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Train format is (number of Ngrams, Ngram_size + 1) with last\n",
    "-- col the count of the N_gram of the line\n",
    "\n",
    "-- Validation format is (number of words to predict, 50 + Ngrams_size -1)\n",
    "-- where the 50 columns stands for the 50 words possibilities in the prediction,\n",
    "-- the next col stands for the current context (goal is to predict the Nth word)\n",
    "\n",
    "myFile = hdf5.open('1-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_1 = data['train']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('2-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_2 = data['train']\n",
    "validation_2 = data['valid']\n",
    "validation_output = data['valid_output']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('3-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_3 = data['train']\n",
    "validation_3 = data['valid']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('4-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_4 = data['train']\n",
    "validation_4 = data['valid']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('5-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_5 = data['train']\n",
    "test_5 = data['test']\n",
    "validation_5 = data['valid']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('6-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_6 = data['train']\n",
    "test_6 = data['test']\n",
    "validation_6 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Second version: reversing the order in F_c_w\n",
    "\n",
    "function build_context_count(count_tensor)\n",
    "    -- Ngram count (depend on w and context)\n",
    "    -- {'index1-...-indexN-1': {'indexN' : count}}\n",
    "    local F_c_w = {}\n",
    "    -- F_c dict (independent of w, only context based)\n",
    "    -- {index1-...-indexN-1 : count all words in c}\n",
    "    local F_c = {}\n",
    "    -- N_c dict (independent of w, only context based)\n",
    "    -- {index1-...-indexN-1 : count unique type of words in c}\n",
    "    local N_c = {}\n",
    "\n",
    "    local N = count_tensor:size(1)\n",
    "    local M = count_tensor:size(2)\n",
    "\n",
    "    for i=1, N do\n",
    "        indexN = count_tensor[{i,M-1}]\n",
    "        \n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(count_tensor[{i,1}])\n",
    "        for j=2, M - 2 do\n",
    "            indexes = indexes .. '-' .. tostring(count_tensor[{i,j}])\n",
    "        end\n",
    "        \n",
    "        -- Filling F_c_w\n",
    "        if F_c_w[indexes] == nil then\n",
    "            F_c_w[indexes] = {[indexN] = count_tensor[{i, M}]}\n",
    "        else\n",
    "            F_c_w[indexes][indexN] = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Updating F_c and F_c\n",
    "        if F_c[indexes] == nil then\n",
    "            F_c[indexes] = count_tensor[{i, M}]\n",
    "            N_c[indexes] = 1\n",
    "        else\n",
    "            F_c[indexes] = count_tensor[{i, M}] + F_c[indexes]\n",
    "            N_c[indexes] = 1 + N_c[indexes]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return F_c_w, F_c, N_c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Prediction with the MLE (with Laplace smoothing)\n",
    "\n",
    "function mle_proba(data, F_c_w, alpha)\n",
    "    local N = data:size(1)\n",
    "    local M = data:size(2)\n",
    "    -- Output format: distribution predicted for each N word along the\n",
    "    -- 50 possibilities\n",
    "    local distribution = torch.DoubleTensor(N, 50)\n",
    "\n",
    "    for i=1, N do\n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(data[{i,51}])\n",
    "        for j=52, M do\n",
    "            indexes = indexes .. '-' .. tostring(data[{i,j}])\n",
    "        end\n",
    "        \n",
    "        -- Look up in the dictionnary for the 50 possible ngrams asked\n",
    "        for j=1, 50 do\n",
    "            indexN = data[{i,j}]\n",
    "            if F_c_w[indexN] == nil or F_c_w[indexN][indexes] == nil then\n",
    "                distribution[{i,j}] = alpha\n",
    "            else\n",
    "                distribution[{i,j}] = F_c_w[indexN][indexes] + alpha\n",
    "            end\n",
    "        end\n",
    "        -- Debug: case where no n-gram were found (only when alpha=0.)\n",
    "        if distribution:narrow(1,i,1):sum(2)[{1,1}] == 0 then\n",
    "            -- Select the first one (most common)\n",
    "            distribution[{i,1}] = 1\n",
    "        end\n",
    "    end\n",
    "    -- normalization (ie we do the MLE given only the 50 possibilities)\n",
    "    distribution:cdiv(torch.expand(distribution:sum(2), N, 50))\n",
    "    \n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Loading train of the gram_size N\n",
    "function get_train(N)\n",
    "    local filename = N .. '-grams.hdf5'\n",
    "    print(filename)\n",
    "    myFile = hdf5.open(filename,'r')\n",
    "    train = myFile:all()['train']\n",
    "    myFile:close()\n",
    "    return train\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Prediction with the MLE (with Laplace smoothing) and fix on never seen context\n",
    "\n",
    "function mle_proba_2(data, N, alpha)\n",
    "    local N_data = data:size(1)\n",
    "    local M = data:size(2)\n",
    "    -- Output format: distribution predicted for each N_data word along the\n",
    "    -- 50 possibilities\n",
    "    local distribution = torch.DoubleTensor(N_data, 50)\n",
    "    local gram_size\n",
    "    \n",
    "    -- Building the count matrix for each ngram size lower than N,\n",
    "    -- if need to reduce the context in case of unseen new context\n",
    "    local F_c_w_table = {}\n",
    "    for i=1,N do\n",
    "        train = get_train(i)\n",
    "        F_c_w, F_c, N_c = build_context_count_2(train)\n",
    "        F_c_w_table[i] = F_c_w\n",
    "    end\n",
    "\n",
    "    for i=1, N_data do\n",
    "        -- Initialize the Ngram_size\n",
    "        gram_size = N\n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(data[{i,51}])\n",
    "        for j=52, M do\n",
    "            indexes = indexes .. '-' .. tostring(data[{i,j}])\n",
    "        end\n",
    "        -- case context never seen before:\n",
    "        -- we look for the reduced context (ie index2-...-indexN-1)\n",
    "        if F_c_w_table[N][indexes] == nil then\n",
    "            -- Look for previous context\n",
    "            for k = N-1,1,-1 do\n",
    "                -- update gram_size\n",
    "                gram_size = k\n",
    "                -- Building index in reduced context\n",
    "                indexes_split = indexes:split(\"-\")\n",
    "                indexes = ''\n",
    "                for i= 1 + N - gram_size, N-1 do\n",
    "                    indexes = indexes .. indexes_split[i]\n",
    "                end\n",
    "                -- look if current context seen\n",
    "                if F_c_w_table[gram_size][indexes] ~= nil then\n",
    "                    break\n",
    "                end\n",
    "            end \n",
    "        end\n",
    "\n",
    "        -- Look up in the dictionnary for the 50 possible ngrams asked\n",
    "        for j=1, 50 do\n",
    "            indexN = data[{i,j}]\n",
    "            if gram_size == 1 then\n",
    "                -- Format of indexes is always string\n",
    "                indexes = tostring(indexN)\n",
    "            end\n",
    "            -- case word never seen in that context: smoothing only\n",
    "            if F_c_w_table[gram_size][indexes][indexN] == nil then\n",
    "                distribution[{i,j}] = alpha\n",
    "            else\n",
    "                distribution[{i,j}] = F_c_w_table[gram_size][indexes][indexN] + alpha\n",
    "            end\n",
    "        end\n",
    "\n",
    "        -- case uniform prediction (because no word present in current context)\n",
    "        if distribution:narrow(1,i,1):sum(2)[{1,1}] == 50 * alpha then\n",
    "            -- TODO: try with previous context\n",
    "            -- Select the first one (most common)\n",
    "            distribution[{i,1}] = 1 + alpha\n",
    "            --print('here', i, j, gram_size, indexes, indexN)\n",
    "        end\n",
    "    end\n",
    "    -- normalization (ie we do the MLE given only the 50 possibilities)\n",
    "    distribution:cdiv(torch.expand(distribution:sum(2), N_data, 50))\n",
    "    \n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function perplexity(distribution, true_words)\n",
    "    -- exp of the average of the cross entropy of the true word for each line\n",
    "    -- true words (N_words to predict, one hot true value among 50)\n",
    "    local perp = 0\n",
    "    local N = true_words:size(1)\n",
    "    for i = 1,N do\n",
    "        mm,aa = true_words[i]:max(1)\n",
    "        perp = perp + math.log(distribution[{i, aa[1]}])\n",
    "    end\n",
    "    perp = math.exp(- perp/N)\n",
    "    return perp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Test of new mle version\n",
    "distribution_mle_2_new = mle_proba_2(validation_2, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t8.8674198826233\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle_2_new, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t8.8680603647573\t\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_c_w, F_c, N_c = build_context_count(train_2)\n",
    "distribution_mle_2 = mle_proba(validation_2, F_c_w, 1)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle_2, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t17.059968522337\t\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Test of new mle version\n",
    "distribution_mle_3_new = mle_proba_2(validation_3, 3, 1)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle_3_new, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_c_w_table = {}\n",
    "for i=1,3 do\n",
    "    train = get_train(i)\n",
    "    F_c_w, F_c, N_c = build_context_count_2(train)\n",
    "    F_c_w_table[i] = F_c_w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 13\n",
       "  500   305  8425  5919  4090  4981  8611  8244  4804  2009  9910   471     6\n",
       "\n",
       "Columns 14 to 26\n",
       " 9904  1198   514  1061  5049  6289  8585  1697  4648  3676  7371  1867  4857\n",
       "\n",
       "Columns 27 to 39\n",
       " 3924  6939  8431  3086  4288  3138  4486  3949  3730  8607  7310  9582  9873\n",
       "\n",
       "Columns 40 to 51\n",
       " 6426  7482  1215  6676  9535  1417  6268  7276  1861  4232  7448  3884\n",
       "[torch.LongTensor of size 1x51]\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_2:narrow(1, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  6594 : 10\n",
       "}\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_c_w_table[1]['6594']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_c_w, F_c, N_c = build_context_count(train_3)\n",
    "distribution_mle_3 = mle_proba(validation_3, F_c_w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_c_w, F_c, N_c = build_context_count(train_4)\n",
    "distribution_mle_4 = mle_proba(validation_4, F_c_w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t8.8680603647573\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 3grams\t21.513973503713\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t29.942969799245\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Results on the validation set\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle_2, validation_output))\n",
    "print('Result on alpha smoothing 3grams', perplexity(distribution_mle_3, validation_output))\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_mle_4, validation_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Witten Bell Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_wb_line(N, entry, F_c_w_table, F_c_table, N_c_table, alpha)\n",
    "    -- Return vector (50) predicting the distribution from entry\n",
    "    -- N represent the Ngram size used in the prediction so context is N-1 gram\n",
    "    -- alpha is only used for the MLE without any context\n",
    "    local prediction = torch.zeros(50)\n",
    "    local indexN\n",
    "    -- case where computation only on the prior\n",
    "    if N == 1 then\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- Corner case when prediction on words not on the dict (case for <s>)\n",
    "            if F_c_w_table[1][tostring(indexN)] == nil then\n",
    "                prediction[j] = 0\n",
    "            else\n",
    "                prediction[j] = F_c_w_table[1][tostring(indexN)][indexN] + alpha\n",
    "            end\n",
    "        end\n",
    "        -- Normalizing\n",
    "        return prediction:div(prediction:sum(1)[1])\n",
    "    else\n",
    "        -- Compute the MLE for current N\n",
    "        -- context (at least with one element)\n",
    "        local indexes = tostring(entry[{1, entry:size(2)}])\n",
    "        for j=entry:size(2) - 1, entry:size(2) - 1 - (N-3), -1 do\n",
    "            indexes = tostring(entry[{1, j}]) .. '-' .. indexes\n",
    "        end\n",
    "        -- check if context is unseen, otherwise go to next context\n",
    "        if F_c_w_table[N][indexes] == nil then\n",
    "            --print('unseen context')\n",
    "            return compute_wb_line(N-1, entry, F_c_w_table, F_c_table, N_c_table, alpha)\n",
    "        end\n",
    "        -- Compute MLE for each word\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            if F_c_w_table[N][indexes][indexN] ~= nil then\n",
    "                prediction[j] = F_c_w_table[N][indexes][indexN]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        -- Check that MLE predicted at least one words, otherwise go to next context\n",
    "        if prediction:sum(1)[1] == 0 then\n",
    "            --print('unseen words')\n",
    "            return compute_wb_line(N-1, entry, F_c_w_table, F_c_table, N_c_table, alpha)\n",
    "        end\n",
    "        \n",
    "        -- Combining with next context\n",
    "        prediction:add(compute_wb_line(N-1, entry, F_c_w_table, F_c_table, N_c_table, alpha):mul(N_c_table[N][indexes]))\n",
    "        -- Normalization\n",
    "        -- TODO: We normalize as we apply on a reduced dataest (50 words)\n",
    "        prediction:div(prediction:sum(1)[1])\n",
    "        return prediction\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Witten Bell: new version, computation done at once line by line\n",
    "--\n",
    "-- p_wb(w|c) = (F_c_w + N_c_. * p_wb(w|c'))/(N_c_. + F_c_.)\n",
    "function distribution_proba_WB(N, data, alpha)\n",
    "    local N_data = data:size(1)\n",
    "    local M = data:size(2)\n",
    "\n",
    "    -- Building the count matrix for each ngram size lower than N.\n",
    "    local F_c_w_table = {}\n",
    "    local F_c_table = {}\n",
    "    local N_c_table = {}\n",
    "    for i=1,N do\n",
    "        train = get_train(i)\n",
    "        F_c_w_table[i], F_c_table[i], N_c_table[i] = build_context_count(train)\n",
    "    end\n",
    "\n",
    "    -- Vector initialisation\n",
    "    local distribution = torch.zeros(N_data, 50)\n",
    "    for i=1,N_data do\n",
    "        -- Compute witten bell for the whole line i\n",
    "        distribution:narrow(1, i, 1):copy(compute_wb_line(N, data:narrow(1,i,1), F_c_w_table, F_c_table, N_c_table, alpha))\n",
    "    end\n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unseen words\t\n"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = compute_wb_line(2, validation_3:narrow(1,6,1), F_c_w_table, F_c_table, N_c_table, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_2 = distribution_proba_WB(2, validation_2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams\t6.0029978880708\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t5.8434260885191\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 3grams\t6.8723895034594\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t7.4900378770477\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_1 = distribution_proba_WB(1, validation_2, 1)\n",
    "print('Result on alpha smoothing 1grams', perplexity(distribution_1, validation_output))\n",
    "distribution_2 = distribution_proba_WB(2, validation_2, 1)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_2, validation_output))\n",
    "distribution_3 = distribution_proba_WB(3, validation_3, 1)\n",
    "print('Result on alpha smoothing 3grams', perplexity(distribution_3, validation_output))\n",
    "distribution_4 = distribution_proba_WB(4, validation_4, 1)\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_4, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams\t6.023602585938\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t5.0433818281195\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 3grams\t4.5689460403288\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.4189330367685\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_1 = distribution_proba_WB(1, validation_2, 0)\n",
    "print('Result on alpha smoothing 1grams', perplexity(distribution_1, validation_output))\n",
    "distribution_2 = distribution_proba_WB(2, validation_2, 0)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_2, validation_output))\n",
    "distribution_3 = distribution_proba_WB(3, validation_3, 0)\n",
    "print('Result on alpha smoothing 3grams', perplexity(distribution_3, validation_output))\n",
    "distribution_4 = distribution_proba_WB(4, validation_4, 0)\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_4, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams\t6.0029978880708\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t5.026210590868\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 3grams\t4.5520551448742\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.4020577683054\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_1 = distribution_proba_WB(1, validation_2, 1)\n",
    "print('Result on alpha smoothing 1grams', perplexity(distribution_1, validation_output))\n",
    "distribution_2 = distribution_proba_WB(2, validation_2, 1)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_2, validation_output))\n",
    "distribution_3 = distribution_proba_WB(3, validation_3, 1)\n",
    "print('Result on alpha smoothing 3grams', perplexity(distribution_3, validation_output))\n",
    "distribution_4 = distribution_proba_WB(4, validation_4, 1)\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_4, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.3572276127565\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_5 = distribution_proba_WB(5, validation_5, 1)\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_5, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.3336779743836\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_6 = distribution_proba_WB(6, validation_6, 1)\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_6, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4718359864769\t\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(4.3572276127565)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 3761\n",
       "   55\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_6:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5-grams.hdf5\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6-grams.hdf5\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Applying on test\n",
    "distribution_test_6 = distribution_proba_WB(6, test_6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Saving the current best model\n",
    "myFile = hdf5.open('pred_test_wb_6', 'w')\n",
    "myFile:write('distribution', distribution_test_6)\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m, true_output = validation_output:max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_accuracy(pred, true_output)\n",
    "    max,argmax = pred:max(2)\n",
    "    acc = 0\n",
    "    for i = 1, true_output:size(1) do\n",
    "        if argmax[i][1] == true_output[i][1] then\n",
    "            acc = acc + 1\n",
    "        end\n",
    "    end\n",
    "    score = acc/true_output:size(1)\n",
    "    \n",
    "    return score\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing \t0.59614243323442\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on alpha smoothing ', compute_accuracy(distribution_2, true_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
