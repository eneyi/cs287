{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'hdf5'\n",
    "\n",
    "local Squeeze, parent = torch.class('nn.Squeeze', 'nn.Module')\n",
    "\n",
    "function Squeeze:updateOutput(input)\n",
    "    self.size = input:size()\n",
    "    self.output = input:squeeze()\n",
    "    return self.output\n",
    "end\n",
    "\n",
    "function Squeeze:updateGradInput(input, gradOutput)\n",
    "  self.gradInput = gradOutput:view(self.size)\n",
    "  return self.gradInput  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('5-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  valid_output : LongTensor - size: 3370x50\n",
       "  train : LongTensor - size: 754037x6\n",
       "  nwords : LongTensor - size: 1\n",
       "  valid : LongTensor - size: 3370x54\n",
       "  test : LongTensor - size: 3761x54\n",
       "}\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data['train']:narrow(2,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input = train:narrow(2,1,4)\n",
    "train_output = train:narrow(2,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "dwin = N-1\n",
    "hid1 = 50\n",
    "hid2 = 50\n",
    "nnlm = nn.Sequential()\n",
    "nwords = data['nwords'][1]\n",
    "\n",
    "concat_layer = nn.Concat(1)\n",
    "\n",
    "tanh = nn.Sequential()\n",
    "tanh:add(nn.LookupTable(nwords,hid1))\n",
    "tanh:add(nn.View(1,-1,dwin*hid1))\n",
    "tanh:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "tanh:add(nn.Linear(dwin*hid1,hid2))\n",
    "tanh:add(nn.Tanh())\n",
    "\n",
    "id = nn.Identity()\n",
    "\n",
    "concat_layer:add(tanh)\n",
    "concat_layer:add(id)\n",
    "\n",
    "nnlm:add(concat_layer)\n",
    "nnlm:add(nn.Linear(hid2+dwin*hid1, nwords))\n",
    "nnlm:add(nn.LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset={};\n",
    "for i=1,train:size(1) do \n",
    "  dataset[i] = {train[i]:view(1,5), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = nn.StochasticGradient(neuralnet, criterion)\n",
    "trainer.learningRate = 0.01\n",
    "trainer.maxIteration = 10\n",
    "trainer:train(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
