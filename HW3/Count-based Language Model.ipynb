{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Train format is (number of Ngrams, Ngram_size + 1) with last\n",
    "-- col the count of the N_gram of the line\n",
    "\n",
    "-- Validation format is (number of words to predict, 50 + Ngrams_size -1)\n",
    "-- where the 50 columns stands for the 50 words possibilities in the prediction,\n",
    "-- the next col stands for the current context (goal is to predict the Nth word)\n",
    "\n",
    "myFile = hdf5.open('1-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_1 = data['train']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('2-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_2 = data['train']\n",
    "validation_2 = data['valid']\n",
    "validation_output = data['valid_output']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('3-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_3 = data['train']\n",
    "validation_3 = data['valid']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('4-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_4 = data['train']\n",
    "validation_4 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('5-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_5 = data['train']\n",
    "test_5 = data['test']\n",
    "validation_5 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('6-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_6 = data['train']\n",
    "test_6 = data['test']\n",
    "validation_output = data['valid_output']\n",
    "validation_6 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('6-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_7 = data['train']\n",
    "test_7 = data['test']\n",
    "validation_7 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Second version: reversing the order in F_c_w\n",
    "\n",
    "function build_context_count(count_tensor)\n",
    "    -- Ngram count (depend on w and context)\n",
    "    -- {'index1-...-indexN-1': {'indexN' : count}}\n",
    "    local F_c_w = {}\n",
    "    -- F_c dict (independent of w, only context based)\n",
    "    -- {index1-...-indexN-1 : count all words in c}\n",
    "    local F_c = {}\n",
    "    -- N_c dict (independent of w, only context based)\n",
    "    -- {index1-...-indexN-1 : count unique type of words in c}\n",
    "    local N_c = {}\n",
    "\n",
    "    local N = count_tensor:size(1)\n",
    "    local M = count_tensor:size(2)\n",
    "\n",
    "    for i=1, N do\n",
    "        indexN = count_tensor[{i,M-1}]\n",
    "        \n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(count_tensor[{i,1}])\n",
    "        for j=2, M - 2 do\n",
    "            indexes = indexes .. '-' .. tostring(count_tensor[{i,j}])\n",
    "        end\n",
    "        \n",
    "        -- Filling F_c_w\n",
    "        if F_c_w[indexes] == nil then\n",
    "            F_c_w[indexes] = {[indexN] = count_tensor[{i, M}]}\n",
    "        else\n",
    "            F_c_w[indexes][indexN] = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Updating F_c and F_c\n",
    "        if F_c[indexes] == nil then\n",
    "            F_c[indexes] = count_tensor[{i, M}]\n",
    "            N_c[indexes] = 1\n",
    "        else\n",
    "            F_c[indexes] = count_tensor[{i, M}] + F_c[indexes]\n",
    "            N_c[indexes] = 1 + N_c[indexes]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return F_c_w, F_c, N_c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Prediction with the MLE (with Laplace smoothing)\n",
    "\n",
    "function mle_proba(data, F_c_w, alpha)\n",
    "    local N = data:size(1)\n",
    "    local M = data:size(2)\n",
    "    -- Output format: distribution predicted for each N word along the\n",
    "    -- 50 possibilities\n",
    "    local distribution = torch.DoubleTensor(N, 50)\n",
    "\n",
    "    for i=1, N do\n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(data[{i,51}])\n",
    "        for j=52, M do\n",
    "            indexes = indexes .. '-' .. tostring(data[{i,j}])\n",
    "        end\n",
    "        \n",
    "        -- Look up in the dictionnary for the 50 possible ngrams asked\n",
    "        for j=1, 50 do\n",
    "            indexN = data[{i,j}]\n",
    "            if F_c_w[indexN] == nil or F_c_w[indexN][indexes] == nil then\n",
    "                distribution[{i,j}] = alpha\n",
    "            else\n",
    "                distribution[{i,j}] = F_c_w[indexN][indexes] + alpha\n",
    "            end\n",
    "        end\n",
    "        -- Debug: case where no n-gram were found (only when alpha=0.)\n",
    "        if distribution:narrow(1,i,1):sum(2)[{1,1}] == 0 then\n",
    "            -- Select the first one (most common)\n",
    "            distribution[{i,1}] = 1\n",
    "        end\n",
    "    end\n",
    "    -- normalization (ie we do the MLE given only the 50 possibilities)\n",
    "    distribution:cdiv(torch.expand(distribution:sum(2), N, 50))\n",
    "    \n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Loading train of the gram_size N\n",
    "function get_train(N)\n",
    "    local filename = N .. '-grams.hdf5'\n",
    "    --print(filename)\n",
    "    myFile = hdf5.open(filename,'r')\n",
    "    train = myFile:all()['train']\n",
    "    myFile:close()\n",
    "    return train\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Prediction with the MLE (with Laplace smoothing) and fix on never seen context\n",
    "\n",
    "function mle_proba_2(data, N, alpha)\n",
    "    local N_data = data:size(1)\n",
    "    local M = data:size(2)\n",
    "    -- Output format: distribution predicted for each N_data word along the\n",
    "    -- 50 possibilities\n",
    "    local distribution = torch.DoubleTensor(N_data, 50)\n",
    "    local gram_size\n",
    "    \n",
    "    -- Building the count matrix for each ngram size lower than N,\n",
    "    -- if need to reduce the context in case of unseen new context\n",
    "    local F_c_w_table = {}\n",
    "    for i=1,N do\n",
    "        train = get_train(i)\n",
    "        F_c_w, F_c, N_c = build_context_count_2(train)\n",
    "        F_c_w_table[i] = F_c_w\n",
    "    end\n",
    "\n",
    "    for i=1, N_data do\n",
    "        -- Initialize the Ngram_size\n",
    "        gram_size = N\n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(data[{i,51}])\n",
    "        for j=52, M do\n",
    "            indexes = indexes .. '-' .. tostring(data[{i,j}])\n",
    "        end\n",
    "        -- case context never seen before:\n",
    "        -- we look for the reduced context (ie index2-...-indexN-1)\n",
    "        if F_c_w_table[N][indexes] == nil then\n",
    "            -- Look for previous context\n",
    "            for k = N-1,1,-1 do\n",
    "                -- update gram_size\n",
    "                gram_size = k\n",
    "                -- Building index in reduced context\n",
    "                indexes_split = indexes:split(\"-\")\n",
    "                indexes = ''\n",
    "                for i= 1 + N - gram_size, N-1 do\n",
    "                    indexes = indexes .. indexes_split[i]\n",
    "                end\n",
    "                -- look if current context seen\n",
    "                if F_c_w_table[gram_size][indexes] ~= nil then\n",
    "                    break\n",
    "                end\n",
    "            end \n",
    "        end\n",
    "\n",
    "        -- Look up in the dictionnary for the 50 possible ngrams asked\n",
    "        for j=1, 50 do\n",
    "            indexN = data[{i,j}]\n",
    "            if gram_size == 1 then\n",
    "                -- Format of indexes is always string\n",
    "                indexes = tostring(indexN)\n",
    "            end\n",
    "            -- case word never seen in that context: smoothing only\n",
    "            if F_c_w_table[gram_size][indexes][indexN] == nil then\n",
    "                distribution[{i,j}] = alpha\n",
    "            else\n",
    "                distribution[{i,j}] = F_c_w_table[gram_size][indexes][indexN] + alpha\n",
    "            end\n",
    "        end\n",
    "\n",
    "        -- case uniform prediction (because no word present in current context)\n",
    "        if distribution:narrow(1,i,1):sum(2)[{1,1}] == 50 * alpha then\n",
    "            -- TODO: try with previous context\n",
    "            -- Select the first one (most common)\n",
    "            distribution[{i,1}] = 1 + alpha\n",
    "            --print('here', i, j, gram_size, indexes, indexN)\n",
    "        end\n",
    "    end\n",
    "    -- normalization (ie we do the MLE given only the 50 possibilities)\n",
    "    distribution:cdiv(torch.expand(distribution:sum(2), N_data, 50))\n",
    "    \n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function perplexity(distribution, true_words)\n",
    "    -- exp of the average of the cross entropy of the true word for each line\n",
    "    -- true words (N_words to predict, one hot true value among 50)\n",
    "    local perp = 0\n",
    "    local N = true_words:size(1)\n",
    "    for i = 1,N do\n",
    "        mm,aa = true_words[i]:max(1)\n",
    "        perp = perp + math.log(distribution[{i, aa[1]}])\n",
    "    end\n",
    "    perp = math.exp(- perp/N)\n",
    "    return perp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"-- Prediction with the MLE (with Laplace smoo...\"]:4: attempt to index local 'data' (a nil value)\nstack traceback:\n\t[string \"-- Prediction with the MLE (with Laplace smoo...\"]:4: in function 'mle_proba_2'\n\t[string \"-- Test of new mle version...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...colasdrizard/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x01075eebb0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"-- Prediction with the MLE (with Laplace smoo...\"]:4: attempt to index local 'data' (a nil value)\nstack traceback:\n\t[string \"-- Prediction with the MLE (with Laplace smoo...\"]:4: in function 'mle_proba_2'\n\t[string \"-- Test of new mle version...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...colasdrizard/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x01075eebb0"
     ]
    }
   ],
   "source": [
    "-- Test of new mle version\n",
    "distribution_mle_2_new = mle_proba_2(validation_2, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t8.8674198826233\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle_2_new, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t8.8680603647573\t\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_c_w, F_c, N_c = build_context_count(train_2)\n",
    "distribution_mle_2 = mle_proba(validation_2, F_c_w, 1)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle_2, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t17.059968522337\t\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Test of new mle version\n",
    "distribution_mle_3_new = mle_proba_2(validation_3, 3, 1)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle_3_new, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_c_w_table = {}\n",
    "for i=1,3 do\n",
    "    train = get_train(i)\n",
    "    F_c_w, F_c, N_c = build_context_count_2(train)\n",
    "    F_c_w_table[i] = F_c_w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 13\n",
       "  500   305  8425  5919  4090  4981  8611  8244  4804  2009  9910   471     6\n",
       "\n",
       "Columns 14 to 26\n",
       " 9904  1198   514  1061  5049  6289  8585  1697  4648  3676  7371  1867  4857\n",
       "\n",
       "Columns 27 to 39\n",
       " 3924  6939  8431  3086  4288  3138  4486  3949  3730  8607  7310  9582  9873\n",
       "\n",
       "Columns 40 to 51\n",
       " 6426  7482  1215  6676  9535  1417  6268  7276  1861  4232  7448  3884\n",
       "[torch.LongTensor of size 1x51]\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_2:narrow(1, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  6594 : 10\n",
       "}\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_c_w_table[1]['6594']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_c_w, F_c, N_c = build_context_count(train_3)\n",
    "distribution_mle_3 = mle_proba(validation_3, F_c_w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_c_w, F_c, N_c = build_context_count(train_4)\n",
    "distribution_mle_4 = mle_proba(validation_4, F_c_w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t8.8680603647573\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 3grams\t21.513973503713\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t29.942969799245\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Results on the validation set\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle_2, validation_output))\n",
    "print('Result on alpha smoothing 3grams', perplexity(distribution_mle_3, validation_output))\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_mle_4, validation_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Witten Bell Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_wb_line(N, entry, F_c_w_table, F_c_table, N_c_table, alpha)\n",
    "    -- Compute the interpolated Witten-Bell model where we jump tp lower\n",
    "    -- order models if the context count is 0 or all the words counts in that\n",
    "    -- context is 0 also.\n",
    "    --\n",
    "    -- Return vector (50) predicting the distribution from entry\n",
    "    -- N represent the Ngram size used in the prediction so context is N-1 gram\n",
    "    -- alpha is only used for the MLE without any context\n",
    "    local prediction = torch.zeros(50)\n",
    "    local indexN\n",
    "    -- case where computation only on the prior\n",
    "    if N == 1 then\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- Corner case when prediction on words not on the dict (case for <s>)\n",
    "            if F_c_w_table[1][tostring(indexN)] == nil then\n",
    "                prediction[j] = 0\n",
    "            else\n",
    "                prediction[j] = F_c_w_table[1][tostring(indexN)][indexN] + alpha\n",
    "            end\n",
    "        end\n",
    "        -- Normalizing\n",
    "        return prediction:div(prediction:sum(1)[1])\n",
    "    else\n",
    "        -- Compute the MLE for current N\n",
    "        -- context (at least with one element)\n",
    "        local indexes = tostring(entry[{1, entry:size(2)}])\n",
    "        for j=entry:size(2) - 1, entry:size(2) - 1 - (N-3), -1 do\n",
    "            indexes = tostring(entry[{1, j}]) .. '-' .. indexes\n",
    "        end\n",
    "        -- check if context is unseen, otherwise go to next context\n",
    "        if F_c_w_table[N][indexes] == nil then\n",
    "            --print('unseen context')\n",
    "            return compute_wb_line(N-1, entry, F_c_w_table, F_c_table, N_c_table, alpha)\n",
    "        end\n",
    "        -- Compute MLE for each word\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            if F_c_w_table[N][indexes][indexN] ~= nil then\n",
    "                prediction[j] = F_c_w_table[N][indexes][indexN]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        -- Check that MLE predicted at least one words, otherwise go to next context\n",
    "        if prediction:sum(1)[1] == 0 then\n",
    "            --print('unseen words')\n",
    "            return compute_wb_line(N-1, entry, F_c_w_table, F_c_table, N_c_table, alpha)\n",
    "        end\n",
    "        \n",
    "        -- Combining with next context\n",
    "        prediction:add(compute_wb_line(N-1, entry, F_c_w_table, F_c_table, N_c_table, alpha):mul(N_c_table[N][indexes])):div(N_c_table[N][indexes] + F_c_table[N][indexes])\n",
    "        -- Normalization\n",
    "        -- TODO: We normalize as we apply on a reduced dataest (50 words)\n",
    "        -- prediction:div(prediction:sum(1)[1])\n",
    "        return prediction\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Witten Bell: new version, computation done at once line by line\n",
    "--\n",
    "-- p_wb(w|c) = (F_c_w + N_c_. * p_wb(w|c'))/(N_c_. + F_c_.)\n",
    "function distribution_proba_WB(N, data, alpha)\n",
    "    local N_data = data:size(1)\n",
    "    local M = data:size(2)\n",
    "\n",
    "    -- Building the count matrix for each ngram size lower than N.\n",
    "    local F_c_w_table = {}\n",
    "    local F_c_table = {}\n",
    "    local N_c_table = {}\n",
    "    for i=1,N do\n",
    "        train = get_train(i)\n",
    "        F_c_w_table[i], F_c_table[i], N_c_table[i] = build_context_count(train)\n",
    "    end\n",
    "\n",
    "    -- Vector initialisation\n",
    "    local distribution = torch.zeros(N_data, 50)\n",
    "    for i=1,N_data do\n",
    "        -- Compute witten bell for the whole line i\n",
    "        distribution:narrow(1, i, 1):copy(compute_wb_line(N, data:narrow(1,i,1), F_c_w_table, F_c_table, N_c_table, alpha))\n",
    "    end\n",
    "    distribution:cdiv(distribution:sum(2):expand(distribution:size(1), distribution:size(2)))\n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unseen words\t\n"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = compute_wb_line(2, validation_3:narrow(1,6,1), F_c_w_table, F_c_table, N_c_table, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_2 = distribution_proba_WB(2, validation_2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 0\t6.023602585938\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 0.25\t6.0169008937015\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 0.5\t6.0114017902934\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 0.75\t6.0068267370782\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 1.25\t5.9997923223228\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 1.5\t5.997120094739\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 1.75\t5.9949125208341\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 2\t5.9931153986828\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 2.25\t5.9916848317703\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 2.5\t5.990584520467\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 2.75\t5.9897839308087\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 3\t5.9892570123825\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 3.25\t5.9889812736852\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 3.5\t5.9889370981486\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 3.75\t5.9891072269655\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 4\t5.9894763604856\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 4.5\t5.9907584281645\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams, alpha 5\t5.9926896910852\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Alpha validation ==> Best value is 3.5\n",
    "alphas = {0, 0.25, 0.5, 0.75, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.5, 5}\n",
    "\n",
    "for i=1, #alphas do\n",
    "    alpha = alphas[i]\n",
    "    distribution = distribution_proba_WB(1, validation_2, alpha)\n",
    "    print('Result on alpha smoothing 1grams, alpha '..alpha, perplexity(distribution, validation_output))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams\t6.023602585938\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t5.0433818281195\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 3grams\t4.5689460403288\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.4189330367685\t\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Without alpha\n",
    "distribution_1 = distribution_proba_WB(1, validation_2, 0)\n",
    "print('Result on alpha smoothing 1grams', perplexity(distribution_1, validation_output))\n",
    "distribution_2 = distribution_proba_WB(2, validation_2, 0)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_2, validation_output))\n",
    "distribution_3 = distribution_proba_WB(3, validation_3, 0)\n",
    "print('Result on alpha smoothing 3grams', perplexity(distribution_3, validation_output))\n",
    "distribution_4 = distribution_proba_WB(4, validation_4, 0)\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_4, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams\t6.0029978880708\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t5.026210590868\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 3grams\t4.5520551448742\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.4020577683054\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- ALpha is 1\n",
    "distribution_1 = distribution_proba_WB(1, validation_2, 1)\n",
    "print('Result on alpha smoothing 1grams', perplexity(distribution_1, validation_output))\n",
    "distribution_2 = distribution_proba_WB(2, validation_2, 1)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_2, validation_output))\n",
    "distribution_3 = distribution_proba_WB(3, validation_3, 1)\n",
    "print('Result on alpha smoothing 3grams', perplexity(distribution_3, validation_output))\n",
    "distribution_4 = distribution_proba_WB(4, validation_4, 1)\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_4, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5-grams.hdf5\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.3572276127565\t\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_5 = distribution_proba_WB(5, validation_5, 1)\n",
    "print('Result on alpha smoothing 5grams', perplexity(distribution_5, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6-grams.hdf5\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.3336779743836\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_6 = distribution_proba_WB(6, validation_6, 1)\n",
    "print('Result on alpha smoothing 6grams', perplexity(distribution_6, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 1grams\t5.9889370981486\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t5.0114654001341\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 3grams\t4.5351174868265\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams\t4.3843212693213\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- ALpha is the optimized one from validation on perplexity\n",
    "distribution_1 = distribution_proba_WB(1, validation_2, 3.5)\n",
    "print('Result on alpha smoothing 1grams', perplexity(distribution_1, validation_output))\n",
    "distribution_2 = distribution_proba_WB(2, validation_2, 3.5)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_2, validation_output))\n",
    "distribution_3 = distribution_proba_WB(3, validation_3, 3.5)\n",
    "print('Result on alpha smoothing 3grams', perplexity(distribution_3, validation_output))\n",
    "distribution_4 = distribution_proba_WB(4, validation_4, 3.5)\n",
    "print('Result on alpha smoothing 4grams', perplexity(distribution_4, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 5grams\t4.33902739378\t\n",
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6-grams.hdf5\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 6grams\t4.3152478550508\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_5 = distribution_proba_WB(5, validation_5, 3.5)\n",
    "print('Result on alpha smoothing 5grams', perplexity(distribution_5, validation_output))\n",
    "distribution_6 = distribution_proba_WB(6, validation_6, 3.5)\n",
    "print('Result on alpha smoothing 6grams', perplexity(distribution_6, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 6grams\t4.0878099970876\t\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_6 = distribution_proba_WB(6, validation_6, 3.5)\n",
    "print('Result on alpha smoothing 6grams', perplexity(distribution_6, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 6grams\t4.0815321835969\t\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_6_bis = distribution_proba_WB(6, validation_6, 5.5)\n",
    "print('Result on alpha smoothing 6grams', perplexity(distribution_6_bis, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 6grams\t4.0778034534132\t\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = distribution_proba_WB(6, validation_6, 7)\n",
    "print('Result on alpha smoothing 6grams', perplexity(distribution, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 6grams\t4.077354417045\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = distribution_proba_WB(6, validation_6, 8)\n",
    "print('Result on alpha smoothing 6grams', perplexity(distribution, validation_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Version tailored for modified Kneser-Ney\n",
    "\n",
    "function build_context_count_split(count_tensor, K)\n",
    "    -- count_tensor in format (N_words, N + 1):\n",
    "    -- col1, ..., colN = indexes for the Ngram, colN+1 = N_gram count\n",
    "    -- K: number of count separate cases (need K > 1, usually K = 3)\n",
    "    --\n",
    "    -- Ngram count (depend on w and context)\n",
    "    -- {'index1-...-indexN-1': {'indexN' : count}}\n",
    "    local F_c_w = {}\n",
    "    -- F_c dict (independent of w, only context based)\n",
    "    -- {index1-...-indexN-1 : count all words in c}\n",
    "    local F_c = {}\n",
    "    -- N_c dict (independent of w, only context based)\n",
    "    -- {k: {index1-...-indexN-1 : # words with count k in c}}\n",
    "    local N_c_split = {}\n",
    "    for j=1,K do\n",
    "        N_c_split[j] = {}\n",
    "    end\n",
    "    -- n_table: stores the total number of N_grams with exact number of occurences\n",
    "    -- stored in their key k: # N_grams with exactly k occurences\n",
    "    local n_table = {}\n",
    "    for j=1,K+1 do\n",
    "        n_table[j] = 0\n",
    "    end\n",
    "\n",
    "    local N = count_tensor:size(1)\n",
    "    local M = count_tensor:size(2)\n",
    "\n",
    "    for i=1, N do\n",
    "        local indexN = count_tensor[{i,M-1}]\n",
    "        \n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(count_tensor[{i,1}])\n",
    "        for j=2, M - 2 do\n",
    "            indexes = indexes .. '-' .. tostring(count_tensor[{i,j}])\n",
    "        end\n",
    "        \n",
    "        -- Filling F_c_w\n",
    "        if F_c_w[indexes] == nil then\n",
    "            F_c_w[indexes] = {[indexN] = count_tensor[{i, M}]}\n",
    "        else\n",
    "            F_c_w[indexes][indexN] = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Building the key to update the corresponding part of N_c_split\n",
    "        if count_tensor[{i, M}] > K then\n",
    "            key_N_c = K\n",
    "        else\n",
    "            key_N_c = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Updating N_c_split\n",
    "        if N_c_split[key_N_c][indexes] == nil then\n",
    "            N_c_split[key_N_c][indexes] = 1\n",
    "        else\n",
    "            N_c_split[key_N_c][indexes] = 1 + N_c_split[key_N_c][indexes]\n",
    "        end\n",
    "        \n",
    "        -- Updating F_c\n",
    "        if F_c[indexes] == nil then\n",
    "            F_c[indexes] = count_tensor[{i, M}]\n",
    "        else\n",
    "            F_c[indexes] = count_tensor[{i, M}] + F_c[indexes]\n",
    "        end\n",
    "        \n",
    "        -- Updating n_table\n",
    "        if count_tensor[{i, M}] <= K + 1 then\n",
    "            n_table[count_tensor[{i, M}]] = n_table[count_tensor[{i, M}]] + 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    -- Compute the D term from n_table\n",
    "    local Y = n_table[1]/(n_table[1] + 2*n_table[2])\n",
    "    local D = {}\n",
    "    for k=1,K do\n",
    "        D[k] = k - (1 + k)*Y*n_table[1 + k]/n_table[k]\n",
    "    end\n",
    "    -- Debugg\n",
    "    -- print(D)\n",
    "    return F_c_w, F_c, N_c_split, D\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_mkn_line(N, entry, F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K)\n",
    "    -- Compute the Modified Kneser Ney model where we jump to lower\n",
    "    -- order models if the context count is 0 or all the words counts in that\n",
    "    -- context is 0 also.\n",
    "    --\n",
    "    -- Return vector (50) predicting the distribution from entry\n",
    "    -- N represent the Ngram size used in the prediction so context is N-1 gram\n",
    "    -- alpha is only used for the MLE without any context\n",
    "    local prediction = torch.zeros(50)\n",
    "    local indexN\n",
    "    -- case where computation only on the prior\n",
    "    if N == 1 then\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- Corner case when prediction on words not on the dict (case for <s>)\n",
    "            if F_c_w_table[1][tostring(indexN)] == nil then\n",
    "                prediction[j] = 0\n",
    "            else\n",
    "                prediction[j] = F_c_w_table[1][tostring(indexN)][indexN] + alpha\n",
    "            end\n",
    "        end\n",
    "        -- Normalizing\n",
    "        return prediction:div(prediction:sum(1)[1])\n",
    "    else\n",
    "        -- Compute the MLE for current N\n",
    "        -- context (at least with one element)\n",
    "        local indexes = tostring(entry[{1, entry:size(2)}])\n",
    "        for j=entry:size(2) - 1, entry:size(2) - 1 - (N-3), -1 do\n",
    "            indexes = tostring(entry[{1, j}]) .. '-' .. indexes\n",
    "        end\n",
    "        -- check if context is unseen, otherwise go to next context\n",
    "        if F_c_w_table[N][indexes] == nil then\n",
    "            --print('unseen context')\n",
    "            return compute_mkn_line(N-1, entry, F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K)\n",
    "        end\n",
    "        -- Compute curent order level with modified absolute discouting for each word\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- case word seen\n",
    "            if F_c_w_table[N][indexes][indexN] ~= nil then\n",
    "                -- Building the key for the different case of absolute discounting\n",
    "                if F_c_w_table[N][indexes][indexN] > K then\n",
    "                    key_N_c = K\n",
    "                else\n",
    "                    key_N_c = F_c_w_table[N][indexes][indexN]\n",
    "                end\n",
    "                prediction[j] = F_c_w_table[N][indexes][indexN] - D_table[N][key_N_c]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        -- Check that MLE predicted at least one words, otherwise go to next context\n",
    "        if prediction:sum(1)[1] == 0 then\n",
    "            --print('unseen words')\n",
    "            return compute_mkn_line(N-1, entry, F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K)\n",
    "        end\n",
    "        \n",
    "        -- Computing factor of lower order model (no denominator because we normalize afterwards)\n",
    "        local gamma = 0\n",
    "        for k=1,K do\n",
    "            if N_c_split_table[N][k][indexes] ~= nil then\n",
    "                gamma = gamma + D_table[N][k]*N_c_split_table[N][k][indexes]\n",
    "            end\n",
    "        end\n",
    "        if gamma == 0 then\n",
    "            print('gamma error')\n",
    "        end\n",
    "        -- Combining with next context\n",
    "        prediction:add(compute_mkn_line(N-1, entry, F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K):mul(gamma)):div(F_c_table[N][indexes])\n",
    "        -- Normalization\n",
    "        -- TODO: why??? We normalize at the end\n",
    "        -- prediction:div(prediction:sum(1)[1])\n",
    "        return prediction\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Modified Kneser Ney: computation done at once line by line\n",
    "--\n",
    "-- p_wb(w|c) = (F_c_w + N_c_. * p_wb(w|c'))/(N_c_. + F_c_.)\n",
    "function distribution_proba_mKN(N, data, alpha, K)\n",
    "    local N_data = data:size(1)\n",
    "    local M = data:size(2)\n",
    "\n",
    "    -- Building the count matrix for each ngram size lower than N.\n",
    "    local F_c_w_table = {}\n",
    "    local F_c_table = {}\n",
    "    local N_c_split_table = {}\n",
    "    local D_table = {}\n",
    "    for i=1,N do\n",
    "        train = get_train(i)\n",
    "        F_c_w_table[i], F_c_table[i], N_c_split_table[i], D_table[i] = build_context_count_split(train, K)\n",
    "    end\n",
    "\n",
    "    -- Vector initialisation\n",
    "    local distribution = torch.zeros(N_data, 50)\n",
    "    for i=1,N_data do\n",
    "        -- Compute witten bell for the whole line i\n",
    "        distribution:narrow(1, i, 1):copy(compute_mkn_line(N, data:narrow(1,i,1), F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K))\n",
    "    end\n",
    "    distribution:cdiv(distribution:sum(2):expand(distribution:size(1), distribution:size(2)))\n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 0.51724137931034\n",
       "  2 : -0.77093596059113\n",
       "  3 : -2.9586206896552\n",
       "}\n",
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 0.71157945361249\n",
       "  2 : 1.0881976981224\n",
       "  3 : 1.446606938822\n",
       "}\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- DEBUG\n",
    "K = 3\n",
    "F_c_w_table = {}\n",
    "F_c_table = {}\n",
    "N_c_split_table = {}\n",
    "D_table = {}\n",
    "n_table = {}\n",
    "for i=1,2 do\n",
    "    train = get_train(i)\n",
    "    F_c_w_table[i], F_c_table[i], N_c_split_table[i], D_table[i] = build_context_count_split(train, K)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unseen words\t\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = compute_mkn_line(2, validation_2:narrow(1, 3, 1), F_c_w_table, F_c_table, N_c_split_table, D_table, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-grams.hdf5\t\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 0.51724137931034\n",
       "  2 : -0.77093596059113\n",
       "  3 : -2.9586206896552\n",
       "}\n",
       "2-grams.hdf5\t\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 0.71157945361249\n",
       "  2 : 1.0881976981224\n",
       "  3 : 1.446606938822\n",
       "}\n",
       "3-grams.hdf5\t\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 0.84848390111054\n",
       "  2 : 1.2170130335274\n",
       "  3 : 1.3775282874792\n",
       "}\n",
       "4-grams.hdf5\t\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 0.92110201391759\n",
       "  2 : 1.2968014992496\n",
       "  3 : 1.2971783075115\n",
       "}\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_4 = distribution_proba_mKN(4, validation_4, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 0\t5.1405210180739\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 0\t4.3017358601582\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 0.25\t5.1345908364747\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 0.25\t4.2960411418246\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 0.5\t5.1296366428649\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 0.5\t4.2911559318662\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 0.75\t5.1254257965317\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 0.75\t4.2868861013792\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 1.25\t5.1186904989754\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 1.25\t4.2797397749426\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 1.5\t5.1159912624387\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 1.5\t4.2767180019646\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 1.75\t5.1136572787308\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 1.75\t4.2739971781502\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 2\t5.1116447378085\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 2\t4.2715410478424\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 2.25\t5.1099183414695\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 2.25\t4.269320445975\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 2.5\t5.1084490354619\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 2.5\t4.2673114027822\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 2.75\t5.1072124819481\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 2.75\t4.2654938680566\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 3\t5.1061879941292\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 3\t4.2638508220318\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 3.25\t5.105357770893\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 3.25\t4.2623676371534\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 3.5\t5.10470633285\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 3.5\t4.2610316081853\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 3.75\t5.1042200975229\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 3.75\t4.2598315985707\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 4\t5.1038870531415\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 4\t4.2587577691279\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 4.5\t5.1036388679001\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 4.5\t4.2569545548285\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams, alpha 5\t5.1038886350349\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 4grams, alpha 5\t4.2555621716216\t\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Cross validation on alpha on higher ngram size\n",
    "\n",
    "-- Alpha validation ==> Best value is 3.5\n",
    "alphas = {0, 0.25, 0.5, 0.75, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.5, 5}\n",
    "K = 3\n",
    "for i=1, #alphas do\n",
    "    alpha = alphas[i]\n",
    "    distribution_2 = distribution_proba_mKN(2, validation_2, alpha, K)\n",
    "    print('Result on alpha smoothing 2grams, alpha '..alpha, perplexity(distribution_2, validation_output))\n",
    "    distribution_4 = distribution_proba_mKN(4, validation_4, alpha, K)\n",
    "    print('Result on alpha smoothing 4grams, alpha '..alpha, perplexity(distribution_4, validation_output))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 3.5, K2\t4.2193148722749\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 5.5, K2\t4.2125914837561\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 7, K2\t4.2110764358717\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 3.5, K3\t4.2610316081853\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 5.5, K3\t4.2545317703483\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 7, K3\t4.2532371481075\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 3.5, K4\t4.2868918144915\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 5.5, K4\t4.2805566409014\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 7, K4\t"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.2794163213392\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 3.5, K5\t4.3102393822349\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 5.5, K5\t4.3040497343289\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Result on 4grams, alpha 7, K5\t4.3030462686139\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Cross validation on K\n",
    "\n",
    "-- Alpha validation ==> Best value is 3.5\n",
    "Ks = {2, 3, 4, 5}\n",
    "--alphas = {0, 2, 3.5, 5, 7}\n",
    "for i=1, #Ks do\n",
    "    K = Ks[i]\n",
    "    distribution_4 = distribution_proba_mKN(4, validation_4, 3.5, K)\n",
    "    print('Result on 4grams, alpha 3.5, K'..K, perplexity(distribution_4, validation_output))\n",
    "    distribution_4_bis = distribution_proba_mKN(4, validation_4, 5.5, K)\n",
    "    print('Result on 4grams, alpha 5.5, K'..K, perplexity(distribution_4_bis, validation_output))\n",
    "    distribution_4_ter = distribution_proba_mKN(4, validation_4, 7, K)\n",
    "    print('Result on 4grams, alpha 7, K'..K, perplexity(distribution_4_ter, validation_output))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on 6grams, alpha 7, K 2\t4.2067088977711\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Test on current best configuration with 6-grams (alpha = 7, K = 2)\n",
    "distribution = distribution_proba_mKN(6, validation_6, 7, 2)\n",
    "print('Result on 6grams, alpha 7, K 2', perplexity(distribution, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Applying on test\n",
    "distribution_test = distribution_proba_WB(6, test_6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Saving the current best model\n",
    "myFile = hdf5.open('pred_test_wb_fnorm_6', 'w')\n",
    "myFile:write('distribution', distribution_test)\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m, true_output = validation_output:max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_accuracy(pred, true_output)\n",
    "    max,argmax = pred:max(2)\n",
    "    acc = 0\n",
    "    for i = 1, true_output:size(1) do\n",
    "        if argmax[i][1] == true_output[i][1] then\n",
    "            acc = acc + 1\n",
    "        end\n",
    "    end\n",
    "    score = acc/true_output:size(1)\n",
    "    \n",
    "    return score\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing \t0.59614243323442\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on alpha smoothing ', compute_accuracy(distribution_2, true_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
