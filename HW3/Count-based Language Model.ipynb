{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Train format is (number of Ngrams, Ngram_size + 1) with last\n",
    "-- col the count of the N_gram of the line\n",
    "\n",
    "-- Validation format is (number of words to predict, 50 + Ngrams_size -1)\n",
    "-- where the 50 columns stands for the 50 words possibilities in the prediction,\n",
    "-- the next col stands for the current context (goal is to predict the Nth word)\n",
    "\n",
    "myFile = hdf5.open('1-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_1 = data['train']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('2-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_2 = data['train']\n",
    "validation_2 = data['valid']\n",
    "validation_output = data['valid_output']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('3-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_3 = data['train']\n",
    "test_3 = data['test']\n",
    "validation_3 = data['valid']\n",
    "myFile:close()\n",
    "\n",
    "myFile = hdf5.open('4-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_4 = data['train']\n",
    "validation_4 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('5-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_5 = data['train']\n",
    "test_5 = data['test']\n",
    "validation_5 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('6-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_6 = data['train']\n",
    "test_6 = data['test']\n",
    "validation_output = data['valid_output']\n",
    "validation_6 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('7-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "train_7 = data['train']\n",
    "test_7 = data['test']\n",
    "validation_7 = data['valid']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Loading train of the gram_size N\n",
    "function get_train(N)\n",
    "    local filename = N .. '-grams.hdf5'\n",
    "    --print(filename)\n",
    "    myFile = hdf5.open(filename,'r')\n",
    "    train = myFile:all()['train']\n",
    "    myFile:close()\n",
    "    return train\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function perplexity(distribution, true_words)\n",
    "    -- exp of the average of the cross entropy of the true word for each line\n",
    "    -- true words (N_words to predict, one hot true value among 50)\n",
    "    local perp = 0\n",
    "    local N = true_words:size(1)\n",
    "    for i = 1,N do\n",
    "        mm,aa = true_words[i]:max(1)\n",
    "        perp = perp + math.log(distribution[{i, aa[1]}])\n",
    "    end\n",
    "    perp = math.exp(- perp/N)\n",
    "    return perp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Second version: reversing the order in F_c_w\n",
    "\n",
    "function build_context_count(count_tensor)\n",
    "    local indexes\n",
    "    local indexN\n",
    "    -- Ngram count (depend on w and context)\n",
    "    -- {'index1-...-indexN-1': {'indexN' : count}}\n",
    "    local F_c_w = {}\n",
    "    -- F_c dict (independent of w, only context based)\n",
    "    -- {index1-...-indexN-1 : count all words in c}\n",
    "    local F_c = {}\n",
    "    -- N_c dict (independent of w, only context based)\n",
    "    -- {index1-...-indexN-1 : count unique type of words in c}\n",
    "    local N_c = {}\n",
    "\n",
    "    local N = count_tensor:size(1)\n",
    "    local M = count_tensor:size(2)\n",
    "\n",
    "    for i=1, N do\n",
    "        indexN = count_tensor[{i,M-1}]\n",
    "        \n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(count_tensor[{i,1}])\n",
    "        for j=2, M - 2 do\n",
    "            indexes = indexes .. '-' .. tostring(count_tensor[{i,j}])\n",
    "        end\n",
    "        \n",
    "        -- Filling F_c_w\n",
    "        if F_c_w[indexes] == nil then\n",
    "            F_c_w[indexes] = {[indexN] = count_tensor[{i, M}]}\n",
    "        else\n",
    "            F_c_w[indexes][indexN] = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Updating F_c and F_c\n",
    "        if F_c[indexes] == nil then\n",
    "            F_c[indexes] = count_tensor[{i, M}]\n",
    "            N_c[indexes] = 1\n",
    "        else\n",
    "            F_c[indexes] = count_tensor[{i, M}] + F_c[indexes]\n",
    "            N_c[indexes] = 1 + N_c[indexes]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return F_c_w, F_c, N_c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_mle_line(N, entry, F_c_w, alpha)\n",
    "    -- Compute the maximum likelihood estimation with alpha smoothing on the \n",
    "    -- input in entry, \n",
    "    --\n",
    "    -- Return vector (50) predicting the distribution from entry\n",
    "    -- N represent the Ngram size used in the prediction so context is N-1 gram\n",
    "    local prediction = torch.zeros(50)\n",
    "    local indexN\n",
    "    \n",
    "    -- context (at least with one element)\n",
    "    local indexes = tostring(entry[{1, entry:size(2)}])\n",
    "    for j=entry:size(2) - 1, entry:size(2) - 1 - (N-3), -1 do\n",
    "        indexes = tostring(entry[{1, j}]) .. '-' .. indexes\n",
    "    end\n",
    "    -- check if context is unseen, otherwise go to next context\n",
    "    if F_c_w[indexes] == nil then\n",
    "        --print('unseen context')\n",
    "        prediction:fill(alpha)\n",
    "    else\n",
    "        -- Compute MLE for each word\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            if F_c_w[indexes][indexN] ~= nil then\n",
    "                prediction[j] = F_c_w[indexes][indexN] + alpha\n",
    "            else\n",
    "                --print('unseen word')\n",
    "                prediction[j] = alpha\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return prediction:div(prediction:sum())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Prediction with the MLE (with Laplace smoothing, no back-off and interpolation)\n",
    "\n",
    "function mle_proba(N, data, alpha)\n",
    "    -- Output format: distribution predicted for each N word along the\n",
    "    -- 50 possibilities\n",
    "    local N_data = data:size(1)\n",
    "    \n",
    "    -- Train model\n",
    "    local train = get_train(N)\n",
    "    local F_c_w = build_context_count(train)\n",
    "\n",
    "    -- Prediction\n",
    "    local distribution = torch.zeros(N_data, 50)\n",
    "    for i=1, N_data do\n",
    "        distribution:narrow(1, i, 1):copy(compute_mle_line(N, data:narrow(1,i,1), F_c_w, alpha))\n",
    "    end\n",
    "    \n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Test of mle version\n",
    "distribution_mle = mle_proba(2, validation_2, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t6.0175980155407\t\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mle, validation_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Witten Bell Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_wb_line(N, entry, F_c_w_table, alpha)\n",
    "    -- Compute the interpolated Witten-Bell model where we jump tp lower\n",
    "    -- order models if the context count is 0 or all the words counts in that\n",
    "    -- context is 0 also.\n",
    "    --\n",
    "    -- Return vector (50) predicting the distribution from entry\n",
    "    -- N represent the Ngram size used in the prediction so context is N-1 gram\n",
    "    -- alpha is only used for the MLE without any context\n",
    "    --\n",
    "    -- NB: the normalization is done based on the words contained in the first 50\n",
    "    -- columns of the entry as we are building a distribution on a sub sample of a\n",
    "    -- dictionnary (so we are using the count only of these words to normalize).\n",
    "    -- Hence the variable denom and N_c_local\n",
    "    local prediction = torch.zeros(50)\n",
    "    local indexN\n",
    "    local indexes\n",
    "    local denom\n",
    "    local N_c_local \n",
    "    \n",
    "    -- case where computation only on the prior\n",
    "    if N == 1 then\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- Corner case when prediction on words not on the dict (case for <s>)\n",
    "            if F_c_w_table[1][tostring(indexN)] == nil then\n",
    "                prediction[j] = 0\n",
    "            else\n",
    "                prediction[j] = F_c_w_table[1][tostring(indexN)][indexN] + alpha\n",
    "            end\n",
    "        end\n",
    "        -- Normalizing\n",
    "        return prediction:div(prediction:sum(1)[1])\n",
    "    else\n",
    "        -- Compute the MLE for current N\n",
    "        -- context (at least with one element)\n",
    "        indexes = tostring(entry[{1, entry:size(2)}])\n",
    "        for j=entry:size(2) - 1, entry:size(2) - 1 - (N-3), -1 do\n",
    "            indexes = tostring(entry[{1, j}]) .. '-' .. indexes\n",
    "        end\n",
    "        \n",
    "        -- check if context is unseen, otherwise go to next context\n",
    "        if F_c_w_table[N][indexes] == nil then\n",
    "            --print('unseen context')\n",
    "            return compute_wb_line(N-1, entry, F_c_w_table, alpha)\n",
    "        end\n",
    "        \n",
    "        -- local variable initialization\n",
    "        denom = 0\n",
    "        N_c_local = 0\n",
    "        -- Compute MLE for each word\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            if F_c_w_table[N][indexes][indexN] ~= nil then\n",
    "                prediction[j] = F_c_w_table[N][indexes][indexN]\n",
    "                denom = denom + F_c_w_table[N][indexes][indexN] + 1\n",
    "                N_c_local = N_c_local + 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        -- Check that MLE predicted at least one words, otherwise go to next context\n",
    "        if prediction:sum(1)[1] == 0 then\n",
    "            --print('unseen words')\n",
    "            return compute_wb_line(N-1, entry, F_c_w_table, alpha)\n",
    "        end\n",
    "        \n",
    "        -- Combining with next context\n",
    "        prediction:add(compute_wb_line(N-1, entry, F_c_w_table, alpha):mul(N_c_local)):div(denom)\n",
    "        -- Normalization\n",
    "        -- TODO: We normalize as we apply on a reduced dataest (50 words)\n",
    "        -- prediction:div(prediction:sum(1)[1])\n",
    "        return prediction\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Witten Bell: new version, computation done at once line by line\n",
    "--\n",
    "-- p_wb(w|c) = (F_c_w + N_c_. * p_wb(w|c'))/(N_c_. + F_c_.)\n",
    "function distribution_proba_WB(N, data, alpha)\n",
    "    local N_data = data:size(1)\n",
    "    local M = data:size(2)\n",
    "\n",
    "    -- Building the count matrix for each ngram size lower than N.\n",
    "    local F_c_w_table = {}\n",
    "    for i=1,N do\n",
    "        train = get_train(i)\n",
    "        F_c_w_table[i] = build_context_count(train)\n",
    "    end\n",
    "\n",
    "    -- Vector initialisation\n",
    "    local distribution = torch.zeros(N_data, 50)\n",
    "    for i=1,N_data do\n",
    "        -- Compute witten bell for the whole line i\n",
    "        distribution:narrow(1, i, 1):copy(compute_wb_line(N, data:narrow(1,i,1), F_c_w_table, alpha))\n",
    "    end\n",
    "    --distribution:cdiv(distribution:sum(2):expand(distribution:size(1), distribution:size(2)))\n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distribution_wb = distribution_proba_WB(2, validation_2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t3.6367209232302\t\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on alpha smoothing 2grams', perplexity(distribution_wb, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t3.6201823235489\t\n"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_wb = distribution_proba_WB(6, validation_6, 10)\n",
    "print('Result on alpha smoothing 2grams', perplexity(distribution_wb, validation_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Version tailored for modified Kneser-Ney\n",
    "\n",
    "function build_context_count_split(count_tensor, K)\n",
    "    -- count_tensor in format (N_words, N + 1):\n",
    "    -- col1, ..., colN = indexes for the Ngram, colN+1 = N_gram count\n",
    "    -- K: number of count separate cases (need K > 1, usually K = 3)\n",
    "    --\n",
    "    -- Ngram count (depend on w and context)\n",
    "    -- {'index1-...-indexN-1': {'indexN' : count}}\n",
    "    local F_c_w = {}\n",
    "    -- F_c dict (independent of w, only context based)\n",
    "    -- {index1-...-indexN-1 : count all words in c}\n",
    "    local F_c = {}\n",
    "    -- N_c dict (independent of w, only context based)\n",
    "    -- {k: {index1-...-indexN-1 : # words with count k in c}}\n",
    "    local N_c_split = {}\n",
    "    for j=1,K do\n",
    "        N_c_split[j] = {}\n",
    "    end\n",
    "    -- n_table: stores the total number of N_grams with exact number of occurences\n",
    "    -- stored in their key k: # N_grams with exactly k occurences\n",
    "    local n_table = {}\n",
    "    for j=1,K+1 do\n",
    "        n_table[j] = 0\n",
    "    end\n",
    "\n",
    "    local N = count_tensor:size(1)\n",
    "    local M = count_tensor:size(2)\n",
    "\n",
    "    for i=1, N do\n",
    "        local indexN = count_tensor[{i,M-1}]\n",
    "        \n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(count_tensor[{i,1}])\n",
    "        for j=2, M - 2 do\n",
    "            indexes = indexes .. '-' .. tostring(count_tensor[{i,j}])\n",
    "        end\n",
    "        \n",
    "        -- Filling F_c_w\n",
    "        if F_c_w[indexes] == nil then\n",
    "            F_c_w[indexes] = {[indexN] = count_tensor[{i, M}]}\n",
    "        else\n",
    "            F_c_w[indexes][indexN] = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Building the key to update the corresponding part of N_c_split\n",
    "        if count_tensor[{i, M}] > K then\n",
    "            key_N_c = K\n",
    "        else\n",
    "            key_N_c = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Updating N_c_split\n",
    "        if N_c_split[key_N_c][indexes] == nil then\n",
    "            N_c_split[key_N_c][indexes] = 1\n",
    "        else\n",
    "            N_c_split[key_N_c][indexes] = 1 + N_c_split[key_N_c][indexes]\n",
    "        end\n",
    "        \n",
    "        -- Updating F_c\n",
    "        if F_c[indexes] == nil then\n",
    "            F_c[indexes] = count_tensor[{i, M}]\n",
    "        else\n",
    "            F_c[indexes] = count_tensor[{i, M}] + F_c[indexes]\n",
    "        end\n",
    "        \n",
    "        -- Updating n_table\n",
    "        if count_tensor[{i, M}] <= K + 1 then\n",
    "            n_table[count_tensor[{i, M}]] = n_table[count_tensor[{i, M}]] + 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    -- Compute the D term from n_table\n",
    "    local Y = n_table[1]/(n_table[1] + 2*n_table[2])\n",
    "    local D = {}\n",
    "    for k=1,K do\n",
    "        D[k] = k - (1 + k)*Y*n_table[1 + k]/n_table[k]\n",
    "    end\n",
    "    -- Debugg\n",
    "    -- print(D)\n",
    "    return F_c_w, F_c, N_c_split, D\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_mkn_line(N, entry, F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K)\n",
    "    -- Compute the Modified Kneser Ney model where we jump to lower\n",
    "    -- order models if the context count is 0 or all the words counts in that\n",
    "    -- context is 0 also.\n",
    "    --\n",
    "    -- Return vector (50) predicting the distribution from entry\n",
    "    -- N represent the Ngram size used in the prediction so context is N-1 gram\n",
    "    -- alpha is only used for the MLE without any context\n",
    "    --\n",
    "    -- No local normalization, should be called to predict on the whole dictionnary\n",
    "    local prediction = torch.zeros(50)\n",
    "    local indexN\n",
    "    -- case where computation only on the prior\n",
    "    if N == 1 then\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- Corner case when prediction on words not on the dict (case for <s>)\n",
    "            if F_c_w_table[1][tostring(indexN)] == nil then\n",
    "                prediction[j] = 0\n",
    "            else\n",
    "                prediction[j] = F_c_w_table[1][tostring(indexN)][indexN] + alpha\n",
    "            end\n",
    "        end\n",
    "        -- Normalizing\n",
    "        return prediction:div(prediction:sum(1)[1])\n",
    "    else\n",
    "        -- Compute the MLE for current N\n",
    "        -- context (at least with one element)\n",
    "        local indexes = tostring(entry[{1, entry:size(2)}])\n",
    "        for j=entry:size(2) - 1, entry:size(2) - 1 - (N-3), -1 do\n",
    "            indexes = tostring(entry[{1, j}]) .. '-' .. indexes\n",
    "        end\n",
    "        -- check if context is unseen, otherwise go to next context\n",
    "        if F_c_w_table[N][indexes] == nil then\n",
    "            --print('unseen context')\n",
    "            return compute_mkn_line(N-1, entry, F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K)\n",
    "        end\n",
    "        -- Compute curent order level with modified absolute discouting for each word\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- case word seen\n",
    "            if F_c_w_table[N][indexes][indexN] ~= nil then\n",
    "                -- Building the key for the different case of absolute discounting\n",
    "                if F_c_w_table[N][indexes][indexN] > K then\n",
    "                    key_N_c = K\n",
    "                else\n",
    "                    key_N_c = F_c_w_table[N][indexes][indexN]\n",
    "                end\n",
    "                prediction[j] = F_c_w_table[N][indexes][indexN] - D_table[N][key_N_c]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        -- Check that MLE predicted at least one words, otherwise go to next context\n",
    "        if prediction:sum(1)[1] == 0 then\n",
    "            --print('unseen words')\n",
    "            return compute_mkn_line(N-1, entry, F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K)\n",
    "        end\n",
    "        \n",
    "        -- Computing factor of lower order model (no denominator because we normalize afterwards)\n",
    "        local gamma = 0\n",
    "        for k=1,K do\n",
    "            if N_c_split_table[N][k][indexes] ~= nil then\n",
    "                gamma = gamma + D_table[N][k]*N_c_split_table[N][k][indexes]\n",
    "            end\n",
    "        end\n",
    "        if gamma == 0 then\n",
    "            print('gamma error')\n",
    "        end\n",
    "        -- Combining with next context\n",
    "        prediction:add(compute_mkn_line(N-1, entry, F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K):mul(gamma)):div(F_c_table[N][indexes])\n",
    "        -- Normalization\n",
    "        -- TODO: why??? We normalize at the end\n",
    "        -- prediction:div(prediction:sum(1)[1])\n",
    "        return prediction\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Modified Kneser Ney: computation done at once line by line\n",
    "--\n",
    "-- p_wb(w|c) = (F_c_w + N_c_. * p_wb(w|c'))/(N_c_. + F_c_.)\n",
    "function distribution_proba_mKN(N, data, alpha, K)\n",
    "    local N_data = data:size(1)\n",
    "    local M = data:size(2)\n",
    "\n",
    "    -- Building the count matrix for each ngram size lower than N.\n",
    "    local F_c_w_table = {}\n",
    "    local F_c_table = {}\n",
    "    local N_c_split_table = {}\n",
    "    local D_table = {}\n",
    "    for i=1,N do\n",
    "        train = get_train(i)\n",
    "        F_c_w_table[i], F_c_table[i], N_c_split_table[i], D_table[i] = build_context_count_split(train, K)\n",
    "    end\n",
    "\n",
    "    -- Vector initialisation\n",
    "    local distribution = torch.zeros(N_data, 50)\n",
    "    for i=1,N_data do\n",
    "        -- Compute witten bell for the whole line i\n",
    "        distribution:narrow(1, i, 1):copy(compute_mkn_line(N, data:narrow(1,i,1), F_c_w_table, F_c_table, N_c_split_table, D_table, alpha, K))\n",
    "    end\n",
    "    distribution:cdiv(distribution:sum(2):expand(distribution:size(1), distribution:size(2)))\n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution_mkn = distribution_proba_mKN(4, validation_4, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing 2grams\t4.283108778554\t\n"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on alpha smoothing 2grams', perplexity(distribution_mkn, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on 6grams, alpha 7, K 2\t4.2067088977711\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Test on current best configuration with 6-grams (alpha = 7, K = 2)\n",
    "distribution = distribution_proba_mKN(6, validation_6, 7, 2)\n",
    "print('Result on 6grams, alpha 7, K 2', perplexity(distribution, validation_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# New local normalization on mKN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Version tailored for modified Kneser-Ney:\n",
    "-- Modif: now we enable a local computation of D\n",
    "-- (that will be based on the sub vocabulary used in the validation and tesst)\n",
    "\n",
    "function build_context_count_split2(count_tensor, K)\n",
    "    -- count_tensor in format (N_words, N + 1):\n",
    "    -- col1, ..., colN = indexes for the Ngram, colN+1 = N_gram count\n",
    "    -- K: number of count separate cases (need K > 1, usually K = 3)\n",
    "    --\n",
    "    -- Ngram count (depend on w and context)\n",
    "    -- {'index1-...-indexN-1': {'indexN' : count}}\n",
    "    local F_c_w = {}\n",
    "    -- n_table: stores the total number of N_grams ending with indexN\n",
    "    -- with exact number of occurences stored in their key k:\n",
    "    -- {k : {'indexN': # N_grams ending with indexN with exactly k occurences}}\n",
    "    local n_table = {}\n",
    "    for j=1,K+1 do\n",
    "        n_table[j] = {}\n",
    "    end\n",
    "\n",
    "    local N = count_tensor:size(1)\n",
    "    local M = count_tensor:size(2)\n",
    "\n",
    "    for i=1, N do\n",
    "        local indexN = count_tensor[{i,M-1}]\n",
    "        \n",
    "        -- build the key index1-...-indexN-1\n",
    "        indexes = tostring(count_tensor[{i,1}])\n",
    "        for j=2, M - 2 do\n",
    "            indexes = indexes .. '-' .. tostring(count_tensor[{i,j}])\n",
    "        end\n",
    "        \n",
    "        -- Filling F_c_w\n",
    "        if F_c_w[indexes] == nil then\n",
    "            F_c_w[indexes] = {[indexN] = count_tensor[{i, M}]}\n",
    "        else\n",
    "            F_c_w[indexes][indexN] = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Building the key to update the corresponding part of n_table\n",
    "        if count_tensor[{i, M}] > K then\n",
    "            key_N_c = K\n",
    "        else\n",
    "            key_N_c = count_tensor[{i, M}]\n",
    "        end\n",
    "        \n",
    "        -- Updating n_table\n",
    "        if count_tensor[{i, M}] <= K + 1 then\n",
    "            if n_table[count_tensor[{i, M}]][indexN] == nil then\n",
    "                n_table[count_tensor[{i, M}]][indexN] = 1\n",
    "            else\n",
    "                n_table[count_tensor[{i, M}]][indexN] = n_table[count_tensor[{i, M}]][indexN] + 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return F_c_w, n_table\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- V2: with local normalization on the validation sub vocabulary\n",
    "\n",
    "function compute_mkn_line2(N, entry, F_c_w_table, n_table, alpha, K, D)\n",
    "    -- Compute the Modified Kneser Ney model where we jump to lower\n",
    "    -- order models if the context count is 0 or all the words counts in that\n",
    "    -- context is 0 also.\n",
    "    --\n",
    "    -- Return vector (50) predicting the distribution from entry\n",
    "    -- N represent the Ngram size used in the prediction so context is N-1 gram\n",
    "    -- alpha is only used for the MLE without any context\n",
    "    local prediction = torch.zeros(50)\n",
    "    local indexN\n",
    "    local F_local\n",
    "    local N_c_local = {}\n",
    "    for k=1,K do\n",
    "        N_c_local[k] = 0\n",
    "    end\n",
    "    local n_table_local = {}\n",
    "    for k=1,K+1 do\n",
    "        n_table_local[k] = 0\n",
    "    end\n",
    "    \n",
    "    -- case where computation only on the prior\n",
    "    if N == 1 then\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- Corner case when prediction on words not on the dict (case for <s>)\n",
    "            if F_c_w_table[1][tostring(indexN)] == nil then\n",
    "                prediction[j] = 0\n",
    "            else\n",
    "                prediction[j] = F_c_w_table[1][tostring(indexN)][indexN] + alpha\n",
    "            end\n",
    "        end\n",
    "        -- Normalizing\n",
    "        return prediction:div(prediction:sum(1)[1])\n",
    "    else\n",
    "        -- Compute the MLE for current N\n",
    "        -- context (at least with one element)\n",
    "        local indexes = tostring(entry[{1, entry:size(2)}])\n",
    "        for j=entry:size(2) - 1, entry:size(2) - 1 - (N-3), -1 do\n",
    "            indexes = tostring(entry[{1, j}]) .. '-' .. indexes\n",
    "        end\n",
    "        -- check if context is unseen, otherwise go to next context\n",
    "        if F_c_w_table[N][indexes] == nil then\n",
    "            --print('unseen context')\n",
    "            return compute_mkn_line2(N-1, entry, F_c_w_table, n_table, alpha, K, D)\n",
    "        end\n",
    "\n",
    "        -- Building local n_table\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- Updating local n_table\n",
    "            for k=1,K+1 do\n",
    "                -- Possible Case where there is no Ngrams ending with indexN with count of K \n",
    "                if n_table[N][k][indexN] ~= nil then\n",
    "                    n_table_local[k] = n_table_local[k] + n_table[N][k][indexN]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        -- Check no 0 in n_table_local\n",
    "        for k=1,K+1 do\n",
    "            if n_table_local[k] == 0 then\n",
    "                print('0 count in n_table_local for ', indexN, k, N)\n",
    "                n_table_local[k] = 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        -- Building D (needed to compute prediction rows)\n",
    "        -- Computing local D\n",
    "\n",
    "        if D == nil then \n",
    "            local Y = n_table_local[1]/(n_table_local[1] + 2*n_table_local[2])\n",
    "            D = {}\n",
    "            for k=1,K do\n",
    "               D[k] = k - (1 + k)*Y*n_table_local[1 + k]/n_table_local[k]\n",
    "            end\n",
    "        end\n",
    "\n",
    "        F_local = 0\n",
    "        -- Compute curent order level with modified absolute discouting for each word\n",
    "        for j=1,50 do\n",
    "            indexN = entry[{1, j}]\n",
    "            -- case word seen\n",
    "            if F_c_w_table[N][indexes][indexN] ~= nil then\n",
    "                -- Building the key for the different case of absolute discounting\n",
    "                if F_c_w_table[N][indexes][indexN] > K then\n",
    "                    key_N_c = K\n",
    "                else\n",
    "                    key_N_c = F_c_w_table[N][indexes][indexN]\n",
    "                end\n",
    "                prediction[j] = F_c_w_table[N][indexes][indexN] - D[key_N_c]\n",
    "                F_local = F_local + F_c_w_table[N][indexes][indexN]\n",
    "                N_c_local[key_N_c] = N_c_local[key_N_c] + 1\n",
    "            end\n",
    "        end\n",
    "\n",
    "        -- Check that MLE predicted at least one words, otherwise go to next context\n",
    "        if prediction:sum(1)[1] == 0 then\n",
    "            --print('unseen words')\n",
    "            return compute_mkn_line2(N-1, entry, F_c_w_table, n_table, alpha, K, D)\n",
    "        end\n",
    "        \n",
    "        -- Computing factor of lower order model (no denominator because we normalize afterwards)\n",
    "        local gamma = 0\n",
    "        for k=1,K do\n",
    "            if N_c_local[k] ~= nil then\n",
    "                gamma = gamma + D[k]*N_c_local[k]\n",
    "            end\n",
    "        end\n",
    "        if gamma < 0 then\n",
    "            --print('gamma error')\n",
    "            return compute_mkn_line2(N-1, entry, F_c_w_table, n_table, alpha, K, D)\n",
    "        end\n",
    "        -- Combining with next context\n",
    "        prediction:add(compute_mkn_line2(N-1, entry, F_c_w_table, n_table, alpha, K, D):mul(gamma)):div(F_local)\n",
    "        -- Normalization\n",
    "        -- TODO: why??? We normalize at the end\n",
    "        -- prediction:div(prediction:sum(1)[1])\n",
    "        return prediction\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Modified Kneser Ney: computation done at once line by line\n",
    "--\n",
    "-- p_wb(w|c) = (F_c_w + N_c_. * p_wb(w|c'))/(N_c_. + F_c_.)\n",
    "function distribution_proba_mKN2(N, data, alpha, K, D)\n",
    "    local N_data = data:size(1)\n",
    "    local M = data:size(2)\n",
    "\n",
    "    -- Building the count matrix for each ngram size lower than N.\n",
    "    local F_c_w_table = {}\n",
    "    local n_table = {}\n",
    "    for i=1,N do\n",
    "        train = get_train(i)\n",
    "        F_c_w_table[i], n_table[i] = build_context_count_split2(train, K)\n",
    "    end\n",
    "\n",
    "    -- Vector initialisation\n",
    "    local distribution = torch.zeros(N_data, 50)\n",
    "    for i=1,N_data do\n",
    "        -- Compute witten bell for the whole line i\n",
    "        distribution:narrow(1, i, 1):copy(compute_mkn_line2(N, data:narrow(1,i,1), F_c_w_table, n_table, alpha, K, D))\n",
    "    end\n",
    "    --distribution:cdiv(distribution:sum(2):expand(distribution:size(1), distribution:size(2)))\n",
    "    return distribution\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distribution_6_mkn = distribution_proba_mKN2(3, validation_3, 15, 3)\n",
    "\n",
    "print('Perplexity:', perplexity(distribution_6_mkn, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5349357395564\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.2\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5330696515949\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.2\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5332328151701\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.2\n",
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5404224547111\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.3\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5386501324031\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.3\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5389002759927\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.3\n",
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5477734285685\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.4\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5460833817531\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.4\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5464116718229\tD\t{\n",
       "  1 : 0.4\n",
       "  2 : 1.4\n",
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4978931128999\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.2\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4965478301411\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.2\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4971593351065\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.2\n",
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5036019166258\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.3\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5023434772225\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.3\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5030362842412\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.3\n",
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5111287870464\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.4\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5099467633812\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.4\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.5107129355715\tD\t{\n",
       "  1 : 0.5\n",
       "  2 : 1.4\n",
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4782131364604\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.2\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4773065411832\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.2\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4783068978757\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.2\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4841469964138\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.3\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4833220525745\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.3\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4843995222808\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.3\n",
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.491870129018\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.4\n",
       "  3 : 1.6\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4911172611547\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.4\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.492264636559\tD\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.4\n",
       "  3 : 1.9\n",
       "}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- CV on D\n",
    "perp = torch.zeros(27)\n",
    "indextoD = {}\n",
    "D1 = {0.4, 0.5, 0.6}\n",
    "D2 = {1.2, 1.3, 1.4}\n",
    "D3 = {1.6, 1.75, 1.9}\n",
    "count = 1\n",
    "\n",
    "for i=1, 3 do\n",
    "    for j=1,3 do\n",
    "        for k=1,3 do\n",
    "            D = {[1] = D1[i], [2] = D2[j], [3] = D3[k]}\n",
    "            distribution_mkn = distribution_proba_mKN2(3, validation_3, 15, 3, D)\n",
    "            p = perplexity(distribution_mkn, validation_output)\n",
    "            print('Perplexity:', p, 'D', D)\n",
    "            perp[count] = p\n",
    "            indextoD[count] = D\n",
    "            count = count + 1\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Best D is\t{\n",
       "  1 : 0.6\n",
       "  2 : 1.2\n",
       "  3 : 1.75\n",
       "}\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, a = perp:min(1)\n",
    "print('Best D is', indextoD[a[1]])\n",
    "best_D = indextoD[a[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t1454\t5\t3\t\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t3720\t5\t3\t\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t5928\t5\t3\t\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t5000\t5\t3\t\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t1038\t5\t3\t\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t1480\t5\t3\t\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4756255721969\t\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_D[4] = 2.1\n",
    "distribution_mkn = distribution_proba_mKN2(3, validation_3, 15, 4, best_D)\n",
    "\n",
    "print('Perplexity:', perplexity(distribution_mkn, validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perplexity:\t3.4619516690308\t\n"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Mixing 2 models\n",
    "w = 0.7\n",
    "\n",
    "distribution_mixed = torch.mul(distribution_mkn, w):add(torch.mul(distribution_wb,(1 - w)))\n",
    "print('Perplexity:', perplexity(distribution_mixed, validation_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying on test Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Applying on test\n",
    "distribution_test = distribution_proba_WB(6, test_6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Saving the current best model\n",
    "myFile = hdf5.open('pred_test_wb_fnorm_6', 'w')\n",
    "myFile:write('distribution', distribution_test)\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_c_w_table = {}\n",
    "n_table = {}\n",
    "for i=1,3 do\n",
    "    train = get_train(i)\n",
    "    F_c_w_table[i], n_table[i] = build_context_count_split2(train, 3)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 0.83764940239044\n",
       "  2 : 1.4604098941657\n",
       "  3 : -0.063403528742174\n",
       "}\n",
       "41\t37.063403528742\t\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "gamma error\t\n",
       "{\n",
       "  1 : 0.71739130434783\n",
       "  2 : 1.3678929765886\n",
       "  3 : 1.0869565217391\n",
       "}\n",
       "37\t0.28260869565217\t\n",
       "41\t38.913043478261\t\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = compute_mkn_line2(3, test_3:narrow(1,2357,1), F_c_w_table, n_table, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t1454\t5\t3\t\n"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t1017\t5\t3\t\n"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0 count in n_table_local for \t5928\t5\t3\t\n"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_mkn_test = distribution_proba_mKN2(3, test_3, 15, 4, best_D)\n",
    "distribution_wb_test = distribution_proba_WB(6, test_6, 10)\n",
    "distribution_mixed_test = torch.mul(distribution_mkn_test, w):add(torch.mul(distribution_wb_test,(1 - w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 10\n",
       " 0.0021  0.0002  0.0006  0.0034  0.0006  0.0002  0.0002  0.0002  0.0027  0.0002\n",
       "\n",
       "Columns 11 to 20\n",
       " 0.0002  0.0004  0.0002  0.0002  0.0002  0.0002  0.0002  0.0010  0.0002  0.0002\n",
       "\n",
       "Columns 21 to 30\n",
       " 0.0004  0.0001  0.4397  0.0002  0.0002  0.0002  0.0002  0.0007  0.0002  0.0006\n",
       "\n",
       "Columns 31 to 40\n",
       " 0.0002  0.0002  0.0002  0.0002  0.0004  0.0002  0.0002  0.0002  0.0003  0.0002\n",
       "\n",
       "Columns 41 to 50\n",
       " 0.3504  0.1890  0.0002  0.0003  0.0002  0.0003  0.0002  0.0011  0.0002  0.0003\n",
       "[torch.DoubleTensor of size 1x50]\n",
       "\n"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_mixed_test:narrow(1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Saving the current best model\n",
    "myFile = hdf5.open('pred_test_mixed_cb', 'w')\n",
    "myFile:write('distribution', distribution_mixed_test)\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m, true_output = validation_output:max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_accuracy(pred, true_output)\n",
    "    max,argmax = pred:max(2)\n",
    "    acc = 0\n",
    "    for i = 1, true_output:size(1) do\n",
    "        if argmax[i][1] == true_output[i][1] then\n",
    "            acc = acc + 1\n",
    "        end\n",
    "    end\n",
    "    score = acc/true_output:size(1)\n",
    "    \n",
    "    return score\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result on alpha smoothing \t0.59614243323442\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on alpha smoothing ', compute_accuracy(distribution_2, true_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
