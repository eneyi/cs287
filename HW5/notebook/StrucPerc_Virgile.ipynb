{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'hdf5'\n",
    "require 'nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Loading data\n",
    "myFile = hdf5.open('../data/MM_data_cap.hdf5','r')\n",
    "data = myFile:all()\n",
    "input_matrix_train_cap = data['input_matrix_train_cap']\n",
    "input_matrix_dev_cap = data['input_matrix_dev_cap']\n",
    "input_matrix_test_cap = data['input_matrix_test_cap']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwords = input_matrix_train_cap:size(1)\n",
    "train_input = torch.Tensor(nwords-1,10)\n",
    "train_input:narrow(2,1,1):copy(input_matrix_train_cap:narrow(2,1,1):narrow(1,2,nwords-1))\n",
    "train_input:narrow(2,2,9):copy(input_matrix_train_cap:narrow(2,2,9):narrow(1,1,nwords-1))\n",
    "train_output = input_matrix_train_cap:narrow(2,16,1):narrow(1,2,nwords-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('../data/embeddings.hdf5','r')\n",
    "data2 = myFile:all()\n",
    "embeddings = data2['embeddings']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LT = nn.LookupTable(400002,50)\n",
    "LT.weight:narrow(1, 1, 400000):copy(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "t1 = nn.ParallelTable()\n",
    "\n",
    "t1_1 = nn.Sequential()\n",
    "t1_1:add(LT)\n",
    "t1_1:add(nn.View(-1,50))\n",
    "\n",
    "t1_2 = nn.Identity()\n",
    "\n",
    "t1:add(t1_1)\n",
    "t1:add(t1_2)\n",
    "\n",
    "model:add(t1)\n",
    "model:add(nn.JoinTable(2))\n",
    "\n",
    "model:add(nn.Linear(59,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function compute_logscore(observations, i, model, C)\n",
    "    local y = torch.zeros(C,C)\n",
    "    local hot_1 = torch.zeros(C)\n",
    "    for j = 1, C do\n",
    "        hot_1:zero()\n",
    "        hot_1[j] = 1\n",
    "        y:narrow(1,j,1):copy(model:forward({observations[i]:view(1,1),hot_1:view(1,9)}))\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "function viterbi(observations, compute_logscore, model, C)\n",
    "    \n",
    "    local y = torch.zeros(C,C)\n",
    "    -- Formating tensors\n",
    "    local initial = torch.zeros(C, 1)\n",
    "    -- initial started with a start of sentence: <t>\n",
    "\n",
    "    initial[{8,1}] = 1\n",
    "    initial:log()\n",
    "\n",
    "    -- number of classes\n",
    "    local n = observations:size(1)\n",
    "    local max_table = torch.Tensor(n, C)\n",
    "    local backpointer_table = torch.Tensor(n, C)\n",
    "    -- first timestep\n",
    "    -- the initial most likely paths are the initial state distribution\n",
    "    -- NOTE: another unnecessary Tensor allocation here\n",
    "    local maxes, backpointers = (initial + compute_logscore(observations, 1, model, C)[8]):max(2)\n",
    "    max_table[1] = maxes\n",
    "    -- remaining timesteps (\"forwarding\" the maxes)\n",
    "    for i=2,n do\n",
    "        -- precompute edge scores\n",
    "       \n",
    "        y:copy(compute_logscore(observations, i, model, C))\n",
    "        scores = y:transpose(1,2) + maxes:view(1, C):expand(C, C)\n",
    "\n",
    "        -- compute new maxes (NOTE: another unnecessary Tensor allocation here)\n",
    "        maxes, backpointers = scores:max(2)\n",
    "\n",
    "        -- record\n",
    "        max_table[i] = maxes\n",
    "        backpointer_table[i] = backpointers\n",
    "    end\n",
    "    -- follow backpointers to recover max path\n",
    "    local classes = torch.Tensor(n)\n",
    "    maxes, classes[n] = maxes:max(1)\n",
    "    for i=n,2,-1 do\n",
    "        classes[i-1] = backpointer_table[{i, classes[i]}]\n",
    "    end\n",
    "\n",
    "    return classes\n",
    "end\n",
    "\n",
    "function train_model(train_input, train_output, model, criterion, din, nclass, eta, nEpochs, batchSize)\n",
    "    -- Train the model with a mini batch SGD\n",
    "    -- standard parameters are\n",
    "    -- nEpochs = 1\n",
    "    -- batchSize = 32\n",
    "    -- eta = 0.01\n",
    "    loss = torch.Tensor(nEpochs)\n",
    "\n",
    "    -- To store the loss\n",
    "    av_L = 0\n",
    "\n",
    "    -- Memory allocation\n",
    "    inputs_batch = torch.DoubleTensor(batchSize, din)\n",
    "    gold_sequence = torch.DoubleTensor(batchSize)\n",
    "    high_score_seq = torch.DoubleTensor(batchSize, nclass)\n",
    "    df_do = torch.zeros(batchSize, nclass)\n",
    "\n",
    "    for i = 1, nEpochs do\n",
    "        -- timing the epoch\n",
    "        timer = torch.Timer()\n",
    "        av_L = 0\n",
    "        \n",
    "        -- mini batch loop\n",
    "        for t = 1, train_input:size(1), batchSize do\n",
    "            -- Mini batch data\n",
    "            current_batch_size = math.min(batchSize,train_input:size(1)-t)\n",
    "--             print('here1')\n",
    "            \n",
    "            inputs_batch:narrow(1,1,current_batch_size):copy(train_input:narrow(1,t,current_batch_size))\n",
    "--             print('here2')\n",
    "            \n",
    "            gold_sequence:narrow(1,1,current_batch_size):copy(train_output:narrow(1,t,current_batch_size))\n",
    "--             print('here3')\n",
    "            \n",
    "            -- reset gradients\n",
    "            model:zeroGradParameters()\n",
    "            --gradParameters:zero()\n",
    "\n",
    "            -- Forward pass on a batch subsequence:\n",
    "            high_score_seq:narrow(1,1,current_batch_size):copy(viterbi(inputs_batch:narrow(2,1,1), compute_logscore, model, nclass))\n",
    "--             print('here4')\n",
    "            \n",
    "            -- TO CODE :\n",
    "            df_do:zero()\n",
    "            for ii = 1, high_score_seq:size(1) do\n",
    "                if high_score_seq[ii] ~= gold_sequence_seq[ii] then\n",
    "                    y = model:forward({inputs_batch:narrow(1,ii,1):narrow(2,1,1),inputs_batch:narrow(2,2,9)})\n",
    "                    \n",
    "                end\n",
    "            end\n",
    "--             print('here6')\n",
    "            model:backward({inputs_batch:narrow(1,1,current_batch_size):narrow(2,1,1), inputs_batch:narrow(1,1,current_batch_size):narrow(2,2,9)}, \n",
    "                            df_do:narrow(1,1,current_batch_size))\n",
    "--             print('here7')\n",
    "            model:updateParameters(eta)\n",
    "            \n",
    "        end\n",
    "            \n",
    "        print('Epoch '..i..': '..timer:time().real)\n",
    "        loss[i] = av_L/math.floor(train_input:size(1)/batchSize)\n",
    "        print('Average Loss: '.. loss[i])\n",
    "       \n",
    "    end\n",
    "    \n",
    "    return loss\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
