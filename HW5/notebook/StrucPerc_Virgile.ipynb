{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'hdf5'\n",
    "require 'nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Loading data\n",
    "myFile = hdf5.open('../data/MM_data_cap.hdf5','r')\n",
    "data = myFile:all()\n",
    "input_matrix_train_cap = data['input_matrix_train_cap']\n",
    "input_matrix_dev_cap = data['input_matrix_dev_cap']\n",
    "input_matrix_test_cap = data['input_matrix_test_cap']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Loading data\n",
    "myFile = hdf5.open('../data/sent_start.hdf5','r')\n",
    "data = myFile:all()\n",
    "sent = data['sent_start']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwords = input_matrix_train_cap:size(1)\n",
    "train_input = torch.Tensor(nwords-1,10)\n",
    "train_input:narrow(2,1,1):copy(input_matrix_train_cap:narrow(2,1,1):narrow(1,2,nwords-1))\n",
    "train_input:narrow(2,2,9):copy(input_matrix_train_cap:narrow(2,2,9):narrow(1,1,nwords-1))\n",
    "train_output = input_matrix_train_cap:narrow(2,16,1):narrow(1,2,nwords-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('../data/embeddings.hdf5','r')\n",
    "data2 = myFile:all()\n",
    "embeddings = data2['embeddings']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LT = nn.LookupTable(400002,50)\n",
    "LT.weight:narrow(1, 1, 400000):copy(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "t1 = nn.ParallelTable()\n",
    "\n",
    "t1_1 = nn.Sequential()\n",
    "t1_1:add(LT)\n",
    "t1_1:add(nn.View(-1,50))\n",
    "\n",
    "t1_2 = nn.Identity()\n",
    "\n",
    "t1:add(t1_1)\n",
    "t1:add(t1_2)\n",
    "\n",
    "model:add(t1)\n",
    "model:add(nn.JoinTable(2))\n",
    "\n",
    "model:add(nn.Linear(59,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function compute_logscore(observations, i, model, C)\n",
    "    local y = torch.zeros(C,C)\n",
    "    local hot_1 = torch.zeros(C)\n",
    "    for j = 1, C do\n",
    "        hot_1:zero()\n",
    "        hot_1[j] = 1\n",
    "        y:narrow(1,j,1):copy(model:forward({observations[i]:view(1,1),hot_1:view(1,9)}))\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "function viterbi(observations, compute_logscore, model, C)\n",
    "    \n",
    "    local y = torch.zeros(C,C)\n",
    "    -- Formating tensors\n",
    "    local initial = torch.zeros(C, 1)\n",
    "    -- initial started with a start of sentence: <t>\n",
    "\n",
    "    initial[{8,1}] = 1\n",
    "    initial:log()\n",
    "\n",
    "    -- number of classes\n",
    "    local n = observations:size(1)\n",
    "    local max_table = torch.Tensor(n, C)\n",
    "    local backpointer_table = torch.Tensor(n, C)\n",
    "    -- first timestep\n",
    "    -- the initial most likely paths are the initial state distribution\n",
    "    -- NOTE: another unnecessary Tensor allocation here\n",
    "    local maxes, backpointers = (initial + compute_logscore(observations, 1, model, C)[8]):max(2)\n",
    "    max_table[1] = maxes\n",
    "    -- remaining timesteps (\"forwarding\" the maxes)\n",
    "    for i=2,n do\n",
    "        -- precompute edge scores\n",
    "       \n",
    "        y:copy(compute_logscore(observations, i, model, C))\n",
    "        scores = y:transpose(1,2) + maxes:view(1, C):expand(C, C)\n",
    "\n",
    "        -- compute new maxes (NOTE: another unnecessary Tensor allocation here)\n",
    "        maxes, backpointers = scores:max(2)\n",
    "\n",
    "        -- record\n",
    "        max_table[i] = maxes\n",
    "        backpointer_table[i] = backpointers\n",
    "    end\n",
    "    -- follow backpointers to recover max path\n",
    "    local classes = torch.Tensor(n)\n",
    "    maxes, classes[n] = maxes:max(1)\n",
    "    for i=n,2,-1 do\n",
    "        classes[i-1] = backpointer_table[{i, classes[i]}]\n",
    "    end\n",
    "\n",
    "    return classes\n",
    "end\n",
    "\n",
    "function train_model(train_input, sent, train_output, model, din, nclass, eta, nEpochs)\n",
    "    -- Train the model with a mini batch SGD\n",
    "    -- standard parameters are\n",
    "    -- nEpochs = 1\n",
    "    -- batchSize = 32\n",
    "    -- eta = 0.01\n",
    "    loss = torch.Tensor(nEpochs)\n",
    "\n",
    "    -- To store the loss\n",
    "    av_L = 0\n",
    "\n",
    "    -- Memory allocation\n",
    "    inputs_batch = torch.DoubleTensor(100, din)\n",
    "    gold_sequence = torch.DoubleTensor(100)\n",
    "    high_score_seq = torch.DoubleTensor(100)\n",
    "    df_do = torch.zeros(100, nclass)\n",
    "\n",
    "    for i = 1, nEpochs do\n",
    "        -- timing the epoch\n",
    "        timer = torch.Timer()\n",
    "        av_L = 0\n",
    "        \n",
    "        -- mini batch loop\n",
    "        for t = 2, sent:size(1) do\n",
    "            -- Mini batch data\n",
    "            sent_size = sent[{t,2}]\n",
    "--             print('here1')\n",
    "            \n",
    "            inputs_batch:narrow(1,1,sent_size):copy(train_input:narrow(1,sent[{t,1}]-1,sent_size))\n",
    "--             print('here2')\n",
    "            \n",
    "            gold_sequence:narrow(1,1,sent_size):copy(train_output:narrow(1,sent[{t,1}]-1,sent_size))\n",
    "--             print('here3')\n",
    "            \n",
    "            -- reset gradients\n",
    "            model:zeroGradParameters()\n",
    "            --gradParameters:zero()\n",
    "\n",
    "            -- Forward pass on a batch subsequence:\n",
    "            high_score_seq:narrow(1,1,sent_size):copy(viterbi(inputs_batch:narrow(1,1,sent_size):narrow(2,1,1), \n",
    "                                                                compute_logscore, model, nclass))\n",
    "--             print('here4')\n",
    "            \n",
    "            -- TO CODE :\n",
    "            df_do:zero()\n",
    "            for ii = 1, sent_size do\n",
    "                if high_score_seq[ii] ~= gold_sequence[ii] then\n",
    "                    y = model:forward({inputs_batch:narrow(1,ii,1):narrow(2,1,1),inputs_batch:narrow(1,ii,1):narrow(2,2,9)})\n",
    "                    df_do[ii]:copy(y)\n",
    "                end\n",
    "            end\n",
    "--             print('here6')\n",
    "            model:backward({inputs_batch:narrow(1,1,sent_size):narrow(2,1,1), inputs_batch:narrow(1,1,sent_size):narrow(2,2,9)}, \n",
    "                            df_do:narrow(1,1,sent_size))\n",
    "--             print('here7')\n",
    "            model:updateParameters(eta)\n",
    "            \n",
    "        end\n",
    "            \n",
    "        print('Epoch '..i..': '..timer:time().real)\n",
    "        loss[i] = av_L/math.floor(train_input:size(1)/batchSize)\n",
    "        print('Average Loss: '.. loss[i])\n",
    "       \n",
    "    end\n",
    "    \n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "...virgileaudi/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 3 module of nn.Sequential:\n...rs/virgileaudi/torch/install/share/lua/5.1/nn/Linear.lua:80: size mismatch, t: [1 x 59], m1: [4 x 9], m2: [9 x 59] at /tmp/luarocks_torch-scm-1-7765/torch7/lib/TH/generic/THTensorMath.c:780\nstack traceback:\n\t[C]: in function 'addmm'\n\t...rs/virgileaudi/torch/install/share/lua/5.1/nn/Linear.lua:80: in function 'updateGradInput'\n\t...rs/virgileaudi/torch/install/share/lua/5.1/nn/Module.lua:31: in function <...rs/virgileaudi/torch/install/share/lua/5.1/nn/Module.lua:29>\n\t[C]: in function 'xpcall'\n\t...virgileaudi/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...irgileaudi/torch/install/share/lua/5.1/nn/Sequential.lua:84: in function 'backward'\n\t[string \"function compute_logscore(observations, i, mo...\"]:107: in function 'f'\n\t[string \"local f = function() return train_model(train...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:179: in function <.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t.../virgileaudi/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010eb57b50\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occured. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...virgileaudi/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...irgileaudi/torch/install/share/lua/5.1/nn/Sequential.lua:84: in function 'backward'\n\t[string \"function compute_logscore(observations, i, mo...\"]:107: in function 'f'\n\t[string \"local f = function() return train_model(train...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:179: in function <.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t.../virgileaudi/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010eb57b50",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...virgileaudi/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 3 module of nn.Sequential:\n...rs/virgileaudi/torch/install/share/lua/5.1/nn/Linear.lua:80: size mismatch, t: [1 x 59], m1: [4 x 9], m2: [9 x 59] at /tmp/luarocks_torch-scm-1-7765/torch7/lib/TH/generic/THTensorMath.c:780\nstack traceback:\n\t[C]: in function 'addmm'\n\t...rs/virgileaudi/torch/install/share/lua/5.1/nn/Linear.lua:80: in function 'updateGradInput'\n\t...rs/virgileaudi/torch/install/share/lua/5.1/nn/Module.lua:31: in function <...rs/virgileaudi/torch/install/share/lua/5.1/nn/Module.lua:29>\n\t[C]: in function 'xpcall'\n\t...virgileaudi/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...irgileaudi/torch/install/share/lua/5.1/nn/Sequential.lua:84: in function 'backward'\n\t[string \"function compute_logscore(observations, i, mo...\"]:107: in function 'f'\n\t[string \"local f = function() return train_model(train...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:179: in function <.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t.../virgileaudi/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010eb57b50\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occured. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...virgileaudi/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...irgileaudi/torch/install/share/lua/5.1/nn/Sequential.lua:84: in function 'backward'\n\t[string \"function compute_logscore(observations, i, mo...\"]:107: in function 'f'\n\t[string \"local f = function() return train_model(train...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:179: in function <.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t.../virgileaudi/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010eb57b50"
     ]
    }
   ],
   "source": [
    "train_model(train_input, sent, train_output, model, 10, 9, 0.001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs_batch = torch.DoubleTensor(100, 10)\n",
    "gold_sequence = torch.DoubleTensor(100)\n",
    "high_score_seq = torch.DoubleTensor(100)\n",
    "df_do = torch.zeros(100, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = 2\n",
    "sent_size = sent[{t,2}]\n",
    "--             print('here1')\n",
    "\n",
    "inputs_batch:narrow(1,1,sent_size):copy(train_input:narrow(1,sent[{t,1}]-1,sent_size))\n",
    "--             print('here2')\n",
    "\n",
    "gold_sequence:narrow(1,1,sent_size):copy(train_output:narrow(1,sent[{t,1}]-1,sent_size))\n",
    "--             print('here3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_score_seq:narrow(1,1,sent_size):copy(viterbi(inputs_batch:narrow(1,1,sent_size):narrow(2,1,1), compute_logscore, model, 9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no\t\n",
       "yes\t\n",
       "2\t8\t\n",
       "yes\t\n",
       "3\t4\t\n",
       "yes\t\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4\t2\t\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ii = 1, sent_size do\n",
    "    if high_score_seq[ii] ~= gold_sequence[ii] then\n",
    "        print('yes')\n",
    "        pr = model:forward({inputs_batch:narrow(1,ii,1):narrow(2,1,1),inputs_batch:narrow(1,ii,1):narrow(2,2,9)})\n",
    "        m, a = pr:max(2)\n",
    "        print(ii, a[1][1])\n",
    "        df_do[{ii,a[1][1]}] = 1\n",
    "        df_do[{ii,gold_sequence[ii]}] = -1          \n",
    "    else\n",
    "        print('no')\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  1  0  0  0  0  0  0  0\n",
       " 1 -1  1  1  1  1  1  1  1\n",
       " 1 -1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1 -1\n",
       "[torch.DoubleTensor of size 4x9]\n",
       "\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_do:narrow(1,1,sent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11\t\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[{t,1}]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
