{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'hdf5'\n",
    "require 'nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Loading data\n",
    "myFile = hdf5.open('../data/MM_data_cap.hdf5','r')\n",
    "data = myFile:all()\n",
    "input_matrix_train_cap = data['input_matrix_train_cap']\n",
    "input_matrix_dev_cap = data['input_matrix_dev_cap']\n",
    "input_matrix_test_cap = data['input_matrix_test_cap']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Loading data\n",
    "myFile = hdf5.open('../data/sent_start.hdf5','r')\n",
    "data = myFile:all()\n",
    "sent = data['sent_start']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwords = input_matrix_train_cap:size(1)\n",
    "train_input = torch.Tensor(nwords-1,10)\n",
    "train_input:narrow(2,1,1):copy(input_matrix_train_cap:narrow(2,1,1):narrow(1,2,nwords-1))\n",
    "train_input:narrow(2,2,9):copy(input_matrix_train_cap:narrow(2,2,9):narrow(1,1,nwords-1))\n",
    "train_output = input_matrix_train_cap:narrow(2,16,1):narrow(1,2,nwords-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('../data/embeddings.hdf5','r')\n",
    "data2 = myFile:all()\n",
    "embeddings = data2['embeddings']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function compute_logscore(observations, i, model, C)\n",
    "    local y = torch.zeros(C,C)\n",
    "    local hot_1 = torch.zeros(C)\n",
    "    for j = 1, C do\n",
    "        hot_1:zero()\n",
    "        hot_1[j] = 1\n",
    "        y:narrow(1,j,1):copy(model:forward({observations[i]:view(1,1),hot_1:view(1,9)}))\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "function viterbi(observations, compute_logscore, model, C)\n",
    "    \n",
    "    local y = torch.zeros(C,C)\n",
    "    -- Formating tensors\n",
    "    local initial = torch.zeros(C, 1)\n",
    "    -- initial started with a start of sentence: <t>\n",
    "\n",
    "    initial[{8,1}] = 1\n",
    "    initial:log()\n",
    "\n",
    "    -- number of classes\n",
    "    local n = observations:size(1)\n",
    "    local max_table = torch.Tensor(n, C)\n",
    "    local backpointer_table = torch.Tensor(n, C)\n",
    "    -- first timestep\n",
    "    -- the initial most likely paths are the initial state distribution\n",
    "    -- NOTE: another unnecessary Tensor allocation here\n",
    "    local maxes, backpointers = (initial + compute_logscore(observations, 1, model, C)[8]):max(2)\n",
    "    max_table[1] = maxes\n",
    "    -- remaining timesteps (\"forwarding\" the maxes)\n",
    "    for i=2,n do\n",
    "        -- precompute edge scores\n",
    "       \n",
    "        y:copy(compute_logscore(observations, i, model, C))\n",
    "        scores = y:transpose(1,2) + maxes:view(1, C):expand(C, C)\n",
    "\n",
    "        -- compute new maxes (NOTE: another unnecessary Tensor allocation here)\n",
    "        maxes, backpointers = scores:max(2)\n",
    "\n",
    "        -- record\n",
    "        max_table[i] = maxes\n",
    "        backpointer_table[i] = backpointers\n",
    "    end\n",
    "    -- follow backpointers to recover max path\n",
    "    local classes = torch.Tensor(n)\n",
    "    maxes, classes[n] = maxes:max(1)\n",
    "    for i=n,2,-1 do\n",
    "        classes[i-1] = backpointer_table[{i, classes[i]}]\n",
    "    end\n",
    "\n",
    "    return classes\n",
    "end\n",
    "\n",
    "function train_model(train_input, sent, train_output, model, din, nclass, eta, nEpochs)\n",
    "    -- Train the model with a mini batch SGD\n",
    "    -- standard parameters are\n",
    "    -- nEpochs = 1\n",
    "    -- batchSize = 32\n",
    "    -- eta = 0.01\n",
    "\n",
    "    -- Memory allocation\n",
    "    inputs_batch = torch.DoubleTensor(100, din)\n",
    "    gold_sequence = torch.DoubleTensor(100)\n",
    "    high_score_seq = torch.DoubleTensor(100)\n",
    "    grad_pos = torch.zeros(9)\n",
    "    grad_neg = torch.zeros(9)\n",
    "    pr1 = torch.zeros(9)\n",
    "    pr2 = torch.zeros(9)\n",
    "    \n",
    "    for i = 1, nEpochs do\n",
    "        -- timing the epoch\n",
    "        timer = torch.Timer()\n",
    "        \n",
    "        -- mini batch loop\n",
    "        for t = 2, sent:size(1)-1 do\n",
    "            -- Mini batch data\n",
    "            sent_size = sent[{t,2}]\n",
    "--             print('here1')\n",
    "            \n",
    "            inputs_batch:narrow(1,1,sent_size+1):copy(train_input:narrow(1,sent[{t,1}]-1,sent_size+1))\n",
    "--             print('here2')\n",
    "            \n",
    "            gold_sequence:narrow(1,1,sent_size+1):copy(train_output:narrow(1,sent[{t,1}]-1,sent_size+1))\n",
    "--             print('here3')\n",
    "            \n",
    "            -- reset gradients\n",
    "            model:zeroGradParameters()\n",
    "            --gradParameters:zero()\n",
    "\n",
    "            -- Forward pass on a batch subsequence:\n",
    "            high_score_seq:narrow(1,1,sent_size+1):copy(viterbi(inputs_batch:narrow(1,1,sent_size+1):narrow(2,1,1), \n",
    "                                                                compute_logscore, model, nclass))\n",
    "--             print('here4')\n",
    "            \n",
    "            \n",
    "            for ii = 1, sent_size+1 do\n",
    "                grad_neg:zero()\n",
    "                grad_pos:zero()\n",
    "                if high_score_seq[ii] ~= gold_sequence[ii] then\n",
    "                    pr1:copy(model:forward({inputs_batch:narrow(1,ii,1):narrow(2,1,1),inputs_batch:narrow(1,ii,1):narrow(2,2,9)}))\n",
    "                    pr2:copy(model:forward({torch.Tensor({high_score_seq[ii-1]}),inputs_batch:narrow(1,ii,1):narrow(2,2,9)}))\n",
    "                    m, a = pr2:view(1,9):max(2)\n",
    "                    grad_neg[gold_sequence[ii]] = -1\n",
    "                    grad_pos[a[1][1]] = 1\n",
    "                    model:backward({inputs_batch:narrow(1,ii,1):narrow(2,1,1),inputs_batch:narrow(1,ii,1):narrow(2,2,9)}, grad_pos:view(1,9))\n",
    "                    model:backward({torch.Tensor({high_score_seq[ii-1]}),inputs_batch:narrow(1,ii,1):narrow(2,2,9)}, grad_neg:view(1,9))\n",
    "                end\n",
    "            end\n",
    "--             print('here7')\n",
    "            model:updateParameters(eta)\n",
    "            \n",
    "        end\n",
    "            \n",
    "        print('Epoch '..i..': '..timer:time().real)\n",
    "       \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs_batch = torch.DoubleTensor(100, 10)\n",
    "gold_sequence = torch.DoubleTensor(100)\n",
    "high_score_seq = torch.DoubleTensor(100)\n",
    "grad_pos = torch.zeros(9)\n",
    "grad_neg = torch.zeros(9)\n",
    "pr1 = torch.zeros(9)\n",
    "pr2 = torch.zeros(9)\n",
    "\n",
    "t = 4\n",
    "\n",
    "sent_size = sent[{t,2}]\n",
    "--             print('here1')\n",
    "\n",
    "inputs_batch:narrow(1,1,sent_size+1):copy(train_input:narrow(1,sent[{t,1}]-1,sent_size+1))\n",
    "--             print('here2')\n",
    "\n",
    "gold_sequence:narrow(1,1,sent_size+1):copy(train_output:narrow(1,sent[{t,1}]-1,sent_size+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function compute_score(predicted_classes, true_classes)\n",
    "    local n = predicted_classes:size(1)\n",
    "    local right_pred = 0\n",
    "    local positive_true = 0\n",
    "    local positive_pred = 0\n",
    "    for i=1,n do\n",
    "        if predicted_classes[i] > 1 then\n",
    "            positive_pred = positive_pred + 1\n",
    "        end\n",
    "        if true_classes[i] > 1 then\n",
    "            positive_true = positive_true + 1\n",
    "        end\n",
    "        if (true_classes[i] == predicted_classes[i]) and true_classes[i] > 1 then\n",
    "            right_pred = right_pred + 1\n",
    "        end\n",
    "    end\n",
    "    print(positive_true)\n",
    "    print(positive_pred)\n",
    "    print(right_pred)\n",
    "    local precision = right_pred/positive_pred\n",
    "    local recall = right_pred/positive_true\n",
    "    return precision, recall\n",
    "end\n",
    "        \n",
    "function f_score(predicted_classes, true_classes)\n",
    "    local p,r = compute_score(predicted_classes, true_classes)\n",
    "    return 2*p*r/(p+r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LT = nn.LookupTable(400002,50)\n",
    "LT.weight:narrow(1, 1, 400000):copy(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "t1 = nn.ParallelTable()\n",
    "\n",
    "t1_1 = nn.Sequential()\n",
    "t1_1:add(LT)\n",
    "t1_1:add(nn.View(-1,50))\n",
    "\n",
    "t1_2 = nn.Identity()\n",
    "\n",
    "t1:add(t1_1)\n",
    "t1:add(t1_2)\n",
    "\n",
    "model:add(t1)\n",
    "model:add(nn.JoinTable(2))\n",
    "\n",
    "lin = nn.Linear(59,9)\n",
    "model:add(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin.weight:zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 292.69423389435\t\n",
       "\n"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_input, sent, train_output, model, 10, 9, 0.0001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations = input_matrix_dev_cap:narrow(2,1,1):narrow(1,1,500):clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_classes = input_matrix_dev_cap:narrow(2,16,1):narrow(1,1,500):squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl = viterbi(observations, compute_logscore, model, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108\t\n",
       "463\t\n",
       "17\t\n",
       "0.05954465849387\t\n"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (f_score(cl, true_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 8\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4\n",
       "[torch.DoubleTensor of size 50]\n",
       "\n"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl:narrow(1,1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
