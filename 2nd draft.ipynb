{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  H5Z_FILTER_CONFIG_ENCODE_ENABLED : 1\n",
       "  H5F_ACC_RDWR : 1\n",
       "  _getTorchType : function: 0x04785a98\n",
       "  H5F_OBJ_FILE : 1\n",
       "  H5S_ALL : 0\n",
       "  H5F_OBJ_GROUP : 4\n",
       "  C : userdata: 0x04a70c50\n",
       "  H5P_DEFAULT : 0\n",
       "  _describeObject : function: 0x047bc430\n",
       "  H5Z_FILTER_NBIT : 5\n",
       "  _debugMode : false\n",
       "  _getObjectType : function: 0x047bc478\n",
       "  H5F_OBJ_ALL : 31\n",
       "  _getObjectName : function: 0x047da038\n",
       "  version : \n",
       "    {\n",
       "      1 : 1\n",
       "      2 : 8\n",
       "      3 : 15\n",
       "    }\n",
       "  H5Z_FILTER_SHUFFLE : 2\n",
       "  HDF5Group : table: 0x047c00f8\n",
       "  open : function: 0x04a76f48\n",
       "  H5Z_FILTER_SZIP : 4\n",
       "  H5F_OBJ_ATTR : 16\n",
       "  H5Z_FILTER_FLETCHER32 : 3\n",
       "  H5F_OBJ_DATATYPE : 8\n",
       "  debugMode : function: 0x0479dcf0\n",
       "  H5F_ACC_EXCL : 4\n",
       "  H5Z_FILTER_NONE : 0\n",
       "  _testUtils : \n",
       "    {\n",
       "      deepAlmostEq : function: 0x0479dcb0\n",
       "      withTmpDir : function: 0x047d5578\n",
       "    }\n",
       "  _nativeTypeForTensorType : function: 0x04d13fd0\n",
       "  H5F_ACC_TRUNC : 2\n",
       "  _config : \n",
       "    {\n",
       "      HDF5_INCLUDE_PATH : /Users/virgileaudi/anaconda/include\n",
       "      HDF5_LIBRARIES : /Users/virgileaudi/anaconda/lib/libhdf5.dylib;/Users/virgileaudi/anaconda/lib/libhdf5_hl.dylib;/Users/virgileaudi/anaconda/lib/libhdf5.dylib;/Users/virgileaudi/anaconda/lib/libz.dylib;/usr/lib/libdl.dylib;/usr/lib/libm.dylib\n",
       "    }\n",
       "  _loadObject : function: 0x04a76f28\n",
       "  DataSetOptions : table: 0x04794ea8\n",
       "  HDF5DataSet : table: 0x04776838\n",
       "  H5Z_FILTER_RESERVED : 256\n",
       "  _logger : \n",
       "    {\n",
       "      error : function: 0x047b6b50\n",
       "      warn : function: 0x047b6b50\n",
       "      debug : function: 0x047b6b98\n",
       "    }\n",
       "  H5F_ACC_RDONLY : 0\n",
       "  _fletcher32Available : function: 0x04a6f988\n",
       "  H5Z_FILTER_CONFIG_DECODE_ENABLED : 2\n",
       "  ffi : \n",
       "    {\n",
       "      abi : function: builtin#202\n",
       "      copy : function: builtin#200\n",
       "      errno : function: builtin#198\n",
       "      typeinfo : function: builtin#193\n",
       "      alignof : function: builtin#196\n",
       "      cdef : function: builtin#189\n",
       "      C : userdata: 0x043992e8\n",
       "      cast : function: builtin#191\n",
       "      load : function: builtin#205\n",
       "      offsetof : function: builtin#197\n",
       "      sizeof : function: builtin#195\n",
       "      string : function: builtin#199\n",
       "      metatype : function: builtin#203\n",
       "      new : function: builtin#190\n",
       "      arch : x64\n",
       "      os : OSX\n",
       "      gc : function: builtin#204\n",
       "      fill : function: builtin#201\n",
       "      istype : function: builtin#194\n",
       "      typeof : function: builtin#192\n",
       "    }\n",
       "  H5Z_FILTER_ERROR : -1\n",
       "  _outputTypeForTensorType : function: 0x04a6fa18\n",
       "  H5Z_FILTER_MAX : 65535\n",
       "  h5t : \n",
       "    {\n",
       "      STD_B32LE : 50331728\n",
       "      IEEE_F32BE : 50331703\n",
       "      NATIVE_B16 : 50331694\n",
       "      NATIVE_HSIZE : 50331698\n",
       "      NO_CLASS : -1\n",
       "      NATIVE_UINT_FAST32 : 50331681\n",
       "      STD_B8LE : 50331724\n",
       "      STD_I64BE : 50331715\n",
       "      NATIVE_LONG : 50331662\n",
       "      NATIVE_DOUBLE : 50331691\n",
       "      TIME : 2\n",
       "      NATIVE_INT16 : 50331670\n",
       "      NATIVE_LLONG : 50331688\n",
       "      STD_I8BE : 50331709\n",
       "      STD_B64LE : 50331730\n",
       "      NATIVE_HSSIZE : 50331699\n",
       "      STD_B32BE : 50331729\n",
       "      INTEGER : 0\n",
       "      NATIVE_INT64 : 50331682\n",
       "      STD_I8LE : 50331708\n",
       "      STD_I32LE : 50331712\n",
       "      NATIVE_SCHAR : 50331656\n",
       "      NATIVE_INT_FAST16 : 50331674\n",
       "      NATIVE_INT : 50331660\n",
       "      BITFIELD : 4\n",
       "      NATIVE_UINT_LEAST16 : 50331673\n",
       "      NATIVE_INT_LEAST16 : 50331672\n",
       "      IEEE_F64LE : 50331704\n",
       "      NATIVE_INT_FAST64 : 50331686\n",
       "      NATIVE_UINT_FAST64 : 50331687\n",
       "      NATIVE_UINT_LEAST64 : 50331685\n",
       "      NATIVE_INT_LEAST64 : 50331684\n",
       "      ENUM : 8\n",
       "      NATIVE_UINT64 : 50331683\n",
       "      NATIVE_HBOOL : 50331701\n",
       "      NATIVE_ULONG : 50331663\n",
       "      NATIVE_INT_FAST32 : 50331680\n",
       "      NATIVE_HADDR : 50331697\n",
       "      NATIVE_UINT : 50331661\n",
       "      NCLASSES : 11\n",
       "      NATIVE_UINT_LEAST32 : 50331679\n",
       "      NATIVE_INT_LEAST32 : 50331678\n",
       "      STD_U64LE : 50331722\n",
       "      NATIVE_UINT32 : 50331677\n",
       "      NATIVE_SHORT : 50331658\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "      NATIVE_INT32 : 50331676\n",
       "      VLEN : 9\n",
       "      ARRAY : 10\n",
       "      STD_U16LE : 50331718\n",
       "      STD_B16LE : 50331726\n",
       "      STD_I64LE : 50331714\n",
       "      NATIVE_UINT16 : 50331671\n",
       "      NATIVE_UINT_FAST8 : 50331669\n",
       "      STD_B16BE : 50331727\n",
       "      NATIVE_INT_FAST8 : 50331668\n",
       "      FLOAT : 1\n",
       "      REFERENCE : 7\n",
       "      STD_U32LE : 50331720\n",
       "      NATIVE_USHORT : 50331659\n",
       "      NATIVE_ULLONG : 50331689\n",
       "      NATIVE_INT8 : 50331664\n",
       "      IEEE_F32LE : 50331702\n",
       "      STD_U8BE : 50331717\n",
       "      NATIVE_INT_LEAST8 : 50331666\n",
       "      NATIVE_UINT8 : 50331665\n",
       "      NATIVE_B32 : 50331695\n",
       "      NATIVE_HERR : 50331700\n",
       "      NATIVE_OPAQUE : 50331736\n",
       "      NATIVE_LDOUBLE : 50331692\n",
       "      NATIVE_UINT_LEAST8 : 50331667\n",
       "      COMPOUND : 6\n",
       "      STD_REF_OBJ : 50331739\n",
       "      NATIVE_UINT_FAST16 : 50331675\n",
       "      NATIVE_B64 : 50331696\n",
       "      STD_U16BE : 50331719\n",
       "      STD_REF_DSETREG : 50331740\n",
       "      NATIVE_B8 : 50331693\n",
       "      STD_I32BE : 50331713\n",
       "      IEEE_F64BE : 50331705\n",
       "      NATIVE_FLOAT : 50331690\n",
       "      NATIVE_UCHAR : 50331657\n",
       "      STD_U32BE : 50331721\n",
       "      OPAQUE : 5\n",
       "      STD_B64BE : 50331731\n",
       "      STD_U8LE : 50331716\n",
       "      STD_I16BE : 50331711\n",
       "      STD_B8BE : 50331725\n",
       "      STRING : 3\n",
       "      STD_I16LE : 50331710\n",
       "      STD_U64BE : 50331723\n",
       "    }\n",
       "  HDF5File : table: 0x047bc3e8\n",
       "  H5Z_FILTER_SCALEOFFSET : 6\n",
       "  H5Z_FILTER_DEFLATE : 1\n",
       "  H5F_ACC_CREAT : 16\n",
       "  H5F_UNLIMITED : 18446744073709551615ULL\n",
       "  _deflateAvailable : function: 0x047cf040\n",
       "  _inDebugMode : function: 0x04a76f08\n",
       "  H5F_OBJ_LOCAL : 32\n",
       "  H5S_SELECT_SET : 0\n",
       "  _datatypeName : function: 0x04a80af0\n",
       "  H5F_ACC_DEBUG : 8\n",
       "  H5F_OBJ_DATASET : 2\n",
       "}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('SST1.hdf5','r')\n",
    "og_train_input = myFile:read('train_input'):all()\n",
    "og_test_input = myFile:read('test_input'):all()\n",
    "nfeatures = myFile:read('nfeatures'):all()\n",
    "og_valid_input = myFile:read('valid_input'):all()\n",
    "og_train_output = myFile:read('train_output'):all()\n",
    "og_valid_output = myFile:read('valid_output'):all()\n",
    "nclasses = myFile:read('nclasses'):all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation LogReg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function logreg(Xtrain,Ytrain,nfeatures,ep_max,m,eta,lambda)\n",
    "    \n",
    "    local shuffle = torch.LongTensor(Xtrain:size(1))\n",
    "    \n",
    "    local W = torch.cat(torch.zeros(1,5),torch.ones(nfeatures[1],5),1)\n",
    "    local b = torch.ones(5):view(-1,1)\n",
    "\n",
    "    local grad_W = torch.zeros(nfeatures[1]+1,5)\n",
    "    local grad_b = torch.zeros(5):view(-1,1)\n",
    "\n",
    "    local z = torch.ones(5):view(-1,1)\n",
    "    local yhat = torch.ones(5):view(-1,1)\n",
    "    local grad_L_dz = torch.ones(5):view(-1,1)\n",
    "    local grad_L_W = torch.zeros(nfeatures[1]+1,5)\n",
    "    \n",
    "    local c_s = 1\n",
    "    \n",
    "    for ep = 1,ep_max do\n",
    "        \n",
    "        timer = torch.Timer()\n",
    "\n",
    "        shuffle:copy(torch.randperm(Xtrain:size(1)):type('torch.LongTensor'))\n",
    "        \n",
    "        local tot_Loss = 0\n",
    "        \n",
    "        for it = 1,math.floor(Xtrain:size(1)/m) do\n",
    "\n",
    "            \n",
    "            grad_W:zero()\n",
    "            grad_b:zero()\n",
    "\n",
    "            for i = (m*(it-1)+1),m*it do\n",
    "\n",
    "                --current sentence:\n",
    "                c_s = shuffle[i]\n",
    "\n",
    "                --evaluate z:\n",
    "                z:copy(W:index(1,Xtrain[c_s]:type('torch.LongTensor')):sum(1):add(b)):view(-1,1)\n",
    "\n",
    "                --evaluate y_hat:\n",
    "                yhat = torch.exp(z)/(z:exp():sum())\n",
    "\n",
    "                --evaluate the loss:\n",
    "                if it == math.floor(Xtrain:size(1)/m) then\n",
    "                    tot_Loss = tot_Loss - z[Ytrain[c_s]][1] + math.log((z-(z:max())):exp():sum())+z:max()\n",
    "                end\n",
    "                --evaluate the gradients:\n",
    "                -- First with respect to dz (which is equal to db):\n",
    "\n",
    "                grad_L_dz:copy(yhat)\n",
    "                grad_L_dz[Ytrain[c_s]]:csub(1)\n",
    "\n",
    "\n",
    "                --Then with respect to dW:\n",
    "                grad_L_W:zero()\n",
    "                grad_L_W:indexAdd(1,Xtrain[c_s]:type('torch.LongTensor'),torch.expand(grad_L_dz,5,53):t())\n",
    "                grad_L_W[1]:zero()\n",
    "\n",
    "                --Update the gradients and total Loss\n",
    "                grad_W:add(grad_L_W*(1/m))\n",
    "                grad_b:add(grad_L_dz*1/m)\n",
    "                \n",
    "\n",
    "            end\n",
    "\n",
    "            --Update the parameters:\n",
    "            if (it%10) == 0 then\n",
    "                \n",
    "                W:mul(1-eta*lambda/(W:nElement()+b:nElement())):csub(grad_W:mul(eta))\n",
    "                b:mul(1-eta*lambda/(W:nElement()+b:nElement())):csub(grad_b:mul(eta))\n",
    "                \n",
    "                if it == math.floor(Xtrain:size(1)/m) then\n",
    "                    tot_Loss = Xtrain:size(1)*tot_Loss/m  + (0.5)*lambda*(torch.pow(W,2):sum()+torch.pow(b,2):sum())\n",
    "                end\n",
    "                \n",
    "            else\n",
    "                W:csub(grad_W:mul(eta))\n",
    "                b:csub(grad_b:mul(eta))\n",
    "                \n",
    "                if it == math.floor(Xtrain:size(1)/m) then\n",
    "                    tot_Loss = Xtrain:size(1)*tot_Loss/m  + (0.5)*lambda*(torch.pow(W,2):sum()+torch.pow(b,2):sum())\n",
    "                end\n",
    "                \n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        print('Time elapsed for epoch ' .. ep ..': ' .. timer:time().real .. ' seconds',\"\\n\")\n",
    "        print('Approximative Loss for the last batch for epoch ' .. ep ..': ' .. tot_Loss,\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return W,b,tot_Loss\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function predict(Xvalid,W,b,Yvalid)\n",
    "    local Yvalid = Yvalid or Nil\n",
    "    local z = torch.zeros(5):view(-1,1)\n",
    "    local Ypred = torch.IntTensor(Xvalid:size()[1])\n",
    "    local max = torch.zeros(1)\n",
    "    local accu = 0\n",
    "    \n",
    "    for i = 1,Xvalid:size()[1] do\n",
    "        z:copy(W:index(1,Xvalid[i]:type('torch.LongTensor')):sum(1):add(b)):view(-1,1)\n",
    "        max, Ypred[i] = z:max(1)\n",
    "        \n",
    "        if Yvalid then\n",
    "            if Yvalid[i] == Ypred[i] then\n",
    "                accu = accu + 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if Yvalid then\n",
    "        print(\"Accuracy: \" .. accu/Xvalid:size()[1])\n",
    "        return Ypred,accu/(Xvalid:size()[1])\n",
    "    else\n",
    "        return Ypred\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hyperparam_ = torch.zeros(27,4)\n",
    "m_  = torch.Tensor({10,20,50})\n",
    "eta_ = torch.Tensor({0.1,1,10})\n",
    "lambda_ = torch.Tensor({0.1,1,10})\n",
    "\n",
    "for i = 0,2 do\n",
    "    for z = 1,9 do\n",
    "        Hyperparam_[9*i+z][1] = m_[i+1]\n",
    "    end\n",
    "end\n",
    "\n",
    "for i = 1,3 do\n",
    "        for z = 0,2 do\n",
    "            Hyperparam_[z*9+i][2] = eta_[1]\n",
    "        end\n",
    "end\n",
    "\n",
    "for i = 4,6 do\n",
    "        for z = 0,2 do\n",
    "            Hyperparam_[z*9+i][2] = eta_[2]\n",
    "        end\n",
    "end\n",
    "\n",
    "for i = 7,9 do\n",
    "        for z = 0,2 do\n",
    "            Hyperparam_[z*9+i][2] = eta_[3]\n",
    "        end\n",
    "end\n",
    "\n",
    "a = torch.Tensor({0.1,1,10})\n",
    "for i =1,8 do\n",
    "    a = torch.cat(a,torch.Tensor({0.1,1,10}),1)\n",
    "end\n",
    "\n",
    "Hyperparam_:narrow(2,3,1):copy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model 1\t\n",
       "\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i = 1,27 do\n",
    "    print('Model '..i,'\\n')\n",
    "    torch.manualSeed(123)\n",
    "    W, b, L = logreg(og_train_input,og_train_output,nfeatures,1,Hyperparam_[i][1],Hyperparam_[i][2],Hyperparam_[i][3])\n",
    "    Validpred, Hyperparam_[i][4] = predict(og_valid_input,W,b,og_valid_output)\n",
    "    print('\\n')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max,argmax = Hyperparam_:max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hyperparam_[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "math.randomseed(1234)\n",
    "W, b, L = logreg(og_train_input,og_train_output,nfeatures,1,50,10,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Validpred, acc_val = predict(og_valid_input,W,b,og_valid_output)\n",
    "Trainpred, acc_train  = predict(og_train_input,W,b,og_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "math.randomseed(1234)\n",
    "W, b, L = logreg(og_train_input,og_train_output,nfeatures,1,20,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossval SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function linearSVM(Xtrain,Ytrain,nfeatures,ep_max,m,eta,lambda)\n",
    "    local shuffle = torch.LongTensor(Xtrain:size(1))\n",
    "    \n",
    "    local W = torch.cat(torch.zeros(1,5),torch.ones(nfeatures[1],5),1)\n",
    "    local b = torch.ones(5):view(-1,1)\n",
    "\n",
    "    local grad_W = torch.zeros(nfeatures[1]+1,5)\n",
    "    local grad_b = torch.zeros(5):view(-1,1)\n",
    "\n",
    "    local yhat = torch.ones(5):view(-1,1)\n",
    "    local y_temp = torch.zeros(5):view(-1,1)\n",
    "    local grad_L_dy = torch.ones(5):view(-1,1)\n",
    "    local grad_L_W = torch.zeros(nfeatures[1]+1,5)\n",
    "    \n",
    "    local c_s = 1\n",
    "    \n",
    "    for ep = 1,ep_max do\n",
    "        \n",
    "        timer = torch.Timer()\n",
    "\n",
    "        shuffle:copy(torch.randperm(Xtrain:size(1)):type('torch.LongTensor'))\n",
    "        \n",
    "        local tot_Loss = 0\n",
    "        \n",
    "        for it = 1,math.floor(Xtrain:size(1)/m) do\n",
    "\n",
    "            tot_Loss = 0\n",
    "            grad_W:zero()\n",
    "            grad_b:zero()\n",
    "\n",
    "            for i = (m*(it-1)+1),m*it do\n",
    "\n",
    "                --current sentence:\n",
    "                c_s = shuffle[i]\n",
    "\n",
    "                --evaluate y_hat:\n",
    "                y_hat = W:index(1,Xtrain[c_s]:type('torch.LongTensor')):sum(1):add(b):view(-1,1)\n",
    "\n",
    "                --evaluate the loss:\n",
    "                \n",
    "                y_temp:copy(y_hat)\n",
    "                y_temp[Ytrain[c_s]]:fill(-9999)\n",
    "                max_c,argmax_c = y_temp:max(1)\n",
    "                \n",
    "                if it == math.floor(Xtrain:size(1)/m) then\n",
    "                    tot_Loss = tot_Loss + math.max(0,1-y_hat[Ytrain[c_s]][1]+max_c[1][1])\n",
    "                end\n",
    "                \n",
    "                -- First with respect to dy (which is equal to db):\n",
    "\n",
    "                grad_L_dy:zero()\n",
    "                if (y_hat[Ytrain[c_s]][1]-max_c[1][1])<1 then\n",
    "                    grad_L_dy[Ytrain[c_s]] = -1\n",
    "                    grad_L_dy[argmax_c[1][1]] = 1\n",
    "                end\n",
    "\n",
    "                --evaluate the gradients:\n",
    "                --Then with respect to dW:\n",
    "                grad_L_W:zero()\n",
    "                grad_L_W:indexAdd(1,Xtrain[c_s]:type('torch.LongTensor'),torch.expand(grad_L_dy,5,53):t())\n",
    "                grad_L_W[1]:zero()\n",
    "\n",
    "                --Update the gradients and total Loss\n",
    "                grad_W:add(grad_L_W*(1/m))\n",
    "                grad_b:add(grad_L_dy*(1/m))\n",
    "\n",
    "            end\n",
    "\n",
    "            --Update the parameters:\n",
    "            \n",
    "            if (it%10) == 0 then \n",
    "                W:mul(1-eta*lambda/(W:nElement()+b:nElement())):csub(grad_W:mul(eta))\n",
    "                b:mul(1-eta*lambda/(W:nElement()+b:nElement())):csub(grad_b:mul(eta))\n",
    "                \n",
    "                if it == math.floor(Xtrain:size(1)/m) then\n",
    "                    tot_Loss = Xtrain:size(1)*tot_Loss/m  + (0.5)*lambda*(torch.pow(W,2):sum()+torch.pow(b,2):sum())\n",
    "                end\n",
    "                \n",
    "            else\n",
    "                W:csub(grad_W:mul(eta))\n",
    "                b:csub(grad_b:mul(eta))\n",
    "                if it == math.floor(Xtrain:size(1)/m) then\n",
    "                    tot_Loss = Xtrain:size(1)*tot_Loss/m + (0.5)*lambda*(torch.pow(W,2):sum()+torch.pow(b,2):sum())\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        print('Time elapsed for epoch ' .. ep ..': ' .. timer:time().real .. ' seconds',\"\\n\")\n",
    "        print('Approximative Loss for epoch ' .. ep ..': ' .. tot_Loss,\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return W,b,tot_Loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hyperparam = torch.zeros(27,4)\n",
    "m_  = torch.Tensor({10,20,50})\n",
    "eta_ = torch.Tensor({0.1,1,10})\n",
    "lambda_ = torch.Tensor({0.1,1,10})\n",
    "\n",
    "for i = 0,2 do\n",
    "    for z = 1,9 do\n",
    "        Hyperparam[9*i+z][1] = m_[i+1]\n",
    "    end\n",
    "end\n",
    "\n",
    "for i = 1,3 do\n",
    "        for z = 0,2 do\n",
    "            Hyperparam[z*9+i][2] = eta_[1]\n",
    "        end\n",
    "end\n",
    "\n",
    "for i = 4,6 do\n",
    "        for z = 0,2 do\n",
    "            Hyperparam[z*9+i][2] = eta_[2]\n",
    "        end\n",
    "end\n",
    "\n",
    "for i = 7,9 do\n",
    "        for z = 0,2 do\n",
    "            Hyperparam[z*9+i][2] = eta_[3]\n",
    "        end\n",
    "end\n",
    "\n",
    "a = torch.Tensor({0.1,1,10})\n",
    "for i =1,8 do\n",
    "    a = torch.cat(a,torch.Tensor({0.1,1,10}),1)\n",
    "end\n",
    "\n",
    "Hyperparam:narrow(2,3,1):copy(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i = 1,27 do\n",
    "    print('Model '..i,'\\n')\n",
    "    math.randomseed(1234)\n",
    "    Ws, bs, Ls = linearSVM(og_train_input,og_train_output,nfeatures,1,Hyperparam[i][1],Hyperparam[i][2],Hyperparam[i][3])\n",
    "    Validpreds, Hyperparam[i][4] = predict(og_valid_input,Ws,bs,og_valid_output)\n",
    "    print('\\n')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max,argmax = Hyperparam:max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "argmax[1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hyperparam[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "math.randomseed(1234)\n",
    "Ws, bs, Ls = linearSVM(og_train_input,og_train_output,nfeatures,3,50,1,0.1)\n",
    "Validpreds, accus = predict(og_valid_input,Ws,bs,og_valid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "math.randomseed(1234)\n",
    "Ws, bs, Ls = linearSVM(og_train_input,og_train_output,nfeatures,5,50,1,10)\n",
    "Validpreds, accus = predict(og_valid_input,Ws,bs,og_valid_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LOGREG on test with best combi from CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "math.randomseed(1234)\n",
    "W, b, L = logreg(og_train_input,og_train_output,nfeatures,1,Hyperparam_[25][1],Hyperparam_[25][2],Hyperparam_[25][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Testpred = predict(og_test_input,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myFile = hdf5.open('Testpred_1.h5', 'w')\n",
    "myFile:write('Testpred', Testpred)\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function logreg(Xtrain, Ytrain, nfeatures, nclasses, ep_max, batch_size, eta, lambda)\n",
    "    \n",
    "    -- Initialization\n",
    "    local nfeatures = nfeatures[1]\n",
    "    local nclasses = nclasses[1]\n",
    "    local shuffle = torch.LongTensor(Xtrain:size(1))\n",
    "    local W = torch.cat(torch.zeros(1,nclasses),torch.ones(nfeatures, nclasses),1)\n",
    "    local b = torch.ones(nclasses, 1)\n",
    "    local grad_W = torch.zeros(nfeatures+1,nclasses)\n",
    "    local grad_b = torch.zeros(nclasses, 1)\n",
    "    local z = torch.ones(nclasses, 1)\n",
    "    local yhat = torch.ones(nclasses, 1)\n",
    "    local grad_L_dz = torch.ones(nclasses, 1)\n",
    "    local grad_L_W = torch.zeros(nfeatures+1,nclasses)\n",
    "    \n",
    "\n",
    "    \n",
    "    local c_s = 1\n",
    "    local it_max = math.floor(Xtrain:size(1)/batch_size)\n",
    "    \n",
    "    for ep = 1,ep_max do\n",
    "        timer = torch.Timer()\n",
    "        shuffle:copy(torch.randperm(Xtrain:size(1)):type('torch.LongTensor'))\n",
    "        tot_Loss = 0\n",
    "        \n",
    "        for it = 1,it_max do\n",
    "            -- Initializing gradient for the current iteration\n",
    "            grad_W:zero()\n",
    "            grad_b:zero()\n",
    "\n",
    "            for i = (batch_size*(it-1)+1),batch_size*it do\n",
    "                --current sentence:\n",
    "                c_s = shuffle[i]\n",
    "                --evaluate z:\n",
    "                z:copy(W:index(1,Xtrain[c_s]:type('torch.LongTensor')):sum(1):add(b)):view(-1,1)\n",
    "                --evaluate y_hat:\n",
    "                yhat:copy(torch.exp(z))\n",
    "                yhat:div(yhat:sum())\n",
    "\n",
    "                 --evaluate the loss (only on the last iteration for the logger)\n",
    "                if it == it_max then\n",
    "                    tot_Loss = tot_Loss - z[Ytrain[c_s]][1] + math.log((z-(z:max())):exp():sum())+z:max()\n",
    "                end\n",
    "                --evaluate the gradients:\n",
    "                -- First with respect to dz (which is equal to db):\n",
    "                grad_L_dz:copy(yhat)\n",
    "                grad_L_dz[Ytrain[c_s]]:csub(1)\n",
    "                --Then with respect to dW:\n",
    "                grad_L_W:zero()\n",
    "                grad_L_W:indexAdd(1,Xtrain[c_s]:type('torch.LongTensor'),torch.expand(grad_L_dz,nclasses,53):t())\n",
    "                grad_L_W[1]:zero()\n",
    "                --Update the gradients\n",
    "                grad_W:add(grad_L_W*(1/batch_size))\n",
    "                grad_b:add(grad_L_dz*(1/batch_size))\n",
    "            end\n",
    "\n",
    "            -- Apply the regularization every 10 iteratin to gain speed (possible because of the dataset sparsity)\n",
    "            if (it%10) == 0 then\n",
    "                W:mul(1-eta*lambda/(W:nElement()+b:nElement())):csub(grad_W:mul(eta))\n",
    "                b:mul(1-eta*lambda/(W:nElement()+b:nElement())):csub(grad_b:mul(eta))\n",
    "            else\n",
    "                W:csub(grad_W:mul(eta))\n",
    "                b:csub(grad_b:mul(eta))\n",
    "            end\n",
    "            -- Updating the loss with the regularization\n",
    "            if it == it_max then\n",
    "                tot_Loss = Xtrain:size(1)*tot_Loss/batch_size + (0.5)*lambda*(torch.pow(W,2):sum()+torch.pow(b,2):sum())\n",
    "            end\n",
    "        end      \n",
    "        print('Time elapsed for epoch ' .. ep ..': ' .. timer:time().real .. ' seconds')\n",
    "        print('Approximative Loss for the last batch for epoch ' .. ep ..': ' .. tot_Loss)\n",
    "    end\n",
    "    return W, b, tot_Loss\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time elapsed for epoch 1: 97.107171058655 seconds\t\n",
       "Approximative Loss for the last batch for epoch 1: 269419.08690127\t\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.randomseed(1234)\n",
    "W,b,L = logreg(og_train_input, og_train_output, nfeatures, nclasses, 1, 50, 10, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy: 0.38328792007266\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validpred, accu = predict(og_valid_input,W,b,og_valid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
