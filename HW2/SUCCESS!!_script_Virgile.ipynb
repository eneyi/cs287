{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'hdf5'\n",
    "\n",
    "local Squeeze, parent = torch.class('nn.Squeeze', 'nn.Module')\n",
    "\n",
    "function Squeeze:updateOutput(input)\n",
    "    self.size = input:size()\n",
    "    self.output = input:squeeze()\n",
    "    return self.output\n",
    "end\n",
    "\n",
    "function Squeeze:updateGradInput(input, gradOutput)\n",
    "  self.gradInput = gradOutput:view(self.size)\n",
    "  return self.gradInput  \n",
    "end\n",
    "\n",
    "myFile = hdf5.open('PTB.hdf5','r')\n",
    "data = myFile:all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input_word_windows = data['train_input_word_windows']\n",
    "train_input_cap_windows = data['train_input_cap_windows']\n",
    "\n",
    "train = train_input_word_windows:clone()\n",
    "train_cap = train_input_cap_windows:clone()\n",
    "train_output = data['train_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet = nn.Sequential()\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet:add(nn.LookupTable(data['nwords'][1],dim_hidden))\n",
    "neuralnet:add(nn.View(1,-1,5*50))\n",
    "neuralnet:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet:add(nn.Linear(5*dim_hidden,dim_hidden2))\n",
    "neuralnet:add(nn.HardTanh())\n",
    "neuralnet:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet:add(nn.LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset={};\n",
    "for i=1,train:size(1) do \n",
    "  dataset[i] = {train[i]:view(1,5), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timer = torch.Timer()\n",
    "\n",
    "trainer = nn.StochasticGradient(neuralnet, criterion)\n",
    "trainer.learningRate = 0.001\n",
    "trainer.maxIteration = 10\n",
    "trainer:train(dataset)\n",
    "\n",
    "print('Time elapsed: ' .. timer:time().real .. ' seconds',\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "val_word = data['valid_input_word_windows']:clone()\n",
    "val_output = data['valid_output']:clone()\n",
    "\n",
    "pred_train = neuralnet:forward(train)\n",
    "max,argmax_train = pred_train:max(2)\n",
    "acc_train = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train[i][1] == train_output[i] then\n",
    "        acc_train = acc_train + 1\n",
    "    end\n",
    "end\n",
    "score_train = acc_train/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_val = neuralnet:forward(val_word)\n",
    "max,argmax_val = pred_val:max(2)\n",
    "acc_val = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val[i][1] == data['valid_output'][i] then\n",
    "        acc_val = acc_val + 1\n",
    "    end\n",
    "end\n",
    "score_val = acc_val/data['valid_output']:size(1)\n",
    "print('Validation Score on Validation is '..score_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding the cap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_new = train_input_word_windows:clone()\n",
    "train_cap_new = train_cap:clone()\n",
    "train_cap_new:add(100002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new = torch.cat(train_new,train_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet_wc = nn.Sequential()\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet_wc:add(nn.LookupTable(data['nwords'][1]+4,dim_hidden))\n",
    "neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "neuralnet_wc:add(nn.HardTanh())\n",
    "neuralnet_wc:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet_wc:add(nn.LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2={};\n",
    "for i=1,train:size(1) do \n",
    "  dataset2[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset2:size() return train:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer2 = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "trainer2.learningRate = 0.01\n",
    "trainer2.maxIteration = 3\n",
    "trainer2:train(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train2 = neuralnet_wc:forward(train_new)\n",
    "max,argmax_train2 = pred_train2:max(2)\n",
    "acc_train2 = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train2[i][1] == train_output[i] then\n",
    "        acc_train2 = acc_train2 + 1\n",
    "    end\n",
    "end\n",
    "score_train2 = acc_train2/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_new = data['valid_input_word_windows']:clone()\n",
    "valid_cap_new = data['valid_input_cap_windows']:clone()\n",
    "valid_cap_new:add(100002)\n",
    "valid_new = torch.cat(valid_new,valid_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_val2 = neuralnet_wc:forward(valid_new)\n",
    "max,argmax_val2 = pred_val2:max(2)\n",
    "acc_val2 = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val2[i][1] == data['valid_output'][i] then\n",
    "        acc_val2 = acc_val2 + 1\n",
    "    end\n",
    "end\n",
    "score_val2 = acc_val2/data['valid_output']:size(1)\n",
    "print('Validation Score on Train is '..score_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new = data['train_input_word_windows']\n",
    "train_cap_new = data['train_input_cap_windows']\n",
    "train_output = data['train_output']\n",
    "train_cap_new:add(100002)\n",
    "train_new = torch.cat(train_new,train_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet_wc = nn.Sequential()\n",
    "\n",
    "par = nn.LookupTable(data['nwords'][1]+4,dim_hidden)\n",
    "word_embeddings = data['word_embeddings']\n",
    "par.weight:narrow(1, 1, data['nwords'][1]):copy(word_embeddings:narrow(1, 1, data['nwords'][1]))\n",
    "print(par.weight[1][1], word_embeddings[1][1])\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet_wc:add(par)\n",
    "neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "neuralnet_wc:add(nn.HardTanh())\n",
    "neuralnet_wc:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet_wc:add(nn.LogSoftMax())\n",
    "\n",
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2={};\n",
    "for i=1,train_new:size(1) do \n",
    "  dataset2[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset2:size() return train_new:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timer = torch.Timer()\n",
    "\n",
    "trainer2 = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "trainer2.learningRate = 0.01\n",
    "trainer2.maxIteration = 3\n",
    "trainer2:train(dataset2)\n",
    "\n",
    "print('Time elapsed: ' .. timer:time().real .. ' seconds',\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Evaluation\n",
    "valid = data['valid_input_word_windows']\n",
    "valid_cap = data['valid_input_cap_windows']\n",
    "valid_new = torch.cat(valid,torch.add(valid_cap,100002) ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- On train set\n",
    "\n",
    "pred_train2 = neuralnet_wc:forward(train_new)\n",
    "max,argmax_train2 = pred_train2:max(2)\n",
    "acc_train2 = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train2[i][1] == train_output[i] then\n",
    "        acc_train2 = acc_train2 + 1\n",
    "    end\n",
    "end\n",
    "score_train2 = acc_train2/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train2)\n",
    "\n",
    "\n",
    "-- On validation set\n",
    "pred_val2 = neuralnet_wc:forward(valid_new)\n",
    "max,argmax_val2 = pred_val2:max(2)\n",
    "acc_val2 = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val2[i][1] == data['valid_output'][i] then\n",
    "        acc_val2 = acc_val2 + 1\n",
    "    end\n",
    "end\n",
    "score_val2 = acc_val2/data['valid_output']:size(1)\n",
    "print('Validation Score on Train is '..score_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all the following cells until the one called SCRIPT TO TUNE. This one is the longest to run, there are 2 loops: one over the parameters to test (here over dim_hidden) and one over the number of iterations (5 iterations of 4 epoch), the latter is to log more information about loss and accuracy and attest that the model converges.\n",
    "In between each iteration of the outer loop the script rewrites in the log files in case of failure.\n",
    "\n",
    "Then the last cell is to load the saved files. The goal is to use the two log of accuracy_train and accuracy_valid to select the best model and then retrieve its test prediction to submit them and take the lead!!\n",
    "We will have enough log to have some nice plots too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Helper\n",
    "\n",
    "function compute_accuracy(pred, true_)\n",
    "    max,argmax = pred:max(2)\n",
    "    acc = 0\n",
    "    for i = 1, true_:size(1) do\n",
    "        if argmax[i][1] == true_[i] then\n",
    "            acc = acc + 1\n",
    "        end\n",
    "    end\n",
    "    score = acc/true_:size(1)\n",
    "    \n",
    "    return score\n",
    "end\n",
    "\n",
    "-- If words_embeddings is nil, weight are initialized randomly by torch\n",
    "function build_nn(nwords, ncap, nclasses, dim_hidden, dim_hidden2, word_embeddings)\n",
    "    --Define the module\n",
    "    neuralnet_wc = nn.Sequential()\n",
    "\n",
    "    par = nn.LookupTable(nwords + ncap,dim_hidden)\n",
    "    \n",
    "    -- Adding the embeddings\n",
    "    if word_embeddings then\n",
    "        par.weight:narrow(1, 1, nwords):copy(word_embeddings:narrow(1, 1, nwords))\n",
    "    end\n",
    "    neuralnet_wc:add(par)\n",
    "\n",
    "    neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "    neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "    neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "    neuralnet_wc:add(nn.HardTanh())\n",
    "    neuralnet_wc:add(nn.Linear(dim_hidden2, nclasses))\n",
    "    neuralnet_wc:add(nn.LogSoftMax())\n",
    "    \n",
    "    return neuralnet_wc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Loading the data\n",
    "\n",
    "-- dimension\n",
    "nwords = data['nwords'][1]\n",
    "ncap = 4\n",
    "nclasses = data['nclasses'][1]\n",
    "\n",
    "-- Training data\n",
    "train_output = data['train_output']\n",
    "train_new = torch.cat(data['train_input_word_windows'],\n",
    "    torch.add(data['train_input_cap_windows'], 100002),2)\n",
    "\n",
    "-- Validation data\n",
    "valid_output = data['valid_output']\n",
    "valid_new = torch.cat(data['valid_input_word_windows'],\n",
    "    torch.add(data['valid_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Test data\n",
    "test_new = torch.cat(data['test_input_word_windows'],\n",
    "    torch.add(data['test_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Formating the dataset to train\n",
    "dataset={};\n",
    "for i=1,train_new:size(1) do \n",
    "  dataset[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train_new:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Hyperparameter grid choice\n",
    "\n",
    "-- Place to change parameter if needed\n",
    "dim_hidden2_list = torch.DoubleTensor({40, 50, 60, 80, 100, 120})\n",
    "num_parameters = dim_hidden2_list:size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Metric storage\n",
    "\n",
    "-- Initialization\n",
    "loss_train = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "training_accuracy = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "valid_accuracy = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "training_time = torch.zeros(num_parameters)\n",
    "test_pred = torch.zeros(test_new:size(1),45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_hidden2_list:size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Script\n",
    "\n",
    "-- REquired value because of the embeddings\n",
    "dim_hidden = 50\n",
    "\n",
    "for d = 1,dim_hidden2_list:size(1) do\n",
    "--for d=1,1 do\n",
    "    dim_hidden2 = dim_hidden2_list[d]\n",
    "\n",
    "    --Define the model\n",
    "    neuralnet_wc = build_nn(nwords, ncap, nclasses, dim_hidden, dim_hidden2, data['word_embeddings'])\n",
    "    criterion = nn.ClassNLLCriterion()\n",
    "    \n",
    "    -- Training\n",
    "    timer = torch.Timer()\n",
    "    trainer = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "    trainer.learningRate = 0.01\n",
    "    trainer.maxIteration = 4\n",
    "    \n",
    "    for j=1, 5 do\n",
    "        trainer:train(dataset)\n",
    "        \n",
    "        -- Pred on train\n",
    "        pred_val = neuralnet_wc:forward(valid_new)\n",
    "        valid_accuracy[d][j] = compute_accuracy(pred_val, valid_output)\n",
    "        \n",
    "        pred_train = neuralnet_wc:forward(train_new)\n",
    "        training_accuracy[d][j] = compute_accuracy(pred_train, train_output)\n",
    "        loss_train[d][j] = criterion:forward(pred_train, train_output)\n",
    "    end\n",
    "\n",
    "    training_time[d] = timer:time().real\n",
    "    test_pred = neuralnet_wc:forward(test_new)\n",
    "    \n",
    "    -- Saving (rewriting the files with update at each iteration)\n",
    "    filename = 'log_hyper_embedding.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('valid_accuracy', valid_accuracy)\n",
    "    myFile:write('training_accuracy', training_accuracy)\n",
    "    myFile:write('training_time', training_time)\n",
    "    myFile:write('loss_train', loss_train)\n",
    "    myFile:close()\n",
    "    \n",
    "    -- Saving the pred for each hyperparameter\n",
    "    filename = 'test_pred_embedding_h_'.. d .. '.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('test_pred', test_pred)\n",
    "    myFile:close()    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--- Script to read the saved files\n",
    "\n",
    "myFile = hdf5.open('log_hyper_embedding.f5','r')\n",
    "hyperparameter_log = myFile:all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparameter_log['loss_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparameter_log['training_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparameter_log['valid_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "myFile = hdf5.open('test_pred_embedding_h_'.. d .. '.f5','r')\n",
    "test_pred_3 = myFile:all()\n",
    "myFile:close()\n",
    "max,argmax_3 = test_pred_3['test_pred']:max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred_3['test_pred']:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'tocsv_3.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('test_pred', argmax_3)\n",
    "myFile:close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 4\n",
    "myFile = hdf5.open('test_pred_embedding_h_'.. d .. '.f5','r')\n",
    "test_pred_4 = myFile:all()\n",
    "myFile:close()\n",
    "max,argmax_4 = test_pred_4['test_pred']:max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'tocsv_4.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('test_pred', argmax_4)\n",
    "myFile:close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fixing the embedings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Loading the data\n",
    "\n",
    "-- dimension\n",
    "nwords = data['nwords'][1]\n",
    "ncap = 4\n",
    "nclasses = data['nclasses'][1]\n",
    "\n",
    "-- Training data\n",
    "train_output = data['train_output']\n",
    "train_new = torch.cat(data['train_input_word_windows'],\n",
    "    torch.add(data['train_input_cap_windows'], 100002),2)\n",
    "\n",
    "-- Validation data\n",
    "valid_output = data['valid_output']\n",
    "valid_new = torch.cat(data['valid_input_word_windows'],\n",
    "    torch.add(data['valid_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Test data\n",
    "test_new = torch.cat(data['test_input_word_windows'],\n",
    "    torch.add(data['test_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Formating the dataset to train\n",
    "dataset={};\n",
    "for i=1,train_new:size(1) do \n",
    "  dataset[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train_new:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_hidden2_list2 = torch.DoubleTensor({50, 60, 80})\n",
    "num_parameters2 = dim_hidden2_list2:size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_parameters2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Metric storage\n",
    "\n",
    "-- Initialization\n",
    "loss_train2 = torch.zeros(num_parameters2, 20) -- Store every 1 epochs\n",
    "training_accuracy2 = torch.zeros(num_parameters2, 20) -- Store every 1 epochs\n",
    "valid_accuracy2 = torch.zeros(num_parameters2, 20) -- Store every 1 epochs\n",
    "training_time2 = torch.zeros(num_parameters2)\n",
    "test_pred2 = torch.zeros(test_new:size(1),45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.2993823805604\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.2993823805604\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.22768876417716\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.22768876417716\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.21530632437137\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.21530632437137\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.20614224246429\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.20614224246429\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.20029157760528\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.20029157760528\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.19622459682985\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.19622459682985\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.19199444337835\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.19199444337835\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.18813224924488\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.18813224924488\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.186796782327\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.186796782327\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.18399098774875\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.18399098774875\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.1816361139745\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.1816361139745\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.18133891236475\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.18133891236475\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.18057696832087\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.18057696832087\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.17923223516813\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.17923223516813\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.17854454619313\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.17854454619313\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.17737269117876\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.17737269117876\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.17681429480513\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.17681429480513\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.17644300133743\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.17644300133743\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.17542851477198\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.17542851477198\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.17400905746581\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.17400905746581\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "...rs/virgileaudi/torch/install/share/lua/5.1/hdf5/file.lua:12: HDF5File: fileID -1 is not valid\nstack traceback:\n\t[C]: in function 'error'\n\t...rs/virgileaudi/torch/install/share/lua/5.1/hdf5/file.lua:12: in function '__init'\n\t...s/virgileaudi/torch/install/share/lua/5.1/torch/init.lua:91: in function <...s/virgileaudi/torch/install/share/lua/5.1/torch/init.lua:87>\n\t[C]: in function 'open'\n\t[string \"-- Script...\"]:37: in main chunk\n\t[C]: in function 'xpcall'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:179: in function <.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t.../virgileaudi/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010fc99b50",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...rs/virgileaudi/torch/install/share/lua/5.1/hdf5/file.lua:12: HDF5File: fileID -1 is not valid\nstack traceback:\n\t[C]: in function 'error'\n\t...rs/virgileaudi/torch/install/share/lua/5.1/hdf5/file.lua:12: in function '__init'\n\t...s/virgileaudi/torch/install/share/lua/5.1/torch/init.lua:91: in function <...s/virgileaudi/torch/install/share/lua/5.1/torch/init.lua:87>\n\t[C]: in function 'open'\n\t[string \"-- Script...\"]:37: in main chunk\n\t[C]: in function 'xpcall'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:179: in function <.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t.../virgileaudi/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rgileaudi/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t.../virgileaudi/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010fc99b50"
     ]
    }
   ],
   "source": [
    "-- Script\n",
    "\n",
    "-- REquired value because of the embeddings\n",
    "dim_hidden = 50\n",
    "\n",
    "for d = 1,dim_hidden2_list2:size(1) do\n",
    "--for d=1,1 do\n",
    "    dim_hidden2 = dim_hidden2_list2[d]\n",
    "\n",
    "    --Define the model\n",
    "    neuralnet_wc = build_nn(nwords, ncap, nclasses, dim_hidden, dim_hidden2, data['word_embeddings'])\n",
    "    criterion = nn.ClassNLLCriterion()\n",
    "    \n",
    "    -- Training\n",
    "    timer = torch.Timer()\n",
    "    trainer = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "    trainer.learningRate = 0.01\n",
    "    trainer.maxIteration = 1\n",
    "    \n",
    "    for j=1, 20 do\n",
    "        trainer:train(dataset)\n",
    "        neuralnet_wc:get(1).weight:narrow(1, 1, nwords):copy(data['word_embeddings']:narrow(1, 1, nwords))\n",
    "        -- Pred on train\n",
    "        pred_val = neuralnet_wc:forward(valid_new)\n",
    "        valid_accuracy2[d][j] = compute_accuracy(pred_val, valid_output)\n",
    "        \n",
    "        pred_train = neuralnet_wc:forward(train_new)\n",
    "        training_accuracy2[d][j] = compute_accuracy(pred_train, train_output)\n",
    "        loss_train2[d][j] = criterion:forward(pred_train, train_output)\n",
    "    end\n",
    "\n",
    "    training_time2[d] = timer:time().real\n",
    "    test_pred2 = neuralnet_wc:forward(test_new)\n",
    "    \n",
    "    -- Saving (rewriting the files with update at each iteration)\n",
    "    filename = 'log_hyper_embedding2.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('valid_accuracy2', valid_accuracy2)\n",
    "    myFile:write('training_accuracy2', training_accuracy2)\n",
    "    myFile:write('training_time2', training_time2)\n",
    "    myFile:write('loss_train2', loss_train2)\n",
    "    myFile:close()\n",
    "    \n",
    "    -- Saving the pred for each hyperparameter\n",
    "    filename = 'test_pred_embedding_h_'.. d .. '_2.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('test_pred', test_pred2)\n",
    "    myFile:close()    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neuralnet_wc:get(1).weight:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 10\n",
       " 0.6333  0.6696  0.5631  0.6084  0.6280  0.6356  0.6716  0.5664  0.6027  0.6449\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 11 to 20\n",
       " 0.6734  0.5784  0.6078  0.5377  0.5950  0.6517  0.6828  0.6016  0.6042  0.5810\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "[torch.DoubleTensor of size 3x20]\n",
       "\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD LAYER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Helper\n",
    "\n",
    "function compute_accuracy(pred, true_)\n",
    "    max,argmax = pred:max(2)\n",
    "    acc = 0\n",
    "    for i = 1, true_:size(1) do\n",
    "        if argmax[i][1] == true_[i] then\n",
    "            acc = acc + 1\n",
    "        end\n",
    "    end\n",
    "    score = acc/true_:size(1)\n",
    "    \n",
    "    return score\n",
    "end\n",
    "\n",
    "-- If words_embeddings is nil, weight are initialized randomly by torch\n",
    "function build_nn2(nwords, ncap, nclasses, dim_hidden, dim_hidden2, dim_hidden3, word_embeddings)\n",
    "    --Define the module\n",
    "    neuralnet_wc = nn.Sequential()\n",
    "\n",
    "    par = nn.LookupTable(nwords + ncap,dim_hidden)\n",
    "    \n",
    "    -- Adding the embeddings\n",
    "    if word_embeddings then\n",
    "        par.weight:narrow(1, 1, nwords):copy(word_embeddings:narrow(1, 1, nwords))\n",
    "    end\n",
    "    neuralnet_wc:add(par)\n",
    "\n",
    "    neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "    neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "    neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "    neuralnet_wc:add(nn.HardTanh())\n",
    "    neuralnet_wc:add(nn.Linear(dim_hidden2, dim_hidden3))\n",
    "    neuralnet_wc:add(nn.HardTanh())\n",
    "    neuralnet_wc:add(nn.Linear(dim_hidden3, nclasses))\n",
    "    neuralnet_wc:add(nn.LogSoftMax())\n",
    "    \n",
    "    return neuralnet_wc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Metric storage\n",
    "\n",
    "-- Initialization\n",
    "loss_train3 = torch.zeros(2, 5) -- Store every 4 epochs\n",
    "training_accuracy3 = torch.zeros(2, 5) -- Store every 4 epochs\n",
    "valid_accuracy3 = torch.zeros(2, 5) -- Store every 4 epochs\n",
    "training_time3 = torch.zeros(2)\n",
    "test_pred3 = torch.zeros(test_new:size(1),45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Script\n",
    "\n",
    "--Define the model\n",
    "neuralnet_wc_1 = build_nn2(nwords, ncap, nclasses, 50, 50, 50 , data['word_embeddings'])\n",
    "criterion = nn.ClassNLLCriterion()\n",
    "    \n",
    "-- Training\n",
    "timer = torch.Timer()\n",
    "trainer = nn.StochasticGradient(neuralnet_wc_1, criterion)\n",
    "trainer.learningRate = 0.01\n",
    "trainer.maxIteration = 4\n",
    "    \n",
    "for j=1, 5 do\n",
    "    trainer:train(dataset)\n",
    "        \n",
    "        -- Pred on train\n",
    "    pred_val = neuralnet_wc_1:forward(valid_new)\n",
    "    valid_accuracy3[1][j] = compute_accuracy(pred_val, valid_output)\n",
    "        \n",
    "    pred_train = neuralnet_wc_1:forward(train_new)\n",
    "    training_accuracy3[1][j] = compute_accuracy(pred_train, train_output)\n",
    "    loss_train3[1][j] = criterion:forward(pred_train, train_output)\n",
    "end\n",
    "\n",
    "training_time3[1] = timer:time().real\n",
    "test_pred3_1 = neuralnet_wc_1:forward(test_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Script\n",
    "\n",
    "--Define the model\n",
    "neuralnet_wc_2 = build_nn2(nwords, ncap, nclasses, 50, 70, 50 , data['word_embeddings'])\n",
    "criterion = nn.ClassNLLCriterion()\n",
    "    \n",
    "-- Training\n",
    "timer = torch.Timer()\n",
    "trainer = nn.StochasticGradient(neuralnet_wc_2, criterion)\n",
    "trainer.learningRate = 0.01\n",
    "trainer.maxIteration = 4\n",
    "    \n",
    "for j=1, 5 do\n",
    "    trainer:train(dataset)\n",
    "        \n",
    "        -- Pred on train\n",
    "    pred_val = neuralnet_wc_2:forward(valid_new)\n",
    "    valid_accuracy3[2][j] = compute_accuracy(pred_val, valid_output)\n",
    "        \n",
    "    pred_train = neuralnet_wc_2:forward(train_new)\n",
    "    training_accuracy3[2][j] = compute_accuracy(pred_train, train_output)\n",
    "    loss_train3[2][j] = criterion:forward(pred_train, train_output)\n",
    "end\n",
    "\n",
    "training_time3[2] = timer:time().real\n",
    "test_pred3_1 = neuralnet_wc_1:forward(test_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Saving (rewriting the files with update at each iteration)\n",
    "filename = 'log_hyper_embedding.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('valid_accuracy', valid_accuracy)\n",
    "myFile:write('training_accuracy', training_accuracy)\n",
    "myFile:write('training_time', training_time)\n",
    "myFile:write('loss_train', loss_train)\n",
    "myFile:close()\n",
    "\n",
    "-- Saving the pred for each hyperparameter\n",
    "filename = 'test_pred_embedding_h_'.. d .. '.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('test_pred', test_pred3)\n",
    "myFile:close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
