{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings(filename, line_count, dimension):\n",
    "    word_embeddings = np.zeros((line_count+2, dimension - 1))\n",
    "    word_embeddings[0,:] = 2 * np.random.random(dimension - 1) - 1\n",
    "    word_embeddings[1,:] = 2 * np.random.random(dimension - 1) - 1\n",
    "    with open(filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            vector = line.split()\n",
    "            word_embeddings[i+2, :] = vector[1:]\n",
    "    return word_embeddings\n",
    "filename = 'data/glove.6B.50d.txt'\n",
    "line_count, dimension = get_number_elements(filename)\n",
    "word_embeddings = get_word_embeddings(filename, line_count, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 50)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Loading the tags to index mapping\n",
    "def get_tags2index(filename):\n",
    "    tags2index = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "           (key, val) = line.split()\n",
    "           tags2index[key] = int(val)\n",
    "    return tags2index\n",
    "tags2index = get_tags2index('data/tags.dict')\n",
    "print len(tags2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the dictionary (words2index mapping)\n",
    "def get_words2index(filename):\n",
    "    words2index = {'PADDING':0, 'RARE':1}\n",
    "    with open(filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # Restricing to the first 100 000 words\n",
    "            if i == 100000:\n",
    "                break\n",
    "            vect = line.split()\n",
    "            # Shifting of two for padding\n",
    "            words2index[vect[0]] = i + 2\n",
    "    return words2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words2index = get_words2index('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to pre-process the words\n",
    "# Return (feature_1, feature_2)\n",
    "def pre_process(word, words2index):\n",
    "    # Removing number if present\n",
    "    word = re.sub(\"\\d\", \"\", word)\n",
    "    # Case if only digits\n",
    "    if not len(word):\n",
    "        word = 'NUMBER'\n",
    "    # Building feature 1\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in words2index:\n",
    "        feature1 = words2index[word_lower]\n",
    "    else:\n",
    "        word = 'RARE'\n",
    "        feature1 = 1\n",
    "    # Building feature 2   \n",
    "    if word.islower() or re.search('[.?\\-\",]+', word):\n",
    "        feature2 = 0\n",
    "    elif word.isupper():\n",
    "        feature2 = 1\n",
    "    elif word[0].isupper():\n",
    "        feature2 = 2\n",
    "    else:\n",
    "        feature2 = 3\n",
    "    return feature1, feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Counting number of elements\n",
    "def get_number_elements(filename):\n",
    "    line_count = 0\n",
    "    dimension = 0\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            sp = line.split()\n",
    "            if sp:\n",
    "                if not dimension:\n",
    "                    dimension = len(sp)\n",
    "                line_count += 1\n",
    "    return line_count, dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: build the array (id_in_sentence, word_feature, cap_feature) and the output array\n",
    "def build_processed_input(filename, line_count, words2index, tags2index, test=False):\n",
    "    output = np.zeros(line_count, dtype=int)\n",
    "    # Contains: id_in_sentence, word_feature, cap_feature\n",
    "    processed_input = np.zeros((line_count, 3), dtype=int)\n",
    "    i = 0\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            sp = line.split()\n",
    "            # Check if blanck\n",
    "            if sp:\n",
    "                idword, id_in_sentence, word, tag = sp\n",
    "                word_feature, cap_feature = pre_process(word, words2index)\n",
    "                if test:\n",
    "                    output[i] = tags2index[tag]\n",
    "                processed_input[i, :] = [id_in_sentence, word_feature, cap_feature]\n",
    "                i += 1\n",
    "    return processed_input, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: building the two arrays for word_feature and cap_feature using window of dim 5 and the output vector\n",
    "def build_feature_array(filename, line_count, processed_input):\n",
    "    # Initialization\n",
    "    input_word = np.zeros((line_count, 5), dtype=int)\n",
    "    input_cap = np.zeros((line_count, 5), dtype=int)\n",
    "\n",
    "    for i in xrange(line_count - 2):\n",
    "        # Last element of the window\n",
    "        id_in_sentence_cur, feature1_cur, feature2_cur = tuple(processed_input[i, :])\n",
    "        id_in_sentence_next1, feature1_next1, feature2_next1 = tuple(processed_input[i+1, :])\n",
    "        id_in_sentence_next2, feature1_next2, feature2_next2 = tuple(processed_input[i+2, :])\n",
    "        # Case current word is the first one of a sentence\n",
    "        if id_in_sentence_cur == 1:\n",
    "            input_word[i,:2] = 0\n",
    "            input_cap[i,:2] = 1\n",
    "            input_word[i,2] = feature1_cur\n",
    "            input_cap[i,2] = feature2_cur\n",
    "            input_word[i,3] = feature1_next1\n",
    "            input_cap[i,3] = feature2_next1\n",
    "            input_word[i,4] = feature1_next2\n",
    "            input_cap[i,4] = feature2_next2\n",
    "        else:\n",
    "            input_word[i,:4] = input_word[i-1,1:5]\n",
    "            input_cap[i,:4] = input_cap[i-1,1:5]\n",
    "            # Case current word is within one position to the last one of a sentence\n",
    "            if id_in_sentence_next2 == 1:\n",
    "                input_word[i,4] = 0\n",
    "                input_cap[i,4] = 1\n",
    "            # Case current word is the last one of a sentence\n",
    "            elif id_in_sentence_next1 == 1:\n",
    "                input_word[i,3] = 0\n",
    "                input_cap[i,3] = 1\n",
    "                input_word[i,4] = 0\n",
    "                input_cap[i,4] = 1\n",
    "            else:\n",
    "                input_word[i,4] = feature1_next2\n",
    "                input_cap[i,4] = feature2_next2\n",
    "    # Corner Case: two last rows\n",
    "    i = line_count - 2\n",
    "    # Case one to last word at a beginning of a sentence\n",
    "    id_in_sentence_last1, feature1_last1, feature2_last1 = tuple(processed_input[i + 1, :])\n",
    "    id_in_sentence_last2, feature1_last2, feature2_last2 = tuple(processed_input[i, :])\n",
    "    if id_in_sentence_last2 == 1:\n",
    "        input_word[i,:2] = 0\n",
    "        input_cap[i,:2] = 1\n",
    "        input_word[i,2] = feature1_last2\n",
    "        input_cap[i,2] = feature2_last2\n",
    "        input_word[i,3] = feature1_last1\n",
    "        input_cap[i,3] = feature2_last1\n",
    "        input_word[i,4] = 0\n",
    "        input_cap[i,4] = 1\n",
    "    else:\n",
    "        input_word[i,:4] = input_word[i-1,1:5]\n",
    "        input_cap[i,:4] = input_cap[i-1,1:5]\n",
    "        input_word[i,4] = 0\n",
    "        input_cap[i,4] = 1\n",
    "    # Last word case\n",
    "    input_word[i+1,:4] = input_word[i,1:5]\n",
    "    input_cap[i+1,:4] = input_cap[i,1:5]\n",
    "    input_word[i+1,4] = 0\n",
    "    input_cap[i+1,4] = 1\n",
    "    \n",
    "    return input_cap.astype(int), input_word.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 255 ms, total: 13.4 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test\n",
    "filename = 'data/train.tags.txt'\n",
    "line_count, dimension = get_number_elements(filename)\n",
    "processed_input, output = build_processed_input(filename, line_count, words2index, tags2index)\n",
    "input_cap, input_word = build_feature_array(filename, line_count, processed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = \"data/train.tags.txt\"\n",
    "valid = \"data/dev.tags.txt\"\n",
    "test = \"data/test.tags.txt\"\n",
    "tag_dict = \"data/tags.dict\"\n",
    "embedding = \"data/glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags2index size 45\n",
      "words2index size 100002\n"
     ]
    }
   ],
   "source": [
    "tags2index = get_tags2index(tag_dict)\n",
    "print 'tags2index size', len(tags2index)\n",
    "C = len(tags2index)\n",
    "words2index = get_words2index(embedding)\n",
    "print 'words2index size', len(words2index)\n",
    "line_count_dict, dimension_dict = get_number_elements(embedding)\n",
    "word_embeddings = get_word_embeddings(embedding, line_count_dict,\n",
    "                                      dimension_dict)\n",
    "\n",
    "input_features = {}\n",
    "for name, filename in zip(['train', 'valid', 'test'], [train, valid, test]):\n",
    "        if name=='test':\n",
    "            test_bool = False\n",
    "        else:\n",
    "            test_bool = True\n",
    "        line_count, dimension = get_number_elements(filename)\n",
    "        processed_input, output = build_processed_input(filename, line_count,\n",
    "                                                        words2index,\n",
    "                                                        tags2index, test=test_bool)\n",
    "        input_cap, input_word = build_feature_array(filename, line_count, processed_input)\n",
    "        input_features[name] = input_word, input_cap, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0, 5031,    1,    3],\n",
       "        [   0, 5031,    1,    3,  225],\n",
       "        [5031,    1,    3,  225,   84],\n",
       "        ..., \n",
       "        [   6,  616, 1520,  775,    4],\n",
       "        [ 616, 1520,  775,    4,    0],\n",
       "        [1520,  775,    4,    0,    0]]), array([[1, 1, 2, 1, 0],\n",
       "        [1, 2, 1, 0, 1],\n",
       "        [2, 1, 0, 1, 0],\n",
       "        ..., \n",
       "        [0, 2, 2, 0, 0],\n",
       "        [2, 2, 0, 0, 1],\n",
       "        [2, 0, 0, 1, 1]]), array([ 1,  1,  2, ...,  1, 17, 11]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('PTB.hdf5', \"r\") as f:\n",
    "    test = f['train_input_word_windows']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not a dataset (Not a dataset)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-57d09b396387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/work/h5py/_objects.c:2579)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/work/h5py/_objects.c:2538)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/nicolasdrizard/anaconda/envs/python2/lib/python2.7/site-packages/h5py/_hl/dataset.pyc\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;34m\"\"\"Numpy dtype representing the datatype\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.dtype.__get__ (/Users/ilan/minonda/conda-bld/work/h5py/h5d.c:2241)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.dtype.__get__ (/Users/ilan/minonda/conda-bld/work/h5py/h5d.c:2158)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/work/h5py/_objects.c:2579)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/work/h5py/_objects.c:2538)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.get_type (/Users/ilan/minonda/conda-bld/work/h5py/h5d.c:4320)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Not a dataset (Not a dataset)"
     ]
    }
   ],
   "source": [
    "test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
