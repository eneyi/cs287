{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'hdf5'\n",
    "\n",
    "local Squeeze, parent = torch.class('nn.Squeeze', 'nn.Module')\n",
    "\n",
    "function Squeeze:updateOutput(input)\n",
    "    self.size = input:size()\n",
    "    self.output = input:squeeze()\n",
    "    return self.output\n",
    "end\n",
    "\n",
    "function Squeeze:updateGradInput(input, gradOutput)\n",
    "  self.gradInput = gradOutput:view(self.size)\n",
    "  return self.gradInput  \n",
    "end\n",
    "\n",
    "myFile = hdf5.open('PTB.hdf5','r')\n",
    "data = myFile:all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input_word_windows = data['train_input_word_windows']\n",
    "train_input_cap_windows = data['train_input_cap_windows']\n",
    "\n",
    "train = train_input_word_windows:clone()\n",
    "train_cap = train_input_cap_windows:clone()\n",
    "train_output = data['train_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet = nn.Sequential()\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet:add(nn.LookupTable(data['nwords'][1],dim_hidden))\n",
    "neuralnet:add(nn.View(1,-1,5*50))\n",
    "neuralnet:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet:add(nn.Linear(5*dim_hidden,dim_hidden2))\n",
    "neuralnet:add(nn.HardTanh())\n",
    "neuralnet:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet:add(nn.LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset={};\n",
    "for i=1,train:size(1) do \n",
    "  dataset[i] = {train[i]:view(1,5), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.25574613654007\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.2501719415205\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.24487123168396\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.23992389741266\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.23531232038182\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.23095860899926\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.22691767798042\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.22318743010994\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.21957943355692\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.21625220273859\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.21625220273859\t\n",
       "Time elapsed: 1561.8227100372 seconds\t\n",
       "\t\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer = torch.Timer()\n",
    "\n",
    "trainer = nn.StochasticGradient(neuralnet, criterion)\n",
    "trainer.learningRate = 0.001\n",
    "trainer.maxIteration = 10\n",
    "trainer:train(dataset)\n",
    "\n",
    "print('Time elapsed: ' .. timer:time().real .. ' seconds',\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Validation Score on Train is 0.93374246438456\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val_word = data['valid_input_word_windows']:clone()\n",
    "val_output = data['valid_output']:clone()\n",
    "\n",
    "pred_train = neuralnet:forward(train)\n",
    "max,argmax_train = pred_train:max(2)\n",
    "acc_train = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train[i][1] == train_output[i] then\n",
    "        acc_train = acc_train + 1\n",
    "    end\n",
    "end\n",
    "score_train = acc_train/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Validation Score on Validation is 0.91404163631949\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val = neuralnet:forward(val_word)\n",
    "max,argmax_val = pred_val:max(2)\n",
    "acc_val = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val[i][1] == data['valid_output'][i] then\n",
    "        acc_val = acc_val + 1\n",
    "    end\n",
    "end\n",
    "score_val = acc_val/data['valid_output']:size(1)\n",
    "print('Validation Score on Validation is '..score_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding the cap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_new = train_input_word_windows:clone()\n",
    "train_cap_new = train_cap:clone()\n",
    "train_cap_new:add(100002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new = torch.cat(train_new,train_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet_wc = nn.Sequential()\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet_wc:add(nn.LookupTable(data['nwords'][1]+4,dim_hidden))\n",
    "neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "neuralnet_wc:add(nn.HardTanh())\n",
    "neuralnet_wc:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet_wc:add(nn.LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2={};\n",
    "for i=1,train:size(1) do \n",
    "  dataset2[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset2:size() return train:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "...asdrizard/torch/install/share/lua/5.1/nn/LookupTable.lua:56: index out of range at /Users/nicolasdrizard/torch/pkg/torch/lib/TH/generic/THTensorMath.c:141\nstack traceback:\n\t[C]: in function 'index'\n\t...asdrizard/torch/install/share/lua/5.1/nn/LookupTable.lua:56: in function 'updateOutput'\n\t...lasdrizard/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'\n\t...rd/torch/install/share/lua/5.1/nn/StochasticGradient.lua:35: in function 'train'\n\t[string \"trainer2 = nn.StochasticGradient(neuralnet_wc...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...colasdrizard/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0102c0fbb0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...asdrizard/torch/install/share/lua/5.1/nn/LookupTable.lua:56: index out of range at /Users/nicolasdrizard/torch/pkg/torch/lib/TH/generic/THTensorMath.c:141\nstack traceback:\n\t[C]: in function 'index'\n\t...asdrizard/torch/install/share/lua/5.1/nn/LookupTable.lua:56: in function 'updateOutput'\n\t...lasdrizard/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'\n\t...rd/torch/install/share/lua/5.1/nn/StochasticGradient.lua:35: in function 'train'\n\t[string \"trainer2 = nn.StochasticGradient(neuralnet_wc...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...colasdrizard/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0102c0fbb0"
     ]
    }
   ],
   "source": [
    "trainer2 = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "trainer2.learningRate = 0.01\n",
    "trainer2.maxIteration = 3\n",
    "trainer2:train(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Validation Score on Train is 0.90422454654824\t\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train2 = neuralnet_wc:forward(train_new)\n",
    "max,argmax_train2 = pred_train2:max(2)\n",
    "acc_train2 = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train2[i][1] == train_output[i] then\n",
    "        acc_train2 = acc_train2 + 1\n",
    "    end\n",
    "end\n",
    "score_train2 = acc_train2/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_new = data['valid_input_word_windows']:clone()\n",
    "valid_cap_new = data['valid_input_cap_windows']:clone()\n",
    "valid_cap_new:add(100002)\n",
    "valid_new = torch.cat(valid_new,valid_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Validation Score on Train is 0.89842801650886\t\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val2 = neuralnet_wc:forward(valid_new)\n",
    "max,argmax_val2 = pred_val2:max(2)\n",
    "acc_val2 = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val2[i][1] == data['valid_output'][i] then\n",
    "        acc_val2 = acc_val2 + 1\n",
    "    end\n",
    "end\n",
    "score_val2 = acc_val2/data['valid_output']:size(1)\n",
    "print('Validation Score on Train is '..score_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new = data['train_input_word_windows']\n",
    "train_cap_new = data['train_input_cap_windows']\n",
    "train_output = data['train_output']\n",
    "train_cap_new:add(100002)\n",
    "train_new = torch.cat(train_new,train_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet_wc = nn.Sequential()\n",
    "\n",
    "par = nn.LookupTable(data['nwords'][1]+4,dim_hidden)\n",
    "word_embeddings = data['word_embeddings']\n",
    "par.weight:narrow(1, 1, data['nwords'][1]):copy(word_embeddings:narrow(1, 1, data['nwords'][1]))\n",
    "print(par.weight[1][1], word_embeddings[1][1])\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet_wc:add(par)\n",
    "neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "neuralnet_wc:add(nn.HardTanh())\n",
    "neuralnet_wc:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet_wc:add(nn.LogSoftMax())\n",
    "\n",
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2={};\n",
    "for i=1,train_new:size(1) do \n",
    "  dataset2[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset2:size() return train_new:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      1       1    5032       2       4  100004  100004  100005  100004  100003\n",
       "[torch.LongTensor of size 1x10]\n",
       "\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.29927499225139\t\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.16488427987802\t\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.13787830303627\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.13787830303627\t\n",
       "Time elapsed: 527.83199000359 seconds\t\n",
       "\t\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer = torch.Timer()\n",
    "\n",
    "trainer2 = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "trainer2.learningRate = 0.01\n",
    "trainer2.maxIteration = 3\n",
    "trainer2:train(dataset2)\n",
    "\n",
    "print('Time elapsed: ' .. timer:time().real .. ' seconds',\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Evaluation\n",
    "valid = data['valid_input_word_windows']\n",
    "valid_cap = data['valid_input_cap_windows']\n",
    "valid_new = torch.cat(valid,torch.add(valid_cap,100002) ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Validation Score on Train is 0.95776987419275\t\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Validation Score on Train is 0.95278738771546\t\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- On train set\n",
    "\n",
    "pred_train2 = neuralnet_wc:forward(train_new)\n",
    "max,argmax_train2 = pred_train2:max(2)\n",
    "acc_train2 = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train2[i][1] == train_output[i] then\n",
    "        acc_train2 = acc_train2 + 1\n",
    "    end\n",
    "end\n",
    "score_train2 = acc_train2/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train2)\n",
    "\n",
    "\n",
    "-- On validation set\n",
    "pred_val2 = neuralnet_wc:forward(valid_new)\n",
    "max,argmax_val2 = pred_val2:max(2)\n",
    "acc_val2 = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val2[i][1] == data['valid_output'][i] then\n",
    "        acc_val2 = acc_val2 + 1\n",
    "    end\n",
    "end\n",
    "score_val2 = acc_val2/data['valid_output']:size(1)\n",
    "print('Validation Score on Train is '..score_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all the following cells until the one called SCRIPT TO TUNE. This one is the longest to run, there are 2 loops: one over the parameters to test (here over dim_hidden) and one over the number of iterations (5 iterations of 4 epoch), the latter is to log more information about loss and accuracy and attest that the model converges.\n",
    "In between each iteration of the outer loop the script rewrites in the log files in case of failure.\n",
    "\n",
    "Then the last cell is to load the saved files. The goal is to use the two log of accuracy_train and accuracy_valid to select the best model and then retrieve its test prediction to submit them and take the lead!!\n",
    "We will have enough log to have some nice plots too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Helper\n",
    "\n",
    "function compute_accuracy(pred, true_)\n",
    "    max,argmax = pred:max(2)\n",
    "    acc = 0\n",
    "    for i = 1, true_:size(1) do\n",
    "        if argmax[i][1] == true_[i] then\n",
    "            acc = acc + 1\n",
    "        end\n",
    "    end\n",
    "    score = acc/true_:size(1)\n",
    "    \n",
    "    return score\n",
    "end\n",
    "\n",
    "-- If words_embeddings is nil, weight are initialized randomly by torch\n",
    "function build_nn(nwords, ncap, nclasses, dim_hidden, dim_hidden2, word_embeddings)\n",
    "    --Define the module\n",
    "    neuralnet_wc = nn.Sequential()\n",
    "\n",
    "    par = nn.LookupTable(nwords + ncap,dim_hidden)\n",
    "    \n",
    "    -- Adding the embeddings\n",
    "    if word_embeddings then\n",
    "        par.weight:narrow(1, 1, nwords):copy(word_embeddings:narrow(1, 1, nwords))\n",
    "    end\n",
    "    neuralnet_wc:add(par)\n",
    "\n",
    "    neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "    neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "    neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "    neuralnet_wc:add(nn.HardTanh())\n",
    "    neuralnet_wc:add(nn.Linear(dim_hidden2, nclasses))\n",
    "    neuralnet_wc:add(nn.LogSoftMax())\n",
    "    \n",
    "    return neuralnet_wc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Loading the data\n",
    "\n",
    "-- dimension\n",
    "nwords = data['nwords'][1]\n",
    "ncap = 4\n",
    "nclasses = data['nclasses'][1]\n",
    "\n",
    "-- Training data\n",
    "train_output = data['train_output']\n",
    "train_new = torch.cat(data['train_input_word_windows'],\n",
    "    torch.add(data['train_input_cap_windows'], 100002),2)\n",
    "\n",
    "-- Validation data\n",
    "valid_output = data['valid_output']\n",
    "valid_new = torch.cat(data['valid_input_word_windows'],\n",
    "    torch.add(data['valid_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Test data\n",
    "test_new = torch.cat(data['test_input_word_windows'],\n",
    "    torch.add(data['test_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Formating the dataset to train\n",
    "dataset={};\n",
    "for i=1,train_new:size(1) do \n",
    "  dataset[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train_new:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Hyperparameter grid choice\n",
    "\n",
    "-- Place to change parameter if needed\n",
    "dim_hidden2_list = torch.DoubleTensor({40, 50, 60, 80, 100, 120})\n",
    "num_parameters = dim_hidden2_list:size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Metric storage\n",
    "\n",
    "-- Initialization\n",
    "loss_train = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "training_accuracy = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "valid_accuracy = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "training_time = torch.zeros(num_parameters)\n",
    "test_pred = torch.zeros(test_new:size(1),45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# StochasticGradient: training\t\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.67797367306208\t\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.4675418220396\t\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.39370319093712\t\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# current error = 0.34702673766335\t\n",
       "# StochasticGradient: you have reached the maximum number of iterations\t\n",
       "# training error = 0.34702673766335\t\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "[string \"-- Script...\"]:30: attempt to index a nil value\nstack traceback:\n\t[string \"-- Script...\"]:30: in main chunk\n\t[C]: in function 'xpcall'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...colasdrizard/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010f039bb0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"-- Script...\"]:30: attempt to index a nil value\nstack traceback:\n\t[string \"-- Script...\"]:30: in main chunk\n\t[C]: in function 'xpcall'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:179: in function <...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t...colasdrizard/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...asdrizard/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...colasdrizard/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010f039bb0"
     ]
    }
   ],
   "source": [
    "-- Script\n",
    "\n",
    "-- REquired value because of the embeddings\n",
    "dim_hidden = 50\n",
    "\n",
    "for d in dim_hidden2_list:size() do\n",
    "--for d=1,1 do\n",
    "    dim_hidden2 = dim_hidden2_list[d]\n",
    "\n",
    "    --Define the model\n",
    "    neuralnet_wc = build_nn(nwords, ncap, nclasses, dim_hidden, dim_hidden2, data['word_embeddings'])\n",
    "    criterion = nn.ClassNLLCriterion()\n",
    "    \n",
    "    -- Training\n",
    "    timer = torch.Timer()\n",
    "    trainer = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "    trainer.learningRate = 0.01\n",
    "    trainer.maxIteration = 4\n",
    "    \n",
    "    for j=1, 5 do\n",
    "        trainer:train(dataset)\n",
    "        \n",
    "        -- Pred on train\n",
    "        pred_val = neuralnet_wc:forward(valid_new)\n",
    "        valid_accuracy[d][j] = compute_accuracy(pred_val, valid_output)\n",
    "        \n",
    "        pred_train = neuralnet_wc:forward(train_new)\n",
    "        training_accuracy[d][j] = compute_accuracy(pred_train, train_output)\n",
    "        loss_train[d][j] = criterion:forward(pred_train, train_output)\n",
    "    end\n",
    "\n",
    "    training_time[d] = timer:time().real\n",
    "    test_pred = neuralnet_wc:forward(test_new)\n",
    "    \n",
    "    -- Saving (rewriting the files with update at each iteration)\n",
    "    filename = 'log_hyper_embedding.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('valid_accuracy', valid_accuracy)\n",
    "    myFile:write('training_accuracy', training_accuracy)\n",
    "    myFile:write('training_time', training_time)\n",
    "    myFile:write('loss_train', loss_train)\n",
    "    myFile:close()\n",
    "    \n",
    "    -- Saving the pred for each hyperparameter\n",
    "    filename = 'test_pred_embedding_h_'.. d .. '.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('test_pred', test_pred)\n",
    "    myFile:close()    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--- Script to read the saved files\n",
    "\n",
    "myFile = hdf5.open('log_hyper_embedding.f5','r')\n",
    "hyperparameter_log = myFile:all()\n",
    "myFile:close()\n",
    "\n",
    "d = 1\n",
    "myFile = hdf5.open('test_pred_embedding_h_'.. d .. '.f5','r')\n",
    "test_pred_1 = myFile:all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
