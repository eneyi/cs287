{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'hdf5'\n",
    "\n",
    "local Squeeze, parent = torch.class('nn.Squeeze', 'nn.Module')\n",
    "\n",
    "function Squeeze:updateOutput(input)\n",
    "    self.size = input:size()\n",
    "    self.output = input:squeeze()\n",
    "    return self.output\n",
    "end\n",
    "\n",
    "function Squeeze:updateGradInput(input, gradOutput)\n",
    "  self.gradInput = gradOutput:view(self.size)\n",
    "  return self.gradInput  \n",
    "end\n",
    "\n",
    "myFile = hdf5.open('PTB.hdf5','r')\n",
    "data = myFile:all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input_word_windows = data['train_input_word_windows']\n",
    "train_input_cap_windows = data['train_input_cap_windows']\n",
    "\n",
    "train = train_input_word_windows:clone()\n",
    "train_cap = train_input_cap_windows:clone()\n",
    "train_output = data['train_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet = nn.Sequential()\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet:add(nn.LookupTable(data['nwords'][1],dim_hidden))\n",
    "neuralnet:add(nn.View(1,-1,5*50))\n",
    "neuralnet:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet:add(nn.Linear(5*dim_hidden,dim_hidden2))\n",
    "neuralnet:add(nn.HardTanh())\n",
    "neuralnet:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet:add(nn.LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset={};\n",
    "for i=1,train:size(1) do \n",
    "  dataset[i] = {train[i]:view(1,5), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timer = torch.Timer()\n",
    "\n",
    "trainer = nn.StochasticGradient(neuralnet, criterion)\n",
    "trainer.learningRate = 0.001\n",
    "trainer.maxIteration = 10\n",
    "trainer:train(dataset)\n",
    "\n",
    "print('Time elapsed: ' .. timer:time().real .. ' seconds',\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "val_word = data['valid_input_word_windows']:clone()\n",
    "val_output = data['valid_output']:clone()\n",
    "\n",
    "pred_train = neuralnet:forward(train)\n",
    "max,argmax_train = pred_train:max(2)\n",
    "acc_train = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train[i][1] == train_output[i] then\n",
    "        acc_train = acc_train + 1\n",
    "    end\n",
    "end\n",
    "score_train = acc_train/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_val = neuralnet:forward(val_word)\n",
    "max,argmax_val = pred_val:max(2)\n",
    "acc_val = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val[i][1] == data['valid_output'][i] then\n",
    "        acc_val = acc_val + 1\n",
    "    end\n",
    "end\n",
    "score_val = acc_val/data['valid_output']:size(1)\n",
    "print('Validation Score on Validation is '..score_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding the cap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_new = train_input_word_windows:clone()\n",
    "train_cap_new = train_cap:clone()\n",
    "train_cap_new:add(100002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new = torch.cat(train_new,train_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet_wc = nn.Sequential()\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet_wc:add(nn.LookupTable(data['nwords'][1]+4,dim_hidden))\n",
    "neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "neuralnet_wc:add(nn.HardTanh())\n",
    "neuralnet_wc:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet_wc:add(nn.LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2={};\n",
    "for i=1,train:size(1) do \n",
    "  dataset2[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset2:size() return train:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer2 = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "trainer2.learningRate = 0.01\n",
    "trainer2.maxIteration = 3\n",
    "trainer2:train(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train2 = neuralnet_wc:forward(train_new)\n",
    "max,argmax_train2 = pred_train2:max(2)\n",
    "acc_train2 = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train2[i][1] == train_output[i] then\n",
    "        acc_train2 = acc_train2 + 1\n",
    "    end\n",
    "end\n",
    "score_train2 = acc_train2/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_new = data['valid_input_word_windows']:clone()\n",
    "valid_cap_new = data['valid_input_cap_windows']:clone()\n",
    "valid_cap_new:add(100002)\n",
    "valid_new = torch.cat(valid_new,valid_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_val2 = neuralnet_wc:forward(valid_new)\n",
    "max,argmax_val2 = pred_val2:max(2)\n",
    "acc_val2 = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val2[i][1] == data['valid_output'][i] then\n",
    "        acc_val2 = acc_val2 + 1\n",
    "    end\n",
    "end\n",
    "score_val2 = acc_val2/data['valid_output']:size(1)\n",
    "print('Validation Score on Train is '..score_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new = data['train_input_word_windows']\n",
    "train_cap_new = data['train_input_cap_windows']\n",
    "train_output = data['train_output']\n",
    "train_cap_new:add(100002)\n",
    "train_new = torch.cat(train_new,train_cap_new,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_hidden = 50\n",
    "dim_hidden2 = 50\n",
    "--Define the module\n",
    "neuralnet_wc = nn.Sequential()\n",
    "\n",
    "par = nn.LookupTable(data['nwords'][1]+4,dim_hidden)\n",
    "word_embeddings = data['word_embeddings']\n",
    "par.weight:narrow(1, 1, data['nwords'][1]):copy(word_embeddings:narrow(1, 1, data['nwords'][1]))\n",
    "print(par.weight[1][1], word_embeddings[1][1])\n",
    "\n",
    "--Include the lookup tables\n",
    "neuralnet_wc:add(par)\n",
    "neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "neuralnet_wc:add(nn.HardTanh())\n",
    "neuralnet_wc:add(nn.Linear(dim_hidden2, data['nclasses'][1]))\n",
    "neuralnet_wc:add(nn.LogSoftMax())\n",
    "\n",
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2={};\n",
    "for i=1,train_new:size(1) do \n",
    "  dataset2[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset2:size() return train_new:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timer = torch.Timer()\n",
    "\n",
    "trainer2 = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "trainer2.learningRate = 0.01\n",
    "trainer2.maxIteration = 3\n",
    "trainer2:train(dataset2)\n",
    "\n",
    "print('Time elapsed: ' .. timer:time().real .. ' seconds',\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Evaluation\n",
    "valid = data['valid_input_word_windows']\n",
    "valid_cap = data['valid_input_cap_windows']\n",
    "valid_new = torch.cat(valid,torch.add(valid_cap,100002) ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- On train set\n",
    "\n",
    "pred_train2 = neuralnet_wc:forward(train_new)\n",
    "max,argmax_train2 = pred_train2:max(2)\n",
    "acc_train2 = 0\n",
    "for i = 1, train_output:size(1) do\n",
    "    if argmax_train2[i][1] == train_output[i] then\n",
    "        acc_train2 = acc_train2 + 1\n",
    "    end\n",
    "end\n",
    "score_train2 = acc_train2/train_output:size(1)\n",
    "print('Validation Score on Train is '..score_train2)\n",
    "\n",
    "\n",
    "-- On validation set\n",
    "pred_val2 = neuralnet_wc:forward(valid_new)\n",
    "max,argmax_val2 = pred_val2:max(2)\n",
    "acc_val2 = 0\n",
    "for i = 1, data['valid_output']:size(1) do\n",
    "    if argmax_val2[i][1] == data['valid_output'][i] then\n",
    "        acc_val2 = acc_val2 + 1\n",
    "    end\n",
    "end\n",
    "score_val2 = acc_val2/data['valid_output']:size(1)\n",
    "print('Validation Score on Train is '..score_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all the following cells until the one called SCRIPT TO TUNE. This one is the longest to run, there are 2 loops: one over the parameters to test (here over dim_hidden) and one over the number of iterations (5 iterations of 4 epoch), the latter is to log more information about loss and accuracy and attest that the model converges.\n",
    "In between each iteration of the outer loop the script rewrites in the log files in case of failure.\n",
    "\n",
    "Then the last cell is to load the saved files. The goal is to use the two log of accuracy_train and accuracy_valid to select the best model and then retrieve its test prediction to submit them and take the lead!!\n",
    "We will have enough log to have some nice plots too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Helper\n",
    "\n",
    "function compute_accuracy(pred, true_)\n",
    "    max,argmax = pred:max(2)\n",
    "    acc = 0\n",
    "    for i = 1, true_:size(1) do\n",
    "        if argmax[i][1] == true_[i] then\n",
    "            acc = acc + 1\n",
    "        end\n",
    "    end\n",
    "    score = acc/true_:size(1)\n",
    "    \n",
    "    return score\n",
    "end\n",
    "\n",
    "-- If words_embeddings is nil, weight are initialized randomly by torch\n",
    "function build_nn(nwords, ncap, nclasses, dim_hidden, dim_hidden2, word_embeddings)\n",
    "    --Define the module\n",
    "    neuralnet_wc = nn.Sequential()\n",
    "\n",
    "    par = nn.LookupTable(nwords + ncap,dim_hidden)\n",
    "    \n",
    "    -- Adding the embeddings\n",
    "    if word_embeddings then\n",
    "        par.weight:narrow(1, 1, nwords):copy(word_embeddings:narrow(1, 1, nwords))\n",
    "    end\n",
    "    neuralnet_wc:add(par)\n",
    "\n",
    "    neuralnet_wc:add(nn.View(1,-1,10*dim_hidden))\n",
    "    neuralnet_wc:add(nn.Squeeze()) -- this layer is to go from a 1xAxB tensor to AxB dimensional tensor (https://groups.google.com/forum/#!topic/torch7/u4OEc0GB74k)\n",
    "    neuralnet_wc:add(nn.Linear(10*dim_hidden,dim_hidden2))\n",
    "    neuralnet_wc:add(nn.HardTanh())\n",
    "    neuralnet_wc:add(nn.Linear(dim_hidden2, nclasses))\n",
    "    neuralnet_wc:add(nn.LogSoftMax())\n",
    "    \n",
    "    return neuralnet_wc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Loading the data\n",
    "\n",
    "-- dimension\n",
    "nwords = data['nwords'][1]\n",
    "ncap = 4\n",
    "nclasses = data['nclasses'][1]\n",
    "\n",
    "-- Training data\n",
    "train_output = data['train_output']\n",
    "train_new = torch.cat(data['train_input_word_windows'],\n",
    "    torch.add(data['train_input_cap_windows'], 100002),2)\n",
    "\n",
    "-- Validation data\n",
    "valid_output = data['valid_output']\n",
    "valid_new = torch.cat(data['valid_input_word_windows'],\n",
    "    torch.add(data['valid_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Test data\n",
    "test_new = torch.cat(data['test_input_word_windows'],\n",
    "    torch.add(data['test_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Formating the dataset to train\n",
    "dataset={};\n",
    "for i=1,train_new:size(1) do \n",
    "  dataset[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train_new:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Hyperparameter grid choice\n",
    "\n",
    "-- Place to change parameter if needed\n",
    "dim_hidden2_list = torch.DoubleTensor({40, 50, 60, 80, 100, 120})\n",
    "num_parameters = dim_hidden2_list:size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Metric storage\n",
    "\n",
    "-- Initialization\n",
    "loss_train = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "training_accuracy = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "valid_accuracy = torch.zeros(num_parameters, 5) -- Store every 4 epochs\n",
    "training_time = torch.zeros(num_parameters)\n",
    "test_pred = torch.zeros(test_new:size(1),45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_hidden2_list:size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Script\n",
    "\n",
    "-- REquired value because of the embeddings\n",
    "dim_hidden = 50\n",
    "\n",
    "for d = 1,dim_hidden2_list:size(1) do\n",
    "--for d=1,1 do\n",
    "    dim_hidden2 = dim_hidden2_list[d]\n",
    "\n",
    "    --Define the model\n",
    "    neuralnet_wc = build_nn(nwords, ncap, nclasses, dim_hidden, dim_hidden2, data['word_embeddings'])\n",
    "    criterion = nn.ClassNLLCriterion()\n",
    "    \n",
    "    -- Training\n",
    "    timer = torch.Timer()\n",
    "    trainer = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "    trainer.learningRate = 0.01\n",
    "    trainer.maxIteration = 4\n",
    "    \n",
    "    for j=1, 5 do\n",
    "        trainer:train(dataset)\n",
    "        \n",
    "        -- Pred on train\n",
    "        pred_val = neuralnet_wc:forward(valid_new)\n",
    "        valid_accuracy[d][j] = compute_accuracy(pred_val, valid_output)\n",
    "        \n",
    "        pred_train = neuralnet_wc:forward(train_new)\n",
    "        training_accuracy[d][j] = compute_accuracy(pred_train, train_output)\n",
    "        loss_train[d][j] = criterion:forward(pred_train, train_output)\n",
    "    end\n",
    "\n",
    "    training_time[d] = timer:time().real\n",
    "    test_pred = neuralnet_wc:forward(test_new)\n",
    "    \n",
    "    -- Saving (rewriting the files with update at each iteration)\n",
    "    filename = 'log_hyper_embedding.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('valid_accuracy', valid_accuracy)\n",
    "    myFile:write('training_accuracy', training_accuracy)\n",
    "    myFile:write('training_time', training_time)\n",
    "    myFile:write('loss_train', loss_train)\n",
    "    myFile:close()\n",
    "    \n",
    "    -- Saving the pred for each hyperparameter\n",
    "    filename = 'test_pred_embedding_h_'.. d .. '.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('test_pred', test_pred)\n",
    "    myFile:close()    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--- Script to read the saved files\n",
    "\n",
    "myFile = hdf5.open('log_hyper_embedding.f5','r')\n",
    "hyperparameter_log = myFile:all()\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparameter_log['loss_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparameter_log['training_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparameter_log['valid_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "myFile = hdf5.open('test_pred_embedding_h_'.. d .. '.f5','r')\n",
    "test_pred_3 = myFile:all()\n",
    "myFile:close()\n",
    "max,argmax_3 = test_pred_3['test_pred']:max(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test_pred_3['test_pred']:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'tocsv_3.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('test_pred', argmax_3)\n",
    "myFile:close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 4\n",
    "myFile = hdf5.open('test_pred_embedding_h_'.. d .. '.f5','r')\n",
    "test_pred_4 = myFile:all()\n",
    "myFile:close()\n",
    "max,argmax_4 = test_pred_4['test_pred']:max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'tocsv_4.f5'\n",
    "myFile = hdf5.open(filename, 'w')\n",
    "myFile:write('test_pred', argmax_4)\n",
    "myFile:close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fixing the embedings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Loading the data\n",
    "\n",
    "-- dimension\n",
    "nwords = data['nwords'][1]\n",
    "ncap = 4\n",
    "nclasses = data['nclasses'][1]\n",
    "\n",
    "-- Training data\n",
    "train_output = data['train_output']\n",
    "train_new = torch.cat(data['train_input_word_windows'],\n",
    "    torch.add(data['train_input_cap_windows'], 100002),2)\n",
    "\n",
    "-- Validation data\n",
    "valid_output = data['valid_output']\n",
    "valid_new = torch.cat(data['valid_input_word_windows'],\n",
    "    torch.add(data['valid_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Test data\n",
    "test_new = torch.cat(data['test_input_word_windows'],\n",
    "    torch.add(data['test_input_cap_windows'],100002) ,2)\n",
    "\n",
    "-- Formating the dataset to train\n",
    "dataset={};\n",
    "for i=1,train_new:size(1) do \n",
    "  dataset[i] = {train_new[i]:view(1,10), train_output[i]}\n",
    "end\n",
    "function dataset:size() return train_new:size(1) end -- 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_hidden2_list2 = torch.DoubleTensor({50, 60, 80})\n",
    "num_parameters2 = dim_hidden2_list2:size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_parameters2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Metric storage\n",
    "\n",
    "-- Initialization\n",
    "loss_train2 = torch.zeros(num_parameters2, 20) -- Store every 1 epochs\n",
    "training_accuracy2 = torch.zeros(num_parameters2, 20) -- Store every 1 epochs\n",
    "valid_accuracy2 = torch.zeros(num_parameters2, 20) -- Store every 1 epochs\n",
    "training_time2 = torch.zeros(num_parameters2)\n",
    "test_pred2 = torch.zeros(test_new:size(1),45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Script\n",
    "\n",
    "-- REquired value because of the embeddings\n",
    "dim_hidden = 50\n",
    "\n",
    "for d = 1,dim_hidden2_list2:size(1) do\n",
    "--for d=1,1 do\n",
    "    dim_hidden2 = dim_hidden2_list2[d]\n",
    "\n",
    "    --Define the model\n",
    "    neuralnet_wc = build_nn(nwords, ncap, nclasses, dim_hidden, dim_hidden2, data['word_embeddings'])\n",
    "    criterion = nn.ClassNLLCriterion()\n",
    "    \n",
    "    -- Training\n",
    "    timer = torch.Timer()\n",
    "    trainer = nn.StochasticGradient(neuralnet_wc, criterion)\n",
    "    trainer.learningRate = 0.01\n",
    "    trainer.maxIteration = 1\n",
    "    \n",
    "    for j=1, 10 do\n",
    "        trainer:train(dataset)\n",
    "        neuralnet_wc:get(1).weight:narrow(1, 1, nwords):copy(data['word_embeddings']:narrow(1, 1, nwords))\n",
    "        -- Pred on train\n",
    "        pred_val = neuralnet_wc:forward(valid_new)\n",
    "        valid_accuracy2[d][j] = compute_accuracy(pred_val, valid_output)\n",
    "        \n",
    "        pred_train = neuralnet_wc:forward(train_new)\n",
    "        training_accuracy2[d][j] = compute_accuracy(pred_train, train_output)\n",
    "        loss_train2[d][j] = criterion:forward(pred_train, train_output)\n",
    "    end\n",
    "\n",
    "    training_time2[d] = timer:time().real\n",
    "    test_pred2 = neuralnet_wc:forward(test_new)\n",
    "    \n",
    "    -- Saving (rewriting the files with update at each iteration)\n",
    "    filename = 'log_hyper_embedding2.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('valid_accuracy2', valid_accuracy)\n",
    "    myFile:write('training_accuracy2', training_accuracy)\n",
    "    myFile:write('training_time2', training_time)\n",
    "    myFile:write('loss_train2', loss_train)\n",
    "    myFile:close()\n",
    "    \n",
    "    -- Saving the pred for each hyperparameter\n",
    "    filename = 'test_pred_embedding_h_'.. d .. '_2.f5'\n",
    "    myFile = hdf5.open(filename, 'w')\n",
    "    myFile:write('test_pred', test_pred2)\n",
    "    myFile:close()    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neuralnet_wc:get(1).weight:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
