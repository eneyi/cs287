{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'hdf5'\n",
    "require 'rnn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "myFile = hdf5.open('../data_preprocessed/'..tostring(N)..'-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "F_train = data['F_train']\n",
    "input_data_valid = data['input_data_valid']\n",
    "output_matrix_train = data['output_matrix_train']\n",
    "input_matrix_train = data['input_matrix_train']\n",
    "input_data_train = data['input_data_train']\n",
    "input_data_valid_nospace = data['input_data_valid_nospace']\n",
    "input_data_test = data['input_data_test']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 599903\n",
       "      1\n",
       "[torch.LongStorage of size 2]\n",
       "\n",
       " 599903\n",
       "[torch.LongStorage of size 1]\n",
       "\n",
       " 599905\n",
       "[torch.LongStorage of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_matrix_train:size())\n",
    "print(output_matrix_train:size())\n",
    "print(input_data_train:size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  1\n",
       " 30\n",
       "  1\n",
       " 20\n",
       " 12\n",
       " 11\n",
       " 11\n",
       " 12\n",
       "  9\n",
       "  7\n",
       "  2\n",
       "[torch.LongTensor of size 11]\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- TOFIX: outpout data does not contain the prediction for the last to the end char (here 7)\n",
    "input_data_train:narrow(1,input_data_train:size(1)-10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 2\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.DoubleTensor of size 11]\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_matrix_train:narrow(1,output_matrix_train:size(1)-10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599905\t\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599200\t\n",
       "749\t\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Formating the input\n",
    "-- Currently using a hack to have nive divisions\n",
    "n = input_data_train:size(1)\n",
    "n_new = n - 705\n",
    "len = 50\n",
    "batch_size = 16\n",
    "print(n_new)\n",
    "print(n_new/(batch_size*len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Issue with last sequence if batch_size does not divide n\n",
    "t_input = torch.split(input_data_train:narrow(1,1,n_new):view(batch_size,n_new/batch_size),len, 2)\n",
    "t_output = torch.split(output_matrix_train:narrow(1,1,n_new):view(batch_size,n_new/batch_size),len, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function build_rnn(embed_dim, vocab_size, batch_size, len)\n",
    "    local batchRNN\n",
    "    local params\n",
    "    local grad_params\n",
    "    -- generic RNN transduced\n",
    "    batchRNN = nn.Sequential()\n",
    "        :add(nn.LookupTable(vocab_size, embed_dim))\n",
    "        :add(nn.SplitTable(1, batch_size))\n",
    "    \n",
    "    batchRNN:add(nn.Sequencer(nn.Recurrent(\n",
    "       embed_dim, nn.Linear(embed_dim, embed_dim), \n",
    "       nn.Linear(embed_dim, embed_dim), nn.Tanh(), len)))\n",
    "    -- Output\n",
    "    batchRNN:add(nn.Sequencer(nn.Linear(embed_dim, 2)))\n",
    "    batchRNN:add(nn.Sequencer(nn.LogSoftMax()))\n",
    "    batchRNN:remember('both')\n",
    "\n",
    "    -- Retrieve parameters (To do only once!!!)\n",
    "    params, grad_params = batchRNN:getParameters()\n",
    "    \n",
    "    return batchRNN, params, grad_params\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function train_model(t_input, t_output, model, params, grad_params,\n",
    "                     criterion, eta, nEpochs, batch_size, len, n)\n",
    "    -- Train the model with a mini batch SGD\n",
    "    -- standard parameters are\n",
    "    -- nEpochs = 1\n",
    "    -- batchSize = 32\n",
    "    -- eta = 0.01\n",
    "    local timer\n",
    "    local pred\n",
    "    local loss\n",
    "    local dLdPred\n",
    "    local t_inputT = torch.DoubleTensor(len,batch_size)\n",
    "    local t_output_table\n",
    "    local delta = 0.2\n",
    "\n",
    "    -- To store the loss\n",
    "    local av_L = 0\n",
    "    \n",
    "    -- Initializing all the parameters between -0.05 and 0.05\n",
    "    for k=1,params:size(1) do\n",
    "        params[k] = torch.uniform(-0.05,0.05)\n",
    "    end\n",
    "\n",
    "    for i = 1, nEpochs do\n",
    "        -- timing the epoch\n",
    "        timer = torch.Timer()\n",
    "        old_L = av_L\n",
    "        av_L = 0\n",
    "        \n",
    "        -- mini batch loop\n",
    "        for k = 1, n/(batch_size * len) do\n",
    "            -- Mini batch data\n",
    "                \n",
    "            t_inputT:copy(t_input[k]:t())\n",
    "            t_output_table = torch.split(t_output[k],1,2)\n",
    "            --format the output\n",
    "            for j=1,len do\n",
    "                t_output_table[j] = t_output_table[j]:squeeze()\n",
    "            end \n",
    "            \n",
    "            -- reset gradients\n",
    "            grad_params:zero()\n",
    "            \n",
    "            -- Forward loop\n",
    "            pred = model:forward(t_inputT)\n",
    "            loss = criterion:forward(pred, t_output_table)\n",
    "            av_L = av_L + loss\n",
    "\n",
    "            -- Backward loop\n",
    "            dLdPred = criterion:backward(pred, t_output_table)\n",
    "            model:backward(t_inputT, dLdPred)\n",
    "            \n",
    "            -- gradient normalization with max norm 5 (l2 norm)\n",
    "            grad_params:view(grad_params:size(1),1):renorm(1,2,5)\n",
    "            model:updateParameters(eta)\n",
    "            \n",
    "        end\n",
    "            \n",
    "        print('Epoch '..i..': '..timer:time().real)\n",
    "        print('Average Loss: '..av_L/math.floor(n/batch_size))\n",
    "        \n",
    "    end\n",
    "    \n",
    "    if (old_L - av_L) < delta then\n",
    "        eta = eta/2\n",
    "        delta = delta/2\n",
    "    end\n",
    "    \n",
    "    if (eta < 0.001) then eta = 0.1 end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_inputT = torch.DoubleTensor(len,batch_size)\n",
    "t_inputT:copy(t_input[1]:t())\n",
    "t_output_table = torch.split(t_output[1],1,2)\n",
    "--format the output\n",
    "for j=1,len do\n",
    "    t_output_table[j] = t_output_table[j]:squeeze()\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 7.2721860408783\t\n",
       "Average Loss: 0.33585895237009\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2: 7.2341029644012\t\n",
       "Average Loss: 0.2950064360526\t"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3: 7.1395890712738\t\n",
       "Average Loss: 0.2794902047449\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 7.2123050689697\t\n",
       "Average Loss: 0.27168270712669\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 7.1313681602478\t\n",
       "Average Loss: 0.26577169023027\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6: 7.103590965271\t\n",
       "Average Loss: 0.26177779364331\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7: 8.225567817688\t\n",
       "Average Loss: 0.25686440773375\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8: 7.3784921169281\t\n",
       "Average Loss: 0.25137301734665\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 9: 7.5839750766754\t\n",
       "Average Loss: 0.2478425477159\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10: 8.4774169921875\t\n",
       "Average Loss: 0.24513761782392\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 49\n",
    "embed_dim = 16\n",
    "eta = 0.1\n",
    "nEpochs = 10\n",
    "\n",
    "-- Building model\n",
    "batchRNN, params, grad_params = build_rnn(embed_dim, vocab_size, len)\n",
    "crit = nn.SequencerCriterion(nn.ClassNLLCriterion())\n",
    "\n",
    "train_model(t_input, t_output, batchRNN, params, grad_params,\n",
    "                     crit, eta, nEpochs, batch_size, len, n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 7.5209510326385\t\n",
       "Average Loss: 0.34168909091067\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2: 7.2128188610077\t\n",
       "Average Loss: 0.29943631797339\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3: 7.9096779823303\t\n",
       "Average Loss: 0.28500375989836\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 7.469899892807\t\n",
       "Average Loss: 0.27697046149751\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 7.1924829483032\t\n",
       "Average Loss: 0.27064046119354\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6: 8.1540579795837\t\n",
       "Average Loss: 0.26681547858756\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7: 7.7046589851379\t\n",
       "Average Loss: 0.2635145372303\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8: 7.9570620059967\t\n",
       "Average Loss: 0.25903938416289\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 9: 7.3833689689636\t\n",
       "Average Loss: 0.25236317554974\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10: 7.1491401195526\t\n",
       "Average Loss: 0.24751554624674\t\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 49\n",
    "embed_dim = 20\n",
    "eta = 0.1\n",
    "nEpochs = 10\n",
    "\n",
    "-- Building model\n",
    "batchRNN, params, grad_params = build_rnn(embed_dim, vocab_size, len)\n",
    "crit = nn.SequencerCriterion(nn.ClassNLLCriterion())\n",
    "\n",
    "train_model(t_input, t_output, batchRNN, params, grad_params,\n",
    "                     crit, eta, nEpochs, batch_size, len, n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 7.0002450942993\t\n",
       "Average Loss: 0.30702439253076\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2: 6.9989540576935\t\n",
       "Average Loss: 0.25638525557878\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3: 7.117436170578\t\n",
       "Average Loss: 0.22885114705167\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 7.1096320152283\t\n",
       "Average Loss: 0.21161694280156\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 7.0728821754456\t\n",
       "Average Loss: 0.20092944715895\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6: 7.387598991394\t\n",
       "Average Loss: 0.19310292051215\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7: 7.9943079948425\t\n",
       "Average Loss: 0.18797150202392\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8: 8.1136260032654\t\n",
       "Average Loss: 0.18410751399451\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 9: 7.8263540267944\t"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Average Loss: 0.18050067656581\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10: 7.7138998508453\t\n",
       "Average Loss: 0.17716816410623\t\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 49\n",
    "embed_dim = 20\n",
    "eta = 0.5\n",
    "nEpochs = 10\n",
    "\n",
    "-- Building model\n",
    "batchRNN, params, grad_params = build_rnn(embed_dim, vocab_size, batch_size, len)\n",
    "crit = nn.SequencerCriterion(nn.ClassNLLCriterion())\n",
    "\n",
    "train_model(t_input, t_output, batchRNN, params, grad_params,\n",
    "                     crit, eta, nEpochs, batch_size, len, n_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Copying model into one with batch_size = 1\n",
    "batchRNN_valid, params_valid, grad_params_valid = build_rnn(embed_dim, vocab_size, 1, len)\n",
    "params_valid:copy(params)\n",
    "\n",
    "-- Compute perplexity on validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_valid = input_data_valid:view(input_data_valid:size(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_valid = batchRNN_valid:forward(input_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1824 -0.1197\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_valid[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_valid_output_table = {}\n",
    "--format the output\n",
    "for j=1,input_valid:size(1) do\n",
    "    if input_valid[j] == 1 then\n",
    "        t_valid_output_table[j] = torch.DoubleTensor({1})\n",
    "    else\n",
    "        t_valid_output_table[j] = torch.DoubleTensor({2})\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_valid = crit:forward(pred_valid, t_valid_output_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = batchRNN_valid:get(1)['weight']\n",
    "w_original = batchRNN:get(1)['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38958298749322\t\n",
       "1.4763650028245\t\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Compute perplexity\n",
    "loss_valid_avg = loss_valid/input_valid:size(1)\n",
    "perp_valid = math.exp(loss_valid_avg)\n",
    "print(loss_valid_avg)\n",
    "print(perp_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_probability_rnn(model, input)\n",
    "    return model:forward(input:view(input:size(1), 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = compute_probability_rnn(batchRNN_valid, input_data_test:narrow(1,1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Prediction on test\n",
    "function predict_rnn_greedy(input, len, model)\n",
    "    -- Last Position filled in predictions\n",
    "    local position = 1\n",
    "    -- We allocate the maximum of memory that could be needed\n",
    "    -- Default value is -1 (to know where predictions end afterwards)\n",
    "    local predictions = torch.ones(2*input:size(1)):mul(-1)\n",
    "    -- Copy the first entry\n",
    "    predictions[1] = input[1]\n",
    "    local probability = torch.zeros(2)\n",
    "    local current_predictions = torch.zeros(len)\n",
    "\n",
    "    -- Build mapping\n",
    "    for i=2,input:size(1)-1 do\n",
    "        -- Line where the model appears\n",
    "        -- We consider only the last 50 elements of the context (if exist)\n",
    "        if (position > 50) then\n",
    "            current_predictions:copy(predictions:narrow(1,position-len + 1, len))\n",
    "            probability_table = compute_probability_rnn(model, current_predictions)\n",
    "            probability:copy(probability_table[50])\n",
    "        else\n",
    "            current_predictions:narrow(1,1, position):copy(predictions:narrow(1,1, position))\n",
    "            probability_table = compute_probability_rnn(model, current_predictions:narrow(1,1, position))\n",
    "            probability:copy(probability_table[position])\n",
    "        end\n",
    "        m,a = probability:max(1)\n",
    "\n",
    "        -- Case space predicted\n",
    "        if (a[1] == 1) then\n",
    "            predictions[position+1] = 1\n",
    "            position = position +1\n",
    "        end\n",
    "\n",
    "        -- Copying next character\n",
    "        predictions[position+1] = input[i]\n",
    "        position = position +1\n",
    "    end\n",
    "    -- Adding last character (</s>)\n",
    "    predictions[position+1] = input[input:size(1)]\n",
    "    -- Cutting the output\n",
    "    return predictions:narrow(1,1,position+1)\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_test_short = predict_rnn_greedy(input_data_test, len, batchRNN_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  7\n",
       "  1\n",
       "  9\n",
       " 12\n",
       " 10\n",
       " 15\n",
       "  1\n",
       "  3\n",
       " 18\n",
       "  1\n",
       "  7\n",
       " 34\n",
       " 10\n",
       "  1\n",
       "  6\n",
       " 11\n",
       "  3\n",
       " 14\n",
       "  8\n",
       "  1\n",
       " 20\n",
       "  9\n",
       "  7\n",
       " 23\n",
       "  1\n",
       "  3\n",
       " 16\n",
       "  1\n",
       "  2\n",
       "  6\n",
       " 17\n",
       " 10\n",
       "  1\n",
       " 15\n",
       " 22\n",
       " 12\n",
       " 11\n",
       "  4\n",
       "  1\n",
       " 10\n",
       " 22\n",
       "  4\n",
       "  1\n",
       "  7\n",
       "  4\n",
       " 15\n",
       "  1\n",
       " 16\n",
       "  9\n",
       "  5\n",
       "  1\n",
       "  8\n",
       " 18\n",
       " 10\n",
       "  9\n",
       " 14\n",
       "  8\n",
       "  1\n",
       "  4\n",
       " 27\n",
       " 14\n",
       " 22\n",
       "  3\n",
       "  7\n",
       " 21\n",
       "  4\n",
       " 23\n",
       "  1\n",
       " 12\n",
       " 23\n",
       "  7\n",
       " 34\n",
       " 10\n",
       " 19\n",
       "  3\n",
       " 11\n",
       "  1\n",
       " 11\n",
       "  3\n",
       " 26\n",
       "  3\n",
       "  5\n",
       " 10\n",
       " 19\n",
       "  5\n",
       " 12\n",
       " 23\n",
       "  3\n",
       " 16\n",
       "  1\n",
       "  3\n",
       " 18\n",
       "  1\n",
       " 10\n",
       " 22\n",
       "  4\n",
       "  1\n",
       " 23\n",
       "  9\n",
       " 15\n",
       " 31\n",
       "  1\n",
       "  9\n",
       "  7\n",
       "  4\n",
       "  1\n",
       " 18\n",
       " 12\n",
       "  7\n",
       " 23\n",
       " 17\n",
       " 18\n",
       " 10\n",
       "  1\n",
       "  5\n",
       " 12\n",
       "  3\n",
       " 11\n",
       "  3\n",
       " 32\n",
       "  4\n",
       "[torch.DoubleTensor of size 121]\n",
       "\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
