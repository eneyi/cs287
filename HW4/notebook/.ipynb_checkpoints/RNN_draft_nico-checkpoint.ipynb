{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'hdf5'\n",
    "require 'rnn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "myFile = hdf5.open('../data_preprocessed/'..tostring(N)..'-grams.hdf5','r')\n",
    "data = myFile:all()\n",
    "F_train = data['F_train']\n",
    "input_data_valid = data['input_data_valid']\n",
    "output_matrix_train = data['output_matrix_train']\n",
    "input_matrix_train = data['input_matrix_train']\n",
    "input_data_train = data['input_data_train']\n",
    "input_data_valid_nospace = data['input_data_valid_nospace']\n",
    "input_data_test = data['input_data_test']\n",
    "myFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 599903\n",
       "      1\n",
       "[torch.LongStorage of size 2]\n",
       "\n",
       " 599903\n",
       "[torch.LongStorage of size 1]\n",
       "\n",
       " 599905\n",
       "[torch.LongStorage of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_matrix_train:size())\n",
    "print(output_matrix_train:size())\n",
    "print(input_data_train:size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  1\n",
       " 30\n",
       "  1\n",
       " 20\n",
       " 12\n",
       " 11\n",
       " 11\n",
       " 12\n",
       "  9\n",
       "  7\n",
       "  2\n",
       "[torch.LongTensor of size 11]\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- TOFIX: outpout data does not contain the prediction for the last to the end char (here 7)\n",
    "input_data_train:narrow(1,input_data_train:size(1)-10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 2\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.DoubleTensor of size 11]\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_matrix_train:narrow(1,output_matrix_train:size(1)-10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599000\t\n",
       "599\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Formating the input\n",
    "-- Currently using a hack to have nive divisions\n",
    "n = input_data_train:size(1)\n",
    "n_new = n - 905\n",
    "len = 100\n",
    "batch_size = 10\n",
    "print(n_new)\n",
    "print(n_new/(batch_size*len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Issue with last sequence if batch_size does not divide n\n",
    "t_input = torch.split(input_data_train:narrow(1,1,n_new):view(batch_size,n_new/batch_size),len, 2)\n",
    "t_output = torch.split(output_matrix_train:narrow(1,1,n_new):view(batch_size,n_new/batch_size),len, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  10\n",
       " 100\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_input[1]:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function build_rnn(embed_dim, vocab_size, len)\n",
    "    local batchRNN\n",
    "    local params\n",
    "    local grad_params\n",
    "    -- generic RNN transduced\n",
    "    batchRNN = nn.Sequential()\n",
    "        :add(nn.LookupTable(vocab_size, embed_dim))\n",
    "        :add(nn.SplitTable(1, batch_size))\n",
    "    \n",
    "    batchRNN:add(nn.Sequencer(nn.Recurrent(\n",
    "       embed_dim, nn.Linear(embed_dim, embed_dim), \n",
    "       nn.Linear(embed_dim, embed_dim), nn.Tanh(), len)))\n",
    "    -- Output\n",
    "    batchRNN:add(nn.Sequencer(nn.Linear(embed_dim, 2)))\n",
    "    batchRNN:add(nn.Sequencer(nn.LogSoftMax()))\n",
    "    batchRNN:remember('both')\n",
    "\n",
    "    -- Retrieve parameters (To do only once!!!)\n",
    "    params, grad_params = batchRNN:getParameters()\n",
    "    \n",
    "    return batchRNN, params, grad_params\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- we can then use a simple BCE Criterion for backprop\n",
    "pred = batchRNN:forward(t_inputT)\n",
    "crit = nn.SequencerCriterion(nn.ClassNLLCriterion())\n",
    "loss = crit:forward(pred, t_output_table)\n",
    "\n",
    "dLdPred = crit:backward(pred, t_output_table)\n",
    "batchRNN:backward(t_inputT, dLdPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function train_model(t_input, t_output, model, params, grad_params,\n",
    "                     criterion, eta, nEpochs, batch_size, batch_number, len)\n",
    "    -- Train the model with a mini batch SGD\n",
    "    -- standard parameters are\n",
    "    -- nEpochs = 1\n",
    "    -- batchSize = 32\n",
    "    -- eta = 0.01\n",
    "    local timer\n",
    "    local pred\n",
    "    local loss\n",
    "    local dLdPred\n",
    "    local t_inputT = torch.DoubleTensor(len,batch_size)\n",
    "    local t_output_table\n",
    "\n",
    "    -- To store the loss\n",
    "    local av_L = 0\n",
    "    \n",
    "    -- Initializing all the parameters between -0.05 and 0.05\n",
    "    for k=1,params:size(1) do\n",
    "        params[k] = torch.uniform(-0.05,0.05)\n",
    "    end\n",
    "\n",
    "    for i = 1, nEpochs do\n",
    "        -- timing the epoch\n",
    "        timer = torch.Timer()\n",
    "        av_L = 0\n",
    "        \n",
    "        -- mini batch loop\n",
    "        for k = 1, batch_number do\n",
    "            -- Mini batch data\n",
    "                \n",
    "            t_inputT:copy(t_input[k]:t())\n",
    "            t_output_table = torch.split(t_output[k],1,2)\n",
    "            --format the output\n",
    "            for j=1,len do\n",
    "                t_output_table[j] = t_output_table[i]:squeeze()\n",
    "            end \n",
    "            \n",
    "            -- reset gradients\n",
    "            model:zeroGradParameters()\n",
    "            pred = model:forward(t_inputT)\n",
    "            loss = crit:forward(pred, t_output_table)\n",
    "            av_L = av_L + loss\n",
    "\n",
    "            dLdPred = crit:backward(pred, t_output_table)\n",
    "            model:backward(t_inputT, dLdPred)\n",
    "            \n",
    "            -- gradient normalization with max norm 5 (l2 norm)\n",
    "            grad_params:view(grad_params:size(1),1):renorm(1,2,5)\n",
    "            model:updateParameters(eta)\n",
    "            \n",
    "        end\n",
    "            \n",
    "        print('Epoch '..i..': '..timer:time().real)\n",
    "        print('Average Loss: '..av_L/math.floor(batch_number))\n",
    "       \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 49\n",
    "embed_dim = 16\n",
    "len = 100\n",
    "eta = 0.1\n",
    "nEpochs = 1\n",
    "batch_number = n_new/(batch_size*len)\n",
    "\n",
    "-- Building model\n",
    "batchRNN, params, grad_params = build_rnn(embed_dim, vocab_size, len)\n",
    "crit = nn.SequencerCriterion(nn.ClassNLLCriterion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1: 10.283056020737\t\n",
       "Average Loss: 47.798446118375\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2: 10.49356508255\t\n",
       "Average Loss: 47.901252699071\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3: 12.212630987167\t\n",
       "Average Loss: 46.838004103291\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4: 12.556469917297\t\n",
       "Average Loss: 49.758562062299\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5: 11.571686029434\t\n",
       "Average Loss: 48.248888095988\t\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(t_input, t_output, batchRNN, params, grad_params,\n",
    "                     criterion, eta, 5, batch_size, batch_number, len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
